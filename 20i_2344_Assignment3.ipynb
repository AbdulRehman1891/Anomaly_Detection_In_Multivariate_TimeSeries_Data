{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preprocess**\n"
      ],
      "metadata": {
        "id": "VsHkaJ5SeNJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6r2lKOweIVg",
        "outputId": "36002876-1d31-4455-e5b2-95a65a8a5894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in 'Class' column: 0\n",
            "Missing values in 'Class' column after dropping rows: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Check for missing values in the 'Class' column\n",
        "print(\"Missing values in 'Class' column:\", data['Class'].isnull().sum())\n",
        "\n",
        "# Drop rows where 'Class' is NaN\n",
        "data = data.dropna(subset=['Class'])\n",
        "\n",
        "# Re-check to confirm no more NaNs in 'Class'\n",
        "print(\"Missing values in 'Class' column after dropping rows:\", data['Class'].isnull().sum())\n",
        "\n",
        "\n",
        "# Normalize 'Time' and 'Amount' using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data['NormalizedTime'] = scaler.fit_transform(data[['Time']])\n",
        "data['NormalizedAmount'] = scaler.fit_transform(data[['Amount']])\n",
        "data.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
        "\n",
        "# Split the dataset\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yGL1D_2aeJ8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Data Augmentation with Geometric Distribution Masks**"
      ],
      "metadata": {
        "id": "bHimjeQ8fBQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_geometric_mask(data, mask_prob=0.1):\n",
        "    mask = np.random.geometric(p=mask_prob, size=data.shape)\n",
        "    masked_data = np.where(mask > 1, data, 0)  # Apply mask where mask > 1\n",
        "    return masked_data\n",
        "\n",
        "# Apply masks to the training data\n",
        "X_train_masked = apply_geometric_mask(X_train.values)\n"
      ],
      "metadata": {
        "id": "ott1mhBZfF16"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer-Based Autoencoder**"
      ],
      "metadata": {
        "id": "-WCz7G3AfH4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, feature_size=256, num_heads=4, num_layers=1, forward_expansion=4, dropout=0.1):\n",
        "        super(TransformerAutoencoder, self).__init__()\n",
        "        # Ensure feature_size is a multiple of num_heads\n",
        "        assert feature_size % num_heads == 0, \"feature_size must be divisible by num_heads\"\n",
        "\n",
        "        self.embedding = nn.Linear(input_size, feature_size)  # Adjust input feature size to transformer feature size\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=feature_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=feature_size * forward_expansion,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=feature_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=feature_size * forward_expansion,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = nn.Linear(feature_size, input_size)  # Map back to the original input size\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)  # Embed input features\n",
        "        encoded = self.encoder(src)\n",
        "        decoded = self.decoder(encoded, encoded)\n",
        "        output = self.output_layer(decoded)  # Map back to original feature space\n",
        "        return output\n",
        "\n",
        "# Use this to initialize your model\n",
        "input_size = X_train.shape[1]\n",
        "feature_size = 256  # Choose a feature size that is a multiple of num_heads\n",
        "num_heads = 4\n",
        "model = TransformerAutoencoder(input_size, feature_size, num_heads).to(device)"
      ],
      "metadata": {
        "id": "AY6-uMO3fMJo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming X_train_masked and y_train are already defined and appropriately processed\n",
        "train_dataset = TensorDataset(torch.tensor(X_train_masked, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()  # For reconstruction loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epoch_losses = []\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, _ in train_loader:  # Labels are not used during training\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            avg_loss=total_loss / len(train_loader)\n",
        "            epoch_losses.append(avg_loss)\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss}')\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQVMZ9p_gJi3",
        "outputId": "4ed3afdc-70da-4558-a297-ef68c7d73143"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.2997524391185833\n",
            "Epoch 2, Average Loss: 0.6956491341079123\n",
            "Epoch 3, Average Loss: 0.28350231071847526\n",
            "Epoch 4, Average Loss: 0.1433485048947174\n",
            "Epoch 5, Average Loss: 0.11341660927414576\n",
            "Epoch 6, Average Loss: 0.10900120821915774\n",
            "Epoch 7, Average Loss: 0.0994759663450862\n",
            "Epoch 8, Average Loss: 0.10284824000943278\n",
            "Epoch 9, Average Loss: 0.09021952378908464\n",
            "Epoch 10, Average Loss: 0.08568075369468584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epoch_losses, marker='o', linestyle='-')\n",
        "plt.title('Training Loss Per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6MujuAwhEqlN",
        "outputId": "fb706010-8f9d-45cb-8180-5714bc741a2b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GAN**"
      ],
      "metadata": {
        "id": "0ZqjhuuBrIcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, autoencoder_model):\n",
        "        super(Generator, self).__init__()\n",
        "        self.autoencoder = autoencoder_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.autoencoder(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVXr7ukbrLGp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contrastive Loss for Anomoly Detection**"
      ],
      "metadata": {
        "id": "EX3bkYy1rRZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output, target, label):\n",
        "        euclidean_distance = torch.nn.functional.pairwise_distance(output, target)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n"
      ],
      "metadata": {
        "id": "Vdh-8xx8rVie"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train GAN alongside with AutoEncoder**"
      ],
      "metadata": {
        "id": "XH_2eS69rZ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan_autoencoder(autoencoder, generator, discriminator, data_loader, epochs=10):\n",
        "    # Optimizers\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "    optimizer_AE = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "    adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (imgs, _) in enumerate(data_loader):\n",
        "\n",
        "            valid = torch.ones(imgs.size(0), 1, device=device, dtype=torch.float32)\n",
        "            fake = torch.zeros(imgs.size(0), 1, device=device, dtype=torch.float32)\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "            z = torch.randn(imgs.shape[0], imgs.shape[1], device=device)\n",
        "            gen_imgs = generator(z)\n",
        "            g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "            real_loss = adversarial_loss(discriminator(imgs), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Autoencoder\n",
        "            # ---------------------\n",
        "            optimizer_AE.zero_grad()\n",
        "            reconstructed_imgs = autoencoder(imgs)\n",
        "            ae_loss = torch.nn.functional.mse_loss(reconstructed_imgs, imgs)\n",
        "            ae_loss.backward()\n",
        "            optimizer_AE.step()\n",
        "\n",
        "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [AE loss: %f]\" %\n",
        "                  (epoch, epochs, i, len(data_loader), d_loss.item(), g_loss.item(), ae_loss.item()))\n"
      ],
      "metadata": {
        "id": "B28PcZPfrd36"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Device setup: CUDA or CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Corrected input size\n",
        "input_dim = 30  # Based on the shape of the data in your DataLoader\n",
        "\n",
        "# Initialize models with correct input dimension\n",
        "autoencoder_model = AutoencoderWithTransformer(input_size=input_dim, hidden_size=64, num_enc_layers=2, num_heads=1)\n",
        "generator = Generator(autoencoder_model)\n",
        "discriminator = Discriminator(input_dim=input_dim)\n",
        "\n",
        "# Move models to the appropriate device\n",
        "autoencoder_model.to(device)\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "\n",
        "# Data loader (assuming X_train_masked and y_train are available)\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train_masked, dtype=torch.float32),\n",
        "                                        torch.tensor(y_train.values, dtype=torch.float32)),\n",
        "                          batch_size=32, shuffle=True)\n",
        "\n",
        "# Call the training function with the correct input size for discriminator\n",
        "train_gan_autoencoder(autoencoder=autoencoder_model,\n",
        "                      generator=generator,\n",
        "                      discriminator=discriminator,\n",
        "                      data_loader=train_loader,\n",
        "                      epochs=10)  # Adjust the number of epochs as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N6TVLbqNc9jg",
        "outputId": "b90f8867-df53-46bb-92d2-f1e5440fe48c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/10] [Batch 0/7121] [D loss: 0.692515] [G loss: 0.707324] [AE loss: 1.245325]\n",
            "[Epoch 0/10] [Batch 1/7121] [D loss: 0.721692] [G loss: 0.643919] [AE loss: 1.027464]\n",
            "[Epoch 0/10] [Batch 2/7121] [D loss: 0.722975] [G loss: 0.636395] [AE loss: 1.073896]\n",
            "[Epoch 0/10] [Batch 3/7121] [D loss: 0.716805] [G loss: 0.640432] [AE loss: 1.797915]\n",
            "[Epoch 0/10] [Batch 4/7121] [D loss: 0.710148] [G loss: 0.644775] [AE loss: 1.345544]\n",
            "[Epoch 0/10] [Batch 5/7121] [D loss: 0.701391] [G loss: 0.653019] [AE loss: 0.845923]\n",
            "[Epoch 0/10] [Batch 6/7121] [D loss: 0.689451] [G loss: 0.657893] [AE loss: 0.860108]\n",
            "[Epoch 0/10] [Batch 7/7121] [D loss: 0.690890] [G loss: 0.660376] [AE loss: 1.675545]\n",
            "[Epoch 0/10] [Batch 8/7121] [D loss: 0.681988] [G loss: 0.655757] [AE loss: 1.057407]\n",
            "[Epoch 0/10] [Batch 9/7121] [D loss: 0.681187] [G loss: 0.649022] [AE loss: 1.028790]\n",
            "[Epoch 0/10] [Batch 10/7121] [D loss: 0.688480] [G loss: 0.631985] [AE loss: 1.019851]\n",
            "[Epoch 0/10] [Batch 11/7121] [D loss: 0.683966] [G loss: 0.620534] [AE loss: 0.864892]\n",
            "[Epoch 0/10] [Batch 12/7121] [D loss: 0.683557] [G loss: 0.609341] [AE loss: 0.975556]\n",
            "[Epoch 0/10] [Batch 13/7121] [D loss: 0.684836] [G loss: 0.597660] [AE loss: 1.035816]\n",
            "[Epoch 0/10] [Batch 14/7121] [D loss: 0.691664] [G loss: 0.587773] [AE loss: 1.253546]\n",
            "[Epoch 0/10] [Batch 15/7121] [D loss: 0.688443] [G loss: 0.581677] [AE loss: 1.062990]\n",
            "[Epoch 0/10] [Batch 16/7121] [D loss: 0.680643] [G loss: 0.576865] [AE loss: 1.382719]\n",
            "[Epoch 0/10] [Batch 17/7121] [D loss: 0.675136] [G loss: 0.572380] [AE loss: 1.896310]\n",
            "[Epoch 0/10] [Batch 18/7121] [D loss: 0.681278] [G loss: 0.567249] [AE loss: 1.339904]\n",
            "[Epoch 0/10] [Batch 19/7121] [D loss: 0.685636] [G loss: 0.563223] [AE loss: 0.923842]\n",
            "[Epoch 0/10] [Batch 20/7121] [D loss: 0.667368] [G loss: 0.561449] [AE loss: 1.355795]\n",
            "[Epoch 0/10] [Batch 21/7121] [D loss: 0.668186] [G loss: 0.557801] [AE loss: 0.938128]\n",
            "[Epoch 0/10] [Batch 22/7121] [D loss: 0.668412] [G loss: 0.549336] [AE loss: 1.639788]\n",
            "[Epoch 0/10] [Batch 23/7121] [D loss: 0.666792] [G loss: 0.543604] [AE loss: 1.289212]\n",
            "[Epoch 0/10] [Batch 24/7121] [D loss: 0.667847] [G loss: 0.535727] [AE loss: 1.549603]\n",
            "[Epoch 0/10] [Batch 25/7121] [D loss: 0.670410] [G loss: 0.530785] [AE loss: 1.160521]\n",
            "[Epoch 0/10] [Batch 26/7121] [D loss: 0.690102] [G loss: 0.520420] [AE loss: 1.077504]\n",
            "[Epoch 0/10] [Batch 27/7121] [D loss: 0.682531] [G loss: 0.508683] [AE loss: 1.336214]\n",
            "[Epoch 0/10] [Batch 28/7121] [D loss: 0.676157] [G loss: 0.504765] [AE loss: 1.622828]\n",
            "[Epoch 0/10] [Batch 29/7121] [D loss: 0.692800] [G loss: 0.497097] [AE loss: 0.973896]\n",
            "[Epoch 0/10] [Batch 30/7121] [D loss: 0.694407] [G loss: 0.493625] [AE loss: 1.169466]\n",
            "[Epoch 0/10] [Batch 31/7121] [D loss: 0.706122] [G loss: 0.479189] [AE loss: 1.004470]\n",
            "[Epoch 0/10] [Batch 32/7121] [D loss: 0.703146] [G loss: 0.478147] [AE loss: 1.002393]\n",
            "[Epoch 0/10] [Batch 33/7121] [D loss: 0.716639] [G loss: 0.462362] [AE loss: 3.081886]\n",
            "[Epoch 0/10] [Batch 34/7121] [D loss: 0.706917] [G loss: 0.463785] [AE loss: 1.461358]\n",
            "[Epoch 0/10] [Batch 35/7121] [D loss: 0.737097] [G loss: 0.458695] [AE loss: 0.903369]\n",
            "[Epoch 0/10] [Batch 36/7121] [D loss: 0.721947] [G loss: 0.453044] [AE loss: 1.557292]\n",
            "[Epoch 0/10] [Batch 37/7121] [D loss: 0.729216] [G loss: 0.455484] [AE loss: 0.989060]\n",
            "[Epoch 0/10] [Batch 38/7121] [D loss: 0.738674] [G loss: 0.450311] [AE loss: 0.850039]\n",
            "[Epoch 0/10] [Batch 39/7121] [D loss: 0.722628] [G loss: 0.452207] [AE loss: 1.458342]\n",
            "[Epoch 0/10] [Batch 40/7121] [D loss: 0.730986] [G loss: 0.454290] [AE loss: 1.343760]\n",
            "[Epoch 0/10] [Batch 41/7121] [D loss: 0.719971] [G loss: 0.456359] [AE loss: 1.070794]\n",
            "[Epoch 0/10] [Batch 42/7121] [D loss: 0.717901] [G loss: 0.464112] [AE loss: 2.438449]\n",
            "[Epoch 0/10] [Batch 43/7121] [D loss: 0.704599] [G loss: 0.474636] [AE loss: 2.390643]\n",
            "[Epoch 0/10] [Batch 44/7121] [D loss: 0.733074] [G loss: 0.477676] [AE loss: 0.832476]\n",
            "[Epoch 0/10] [Batch 45/7121] [D loss: 0.695850] [G loss: 0.486837] [AE loss: 1.506032]\n",
            "[Epoch 0/10] [Batch 46/7121] [D loss: 0.690547] [G loss: 0.500062] [AE loss: 2.946450]\n",
            "[Epoch 0/10] [Batch 47/7121] [D loss: 0.690388] [G loss: 0.510169] [AE loss: 4.089941]\n",
            "[Epoch 0/10] [Batch 48/7121] [D loss: 0.663092] [G loss: 0.519569] [AE loss: 1.687362]\n",
            "[Epoch 0/10] [Batch 49/7121] [D loss: 0.677600] [G loss: 0.524230] [AE loss: 1.285811]\n",
            "[Epoch 0/10] [Batch 50/7121] [D loss: 0.665132] [G loss: 0.539436] [AE loss: 1.469658]\n",
            "[Epoch 0/10] [Batch 51/7121] [D loss: 0.640891] [G loss: 0.548363] [AE loss: 2.826520]\n",
            "[Epoch 0/10] [Batch 52/7121] [D loss: 0.665193] [G loss: 0.544182] [AE loss: 1.236398]\n",
            "[Epoch 0/10] [Batch 53/7121] [D loss: 0.642589] [G loss: 0.557593] [AE loss: 1.456027]\n",
            "[Epoch 0/10] [Batch 54/7121] [D loss: 0.672402] [G loss: 0.544049] [AE loss: 1.181630]\n",
            "[Epoch 0/10] [Batch 55/7121] [D loss: 0.658576] [G loss: 0.535127] [AE loss: 1.360196]\n",
            "[Epoch 0/10] [Batch 56/7121] [D loss: 0.690397] [G loss: 0.526240] [AE loss: 1.104001]\n",
            "[Epoch 0/10] [Batch 57/7121] [D loss: 0.700486] [G loss: 0.508049] [AE loss: 0.986272]\n",
            "[Epoch 0/10] [Batch 58/7121] [D loss: 0.718505] [G loss: 0.504615] [AE loss: 1.121582]\n",
            "[Epoch 0/10] [Batch 59/7121] [D loss: 0.709528] [G loss: 0.490503] [AE loss: 1.219814]\n",
            "[Epoch 0/10] [Batch 60/7121] [D loss: 0.708342] [G loss: 0.474533] [AE loss: 1.378870]\n",
            "[Epoch 0/10] [Batch 61/7121] [D loss: 0.724283] [G loss: 0.476409] [AE loss: 1.282229]\n",
            "[Epoch 0/10] [Batch 62/7121] [D loss: 0.736121] [G loss: 0.480124] [AE loss: 1.192872]\n",
            "[Epoch 0/10] [Batch 63/7121] [D loss: 0.717617] [G loss: 0.481060] [AE loss: 1.547983]\n",
            "[Epoch 0/10] [Batch 64/7121] [D loss: 0.721747] [G loss: 0.483706] [AE loss: 1.094823]\n",
            "[Epoch 0/10] [Batch 65/7121] [D loss: 0.734227] [G loss: 0.482010] [AE loss: 1.379562]\n",
            "[Epoch 0/10] [Batch 66/7121] [D loss: 0.712255] [G loss: 0.491248] [AE loss: 1.422334]\n",
            "[Epoch 0/10] [Batch 67/7121] [D loss: 0.721172] [G loss: 0.488528] [AE loss: 1.977366]\n",
            "[Epoch 0/10] [Batch 68/7121] [D loss: 0.747165] [G loss: 0.485654] [AE loss: 1.058846]\n",
            "[Epoch 0/10] [Batch 69/7121] [D loss: 0.734674] [G loss: 0.488847] [AE loss: 1.421423]\n",
            "[Epoch 0/10] [Batch 70/7121] [D loss: 0.736162] [G loss: 0.494574] [AE loss: 1.296621]\n",
            "[Epoch 0/10] [Batch 71/7121] [D loss: 0.736519] [G loss: 0.511574] [AE loss: 1.776331]\n",
            "[Epoch 0/10] [Batch 72/7121] [D loss: 0.748018] [G loss: 0.518907] [AE loss: 0.863152]\n",
            "[Epoch 0/10] [Batch 73/7121] [D loss: 0.723347] [G loss: 0.537937] [AE loss: 1.437320]\n",
            "[Epoch 0/10] [Batch 74/7121] [D loss: 0.721219] [G loss: 0.544083] [AE loss: 1.010794]\n",
            "[Epoch 0/10] [Batch 75/7121] [D loss: 0.703732] [G loss: 0.560447] [AE loss: 1.203619]\n",
            "[Epoch 0/10] [Batch 76/7121] [D loss: 0.712595] [G loss: 0.567614] [AE loss: 1.148456]\n",
            "[Epoch 0/10] [Batch 77/7121] [D loss: 0.694250] [G loss: 0.581659] [AE loss: 1.100645]\n",
            "[Epoch 0/10] [Batch 78/7121] [D loss: 0.684934] [G loss: 0.588700] [AE loss: 1.415866]\n",
            "[Epoch 0/10] [Batch 79/7121] [D loss: 0.699033] [G loss: 0.591719] [AE loss: 1.097434]\n",
            "[Epoch 0/10] [Batch 80/7121] [D loss: 0.700344] [G loss: 0.587732] [AE loss: 0.987542]\n",
            "[Epoch 0/10] [Batch 81/7121] [D loss: 0.711911] [G loss: 0.584927] [AE loss: 1.014133]\n",
            "[Epoch 0/10] [Batch 82/7121] [D loss: 0.711142] [G loss: 0.580296] [AE loss: 1.139864]\n",
            "[Epoch 0/10] [Batch 83/7121] [D loss: 0.708277] [G loss: 0.574407] [AE loss: 1.532519]\n",
            "[Epoch 0/10] [Batch 84/7121] [D loss: 0.706488] [G loss: 0.567096] [AE loss: 1.945110]\n",
            "[Epoch 0/10] [Batch 85/7121] [D loss: 0.731374] [G loss: 0.559781] [AE loss: 0.974804]\n",
            "[Epoch 0/10] [Batch 86/7121] [D loss: 0.727774] [G loss: 0.554944] [AE loss: 1.031546]\n",
            "[Epoch 0/10] [Batch 87/7121] [D loss: 0.726360] [G loss: 0.554320] [AE loss: 1.202619]\n",
            "[Epoch 0/10] [Batch 88/7121] [D loss: 0.733568] [G loss: 0.555662] [AE loss: 0.911945]\n",
            "[Epoch 0/10] [Batch 89/7121] [D loss: 0.741714] [G loss: 0.560635] [AE loss: 0.947542]\n",
            "[Epoch 0/10] [Batch 90/7121] [D loss: 0.744885] [G loss: 0.565726] [AE loss: 0.988856]\n",
            "[Epoch 0/10] [Batch 91/7121] [D loss: 0.732019] [G loss: 0.577733] [AE loss: 1.197055]\n",
            "[Epoch 0/10] [Batch 92/7121] [D loss: 0.723952] [G loss: 0.597506] [AE loss: 1.270856]\n",
            "[Epoch 0/10] [Batch 93/7121] [D loss: 0.722996] [G loss: 0.609032] [AE loss: 1.472631]\n",
            "[Epoch 0/10] [Batch 94/7121] [D loss: 0.692929] [G loss: 0.631519] [AE loss: 1.806019]\n",
            "[Epoch 0/10] [Batch 95/7121] [D loss: 0.713752] [G loss: 0.644820] [AE loss: 0.935853]\n",
            "[Epoch 0/10] [Batch 96/7121] [D loss: 0.693682] [G loss: 0.664312] [AE loss: 1.442403]\n",
            "[Epoch 0/10] [Batch 97/7121] [D loss: 0.682095] [G loss: 0.681835] [AE loss: 1.482821]\n",
            "[Epoch 0/10] [Batch 98/7121] [D loss: 0.684579] [G loss: 0.699802] [AE loss: 0.970686]\n",
            "[Epoch 0/10] [Batch 99/7121] [D loss: 0.672832] [G loss: 0.717302] [AE loss: 1.252049]\n",
            "[Epoch 0/10] [Batch 100/7121] [D loss: 0.670426] [G loss: 0.731284] [AE loss: 2.516366]\n",
            "[Epoch 0/10] [Batch 101/7121] [D loss: 0.653212] [G loss: 0.742612] [AE loss: 1.237786]\n",
            "[Epoch 0/10] [Batch 102/7121] [D loss: 0.665378] [G loss: 0.753315] [AE loss: 0.971873]\n",
            "[Epoch 0/10] [Batch 103/7121] [D loss: 0.666352] [G loss: 0.758294] [AE loss: 1.044138]\n",
            "[Epoch 0/10] [Batch 104/7121] [D loss: 0.658361] [G loss: 0.760847] [AE loss: 1.183100]\n",
            "[Epoch 0/10] [Batch 105/7121] [D loss: 0.649123] [G loss: 0.759079] [AE loss: 1.026395]\n",
            "[Epoch 0/10] [Batch 106/7121] [D loss: 0.669133] [G loss: 0.756331] [AE loss: 1.022301]\n",
            "[Epoch 0/10] [Batch 107/7121] [D loss: 0.660633] [G loss: 0.754826] [AE loss: 0.914914]\n",
            "[Epoch 0/10] [Batch 108/7121] [D loss: 0.665478] [G loss: 0.744986] [AE loss: 0.809705]\n",
            "[Epoch 0/10] [Batch 109/7121] [D loss: 0.686250] [G loss: 0.735341] [AE loss: 1.389410]\n",
            "[Epoch 0/10] [Batch 110/7121] [D loss: 0.677563] [G loss: 0.731497] [AE loss: 0.859860]\n",
            "[Epoch 0/10] [Batch 111/7121] [D loss: 0.687604] [G loss: 0.711016] [AE loss: 1.266070]\n",
            "[Epoch 0/10] [Batch 112/7121] [D loss: 0.672842] [G loss: 0.706389] [AE loss: 3.183463]\n",
            "[Epoch 0/10] [Batch 113/7121] [D loss: 0.679267] [G loss: 0.692869] [AE loss: 1.185352]\n",
            "[Epoch 0/10] [Batch 114/7121] [D loss: 0.693230] [G loss: 0.683008] [AE loss: 0.944342]\n",
            "[Epoch 0/10] [Batch 115/7121] [D loss: 0.690854] [G loss: 0.674630] [AE loss: 1.256924]\n",
            "[Epoch 0/10] [Batch 116/7121] [D loss: 0.694323] [G loss: 0.661096] [AE loss: 1.150759]\n",
            "[Epoch 0/10] [Batch 117/7121] [D loss: 0.703671] [G loss: 0.652035] [AE loss: 1.569297]\n",
            "[Epoch 0/10] [Batch 118/7121] [D loss: 0.688826] [G loss: 0.640277] [AE loss: 1.291501]\n",
            "[Epoch 0/10] [Batch 119/7121] [D loss: 0.701995] [G loss: 0.636021] [AE loss: 0.933653]\n",
            "[Epoch 0/10] [Batch 120/7121] [D loss: 0.698418] [G loss: 0.622112] [AE loss: 1.089651]\n",
            "[Epoch 0/10] [Batch 121/7121] [D loss: 0.698421] [G loss: 0.623608] [AE loss: 1.108327]\n",
            "[Epoch 0/10] [Batch 122/7121] [D loss: 0.666198] [G loss: 0.620044] [AE loss: 1.288448]\n",
            "[Epoch 0/10] [Batch 123/7121] [D loss: 0.677637] [G loss: 0.613192] [AE loss: 16.280460]\n",
            "[Epoch 0/10] [Batch 124/7121] [D loss: 0.669876] [G loss: 0.620527] [AE loss: 0.978660]\n",
            "[Epoch 0/10] [Batch 125/7121] [D loss: 0.678497] [G loss: 0.615564] [AE loss: 1.133367]\n",
            "[Epoch 0/10] [Batch 126/7121] [D loss: 0.674655] [G loss: 0.626427] [AE loss: 1.118528]\n",
            "[Epoch 0/10] [Batch 127/7121] [D loss: 0.660614] [G loss: 0.627862] [AE loss: 0.983136]\n",
            "[Epoch 0/10] [Batch 128/7121] [D loss: 0.654251] [G loss: 0.634231] [AE loss: 0.985121]\n",
            "[Epoch 0/10] [Batch 129/7121] [D loss: 0.641540] [G loss: 0.641607] [AE loss: 1.021154]\n",
            "[Epoch 0/10] [Batch 130/7121] [D loss: 0.646469] [G loss: 0.647939] [AE loss: 1.173302]\n",
            "[Epoch 0/10] [Batch 131/7121] [D loss: 0.639542] [G loss: 0.652911] [AE loss: 0.932245]\n",
            "[Epoch 0/10] [Batch 132/7121] [D loss: 0.629820] [G loss: 0.660367] [AE loss: 0.904253]\n",
            "[Epoch 0/10] [Batch 133/7121] [D loss: 0.616600] [G loss: 0.664914] [AE loss: 1.525644]\n",
            "[Epoch 0/10] [Batch 134/7121] [D loss: 0.623189] [G loss: 0.668343] [AE loss: 0.866675]\n",
            "[Epoch 0/10] [Batch 135/7121] [D loss: 0.598237] [G loss: 0.676421] [AE loss: 1.245705]\n",
            "[Epoch 0/10] [Batch 136/7121] [D loss: 0.602362] [G loss: 0.678219] [AE loss: 1.090574]\n",
            "[Epoch 0/10] [Batch 137/7121] [D loss: 0.588558] [G loss: 0.681109] [AE loss: 1.137595]\n",
            "[Epoch 0/10] [Batch 138/7121] [D loss: 0.614394] [G loss: 0.675128] [AE loss: 0.856494]\n",
            "[Epoch 0/10] [Batch 139/7121] [D loss: 0.607927] [G loss: 0.673355] [AE loss: 0.942724]\n",
            "[Epoch 0/10] [Batch 140/7121] [D loss: 0.609394] [G loss: 0.671285] [AE loss: 0.872044]\n",
            "[Epoch 0/10] [Batch 141/7121] [D loss: 0.599324] [G loss: 0.672253] [AE loss: 0.877865]\n",
            "[Epoch 0/10] [Batch 142/7121] [D loss: 0.564909] [G loss: 0.661419] [AE loss: 1.978253]\n",
            "[Epoch 0/10] [Batch 143/7121] [D loss: 0.592051] [G loss: 0.658404] [AE loss: 1.492601]\n",
            "[Epoch 0/10] [Batch 144/7121] [D loss: 0.579281] [G loss: 0.651719] [AE loss: 1.947538]\n",
            "[Epoch 0/10] [Batch 145/7121] [D loss: 0.597305] [G loss: 0.648263] [AE loss: 0.945342]\n",
            "[Epoch 0/10] [Batch 146/7121] [D loss: 0.602929] [G loss: 0.639094] [AE loss: 0.910195]\n",
            "[Epoch 0/10] [Batch 147/7121] [D loss: 0.605469] [G loss: 0.634878] [AE loss: 0.996237]\n",
            "[Epoch 0/10] [Batch 148/7121] [D loss: 0.596471] [G loss: 0.618182] [AE loss: 1.349567]\n",
            "[Epoch 0/10] [Batch 149/7121] [D loss: 0.600868] [G loss: 0.618043] [AE loss: 1.260503]\n",
            "[Epoch 0/10] [Batch 150/7121] [D loss: 0.608287] [G loss: 0.614277] [AE loss: 1.088351]\n",
            "[Epoch 0/10] [Batch 151/7121] [D loss: 0.588077] [G loss: 0.618932] [AE loss: 1.142082]\n",
            "[Epoch 0/10] [Batch 152/7121] [D loss: 0.601530] [G loss: 0.618361] [AE loss: 1.092245]\n",
            "[Epoch 0/10] [Batch 153/7121] [D loss: 0.609673] [G loss: 0.614782] [AE loss: 0.994838]\n",
            "[Epoch 0/10] [Batch 154/7121] [D loss: 0.592593] [G loss: 0.623510] [AE loss: 1.406211]\n",
            "[Epoch 0/10] [Batch 155/7121] [D loss: 0.574547] [G loss: 0.626066] [AE loss: 1.525525]\n",
            "[Epoch 0/10] [Batch 156/7121] [D loss: 0.591610] [G loss: 0.635574] [AE loss: 1.049011]\n",
            "[Epoch 0/10] [Batch 157/7121] [D loss: 0.568122] [G loss: 0.631082] [AE loss: 1.141258]\n",
            "[Epoch 0/10] [Batch 158/7121] [D loss: 0.567511] [G loss: 0.642690] [AE loss: 1.182306]\n",
            "[Epoch 0/10] [Batch 159/7121] [D loss: 0.570873] [G loss: 0.656709] [AE loss: 1.293922]\n",
            "[Epoch 0/10] [Batch 160/7121] [D loss: 0.567454] [G loss: 0.646482] [AE loss: 1.363538]\n",
            "[Epoch 0/10] [Batch 161/7121] [D loss: 0.552303] [G loss: 0.647391] [AE loss: 1.781845]\n",
            "[Epoch 0/10] [Batch 162/7121] [D loss: 0.571550] [G loss: 0.642218] [AE loss: 1.058793]\n",
            "[Epoch 0/10] [Batch 163/7121] [D loss: 0.583624] [G loss: 0.635101] [AE loss: 1.423166]\n",
            "[Epoch 0/10] [Batch 164/7121] [D loss: 0.582703] [G loss: 0.637136] [AE loss: 1.117920]\n",
            "[Epoch 0/10] [Batch 165/7121] [D loss: 0.602958] [G loss: 0.623629] [AE loss: 1.014913]\n",
            "[Epoch 0/10] [Batch 166/7121] [D loss: 0.583448] [G loss: 0.607892] [AE loss: 1.350299]\n",
            "[Epoch 0/10] [Batch 167/7121] [D loss: 0.600728] [G loss: 0.584496] [AE loss: 1.324786]\n",
            "[Epoch 0/10] [Batch 168/7121] [D loss: 0.626848] [G loss: 0.562125] [AE loss: 1.202662]\n",
            "[Epoch 0/10] [Batch 169/7121] [D loss: 0.653755] [G loss: 0.526448] [AE loss: 1.030756]\n",
            "[Epoch 0/10] [Batch 170/7121] [D loss: 0.681046] [G loss: 0.492457] [AE loss: 1.220580]\n",
            "[Epoch 0/10] [Batch 171/7121] [D loss: 0.677658] [G loss: 0.461665] [AE loss: 1.487280]\n",
            "[Epoch 0/10] [Batch 172/7121] [D loss: 0.743728] [G loss: 0.428278] [AE loss: 1.555598]\n",
            "[Epoch 0/10] [Batch 173/7121] [D loss: 0.745827] [G loss: 0.396158] [AE loss: 1.107834]\n",
            "[Epoch 0/10] [Batch 174/7121] [D loss: 0.774578] [G loss: 0.377559] [AE loss: 1.152615]\n",
            "[Epoch 0/10] [Batch 175/7121] [D loss: 0.817206] [G loss: 0.359495] [AE loss: 1.124170]\n",
            "[Epoch 0/10] [Batch 176/7121] [D loss: 0.779958] [G loss: 0.358643] [AE loss: 1.663867]\n",
            "[Epoch 0/10] [Batch 177/7121] [D loss: 0.816059] [G loss: 0.357760] [AE loss: 1.044518]\n",
            "[Epoch 0/10] [Batch 178/7121] [D loss: 0.817203] [G loss: 0.348543] [AE loss: 1.344450]\n",
            "[Epoch 0/10] [Batch 179/7121] [D loss: 0.838897] [G loss: 0.347836] [AE loss: 1.148676]\n",
            "[Epoch 0/10] [Batch 180/7121] [D loss: 0.831161] [G loss: 0.353288] [AE loss: 1.086268]\n",
            "[Epoch 0/10] [Batch 181/7121] [D loss: 0.792001] [G loss: 0.373522] [AE loss: 1.373074]\n",
            "[Epoch 0/10] [Batch 182/7121] [D loss: 0.800378] [G loss: 0.387532] [AE loss: 0.925037]\n",
            "[Epoch 0/10] [Batch 183/7121] [D loss: 0.778214] [G loss: 0.395941] [AE loss: 1.169953]\n",
            "[Epoch 0/10] [Batch 184/7121] [D loss: 0.799892] [G loss: 0.404631] [AE loss: 1.007050]\n",
            "[Epoch 0/10] [Batch 185/7121] [D loss: 0.764277] [G loss: 0.426070] [AE loss: 1.257581]\n",
            "[Epoch 0/10] [Batch 186/7121] [D loss: 0.763275] [G loss: 0.440117] [AE loss: 1.115751]\n",
            "[Epoch 0/10] [Batch 187/7121] [D loss: 0.747022] [G loss: 0.457720] [AE loss: 1.324382]\n",
            "[Epoch 0/10] [Batch 188/7121] [D loss: 0.760308] [G loss: 0.475568] [AE loss: 1.442652]\n",
            "[Epoch 0/10] [Batch 189/7121] [D loss: 0.771335] [G loss: 0.483207] [AE loss: 1.293994]\n",
            "[Epoch 0/10] [Batch 190/7121] [D loss: 0.729904] [G loss: 0.508377] [AE loss: 1.393542]\n",
            "[Epoch 0/10] [Batch 191/7121] [D loss: 0.740715] [G loss: 0.528251] [AE loss: 1.321466]\n",
            "[Epoch 0/10] [Batch 192/7121] [D loss: 0.740222] [G loss: 0.552292] [AE loss: 1.117602]\n",
            "[Epoch 0/10] [Batch 193/7121] [D loss: 0.716074] [G loss: 0.565742] [AE loss: 1.123719]\n",
            "[Epoch 0/10] [Batch 194/7121] [D loss: 0.717594] [G loss: 0.585685] [AE loss: 1.007403]\n",
            "[Epoch 0/10] [Batch 195/7121] [D loss: 0.678042] [G loss: 0.594959] [AE loss: 1.874226]\n",
            "[Epoch 0/10] [Batch 196/7121] [D loss: 0.676461] [G loss: 0.626738] [AE loss: 1.290327]\n",
            "[Epoch 0/10] [Batch 197/7121] [D loss: 0.702214] [G loss: 0.620346] [AE loss: 1.079690]\n",
            "[Epoch 0/10] [Batch 198/7121] [D loss: 0.720237] [G loss: 0.634885] [AE loss: 0.958465]\n",
            "[Epoch 0/10] [Batch 199/7121] [D loss: 0.723921] [G loss: 0.651491] [AE loss: 1.031684]\n",
            "[Epoch 0/10] [Batch 200/7121] [D loss: 0.712701] [G loss: 0.661426] [AE loss: 1.078877]\n",
            "[Epoch 0/10] [Batch 201/7121] [D loss: 0.692071] [G loss: 0.665939] [AE loss: 1.241059]\n",
            "[Epoch 0/10] [Batch 202/7121] [D loss: 0.695821] [G loss: 0.649503] [AE loss: 1.146991]\n",
            "[Epoch 0/10] [Batch 203/7121] [D loss: 0.694139] [G loss: 0.646682] [AE loss: 1.327581]\n",
            "[Epoch 0/10] [Batch 204/7121] [D loss: 0.725959] [G loss: 0.625873] [AE loss: 0.963139]\n",
            "[Epoch 0/10] [Batch 205/7121] [D loss: 0.746521] [G loss: 0.602580] [AE loss: 1.056813]\n",
            "[Epoch 0/10] [Batch 206/7121] [D loss: 0.788061] [G loss: 0.583897] [AE loss: 1.814587]\n",
            "[Epoch 0/10] [Batch 207/7121] [D loss: 0.784732] [G loss: 0.562612] [AE loss: 1.186392]\n",
            "[Epoch 0/10] [Batch 208/7121] [D loss: 0.782284] [G loss: 0.526970] [AE loss: 1.238140]\n",
            "[Epoch 0/10] [Batch 209/7121] [D loss: 0.835073] [G loss: 0.491903] [AE loss: 1.156363]\n",
            "[Epoch 0/10] [Batch 210/7121] [D loss: 0.844040] [G loss: 0.480411] [AE loss: 1.155661]\n",
            "[Epoch 0/10] [Batch 211/7121] [D loss: 0.854160] [G loss: 0.468557] [AE loss: 1.240540]\n",
            "[Epoch 0/10] [Batch 212/7121] [D loss: 0.863547] [G loss: 0.455122] [AE loss: 1.076876]\n",
            "[Epoch 0/10] [Batch 213/7121] [D loss: 0.845153] [G loss: 0.451677] [AE loss: 0.967915]\n",
            "[Epoch 0/10] [Batch 214/7121] [D loss: 0.869451] [G loss: 0.445896] [AE loss: 1.221568]\n",
            "[Epoch 0/10] [Batch 215/7121] [D loss: 0.832408] [G loss: 0.462332] [AE loss: 1.184152]\n",
            "[Epoch 0/10] [Batch 216/7121] [D loss: 0.844880] [G loss: 0.468787] [AE loss: 1.035615]\n",
            "[Epoch 0/10] [Batch 217/7121] [D loss: 0.802021] [G loss: 0.474230] [AE loss: 1.056600]\n",
            "[Epoch 0/10] [Batch 218/7121] [D loss: 0.803946] [G loss: 0.491689] [AE loss: 1.030632]\n",
            "[Epoch 0/10] [Batch 219/7121] [D loss: 0.777801] [G loss: 0.501877] [AE loss: 2.613160]\n",
            "[Epoch 0/10] [Batch 220/7121] [D loss: 0.763293] [G loss: 0.510633] [AE loss: 1.308418]\n",
            "[Epoch 0/10] [Batch 221/7121] [D loss: 0.775325] [G loss: 0.525760] [AE loss: 1.211137]\n",
            "[Epoch 0/10] [Batch 222/7121] [D loss: 0.770563] [G loss: 0.531954] [AE loss: 1.109619]\n",
            "[Epoch 0/10] [Batch 223/7121] [D loss: 0.748473] [G loss: 0.543371] [AE loss: 1.102537]\n",
            "[Epoch 0/10] [Batch 224/7121] [D loss: 0.748969] [G loss: 0.562313] [AE loss: 1.960682]\n",
            "[Epoch 0/10] [Batch 225/7121] [D loss: 0.741449] [G loss: 0.573541] [AE loss: 1.081907]\n",
            "[Epoch 0/10] [Batch 226/7121] [D loss: 0.732612] [G loss: 0.592076] [AE loss: 1.365073]\n",
            "[Epoch 0/10] [Batch 227/7121] [D loss: 0.737001] [G loss: 0.600279] [AE loss: 1.123148]\n",
            "[Epoch 0/10] [Batch 228/7121] [D loss: 0.709567] [G loss: 0.620309] [AE loss: 1.147064]\n",
            "[Epoch 0/10] [Batch 229/7121] [D loss: 0.695180] [G loss: 0.633611] [AE loss: 2.349790]\n",
            "[Epoch 0/10] [Batch 230/7121] [D loss: 0.689010] [G loss: 0.653748] [AE loss: 1.511177]\n",
            "[Epoch 0/10] [Batch 231/7121] [D loss: 0.681657] [G loss: 0.672801] [AE loss: 1.321755]\n",
            "[Epoch 0/10] [Batch 232/7121] [D loss: 0.683352] [G loss: 0.693630] [AE loss: 2.557594]\n",
            "[Epoch 0/10] [Batch 233/7121] [D loss: 0.672106] [G loss: 0.709603] [AE loss: 1.375802]\n",
            "[Epoch 0/10] [Batch 234/7121] [D loss: 0.654402] [G loss: 0.726903] [AE loss: 1.379450]\n",
            "[Epoch 0/10] [Batch 235/7121] [D loss: 0.661335] [G loss: 0.741040] [AE loss: 1.633450]\n",
            "[Epoch 0/10] [Batch 236/7121] [D loss: 0.663491] [G loss: 0.764994] [AE loss: 1.068346]\n",
            "[Epoch 0/10] [Batch 237/7121] [D loss: 0.649879] [G loss: 0.778100] [AE loss: 1.071445]\n",
            "[Epoch 0/10] [Batch 238/7121] [D loss: 0.630040] [G loss: 0.791416] [AE loss: 1.972283]\n",
            "[Epoch 0/10] [Batch 239/7121] [D loss: 0.627185] [G loss: 0.804633] [AE loss: 1.268334]\n",
            "[Epoch 0/10] [Batch 240/7121] [D loss: 0.614288] [G loss: 0.817476] [AE loss: 1.857255]\n",
            "[Epoch 0/10] [Batch 241/7121] [D loss: 0.641005] [G loss: 0.810540] [AE loss: 1.040466]\n",
            "[Epoch 0/10] [Batch 242/7121] [D loss: 0.634312] [G loss: 0.817227] [AE loss: 1.180284]\n",
            "[Epoch 0/10] [Batch 243/7121] [D loss: 0.632602] [G loss: 0.811984] [AE loss: 1.110504]\n",
            "[Epoch 0/10] [Batch 244/7121] [D loss: 0.630826] [G loss: 0.806833] [AE loss: 1.193505]\n",
            "[Epoch 0/10] [Batch 245/7121] [D loss: 0.632074] [G loss: 0.804293] [AE loss: 0.907659]\n",
            "[Epoch 0/10] [Batch 246/7121] [D loss: 0.634805] [G loss: 0.796447] [AE loss: 1.269879]\n",
            "[Epoch 0/10] [Batch 247/7121] [D loss: 0.628192] [G loss: 0.790188] [AE loss: 0.833703]\n",
            "[Epoch 0/10] [Batch 248/7121] [D loss: 0.644848] [G loss: 0.777715] [AE loss: 1.069328]\n",
            "[Epoch 0/10] [Batch 249/7121] [D loss: 0.647402] [G loss: 0.765316] [AE loss: 0.903593]\n",
            "[Epoch 0/10] [Batch 250/7121] [D loss: 0.645194] [G loss: 0.747870] [AE loss: 1.043382]\n",
            "[Epoch 0/10] [Batch 251/7121] [D loss: 0.653568] [G loss: 0.741296] [AE loss: 0.789100]\n",
            "[Epoch 0/10] [Batch 252/7121] [D loss: 0.654711] [G loss: 0.726049] [AE loss: 1.417593]\n",
            "[Epoch 0/10] [Batch 253/7121] [D loss: 0.639702] [G loss: 0.722675] [AE loss: 1.454371]\n",
            "[Epoch 0/10] [Batch 254/7121] [D loss: 0.649143] [G loss: 0.711271] [AE loss: 1.134719]\n",
            "[Epoch 0/10] [Batch 255/7121] [D loss: 0.645600] [G loss: 0.707545] [AE loss: 1.067409]\n",
            "[Epoch 0/10] [Batch 256/7121] [D loss: 0.665503] [G loss: 0.700301] [AE loss: 1.151503]\n",
            "[Epoch 0/10] [Batch 257/7121] [D loss: 0.658778] [G loss: 0.692076] [AE loss: 0.771282]\n",
            "[Epoch 0/10] [Batch 258/7121] [D loss: 0.651593] [G loss: 0.687313] [AE loss: 0.988021]\n",
            "[Epoch 0/10] [Batch 259/7121] [D loss: 0.649739] [G loss: 0.683231] [AE loss: 8.169630]\n",
            "[Epoch 0/10] [Batch 260/7121] [D loss: 0.651052] [G loss: 0.676765] [AE loss: 0.984643]\n",
            "[Epoch 0/10] [Batch 261/7121] [D loss: 0.659626] [G loss: 0.677863] [AE loss: 0.951401]\n",
            "[Epoch 0/10] [Batch 262/7121] [D loss: 0.637506] [G loss: 0.669959] [AE loss: 1.975002]\n",
            "[Epoch 0/10] [Batch 263/7121] [D loss: 0.634616] [G loss: 0.677279] [AE loss: 0.951332]\n",
            "[Epoch 0/10] [Batch 264/7121] [D loss: 0.671126] [G loss: 0.675154] [AE loss: 1.118813]\n",
            "[Epoch 0/10] [Batch 265/7121] [D loss: 0.631843] [G loss: 0.676778] [AE loss: 0.893790]\n",
            "[Epoch 0/10] [Batch 266/7121] [D loss: 0.600967] [G loss: 0.677487] [AE loss: 1.227535]\n",
            "[Epoch 0/10] [Batch 267/7121] [D loss: 0.650570] [G loss: 0.675690] [AE loss: 0.907903]\n",
            "[Epoch 0/10] [Batch 268/7121] [D loss: 0.623011] [G loss: 0.678481] [AE loss: 0.974089]\n",
            "[Epoch 0/10] [Batch 269/7121] [D loss: 0.609152] [G loss: 0.681513] [AE loss: 1.556414]\n",
            "[Epoch 0/10] [Batch 270/7121] [D loss: 0.647260] [G loss: 0.690010] [AE loss: 0.982046]\n",
            "[Epoch 0/10] [Batch 271/7121] [D loss: 0.587654] [G loss: 0.687665] [AE loss: 1.166322]\n",
            "[Epoch 0/10] [Batch 272/7121] [D loss: 0.572385] [G loss: 0.685409] [AE loss: 1.324423]\n",
            "[Epoch 0/10] [Batch 273/7121] [D loss: 0.637180] [G loss: 0.683871] [AE loss: 1.015657]\n",
            "[Epoch 0/10] [Batch 274/7121] [D loss: 0.609878] [G loss: 0.663810] [AE loss: 0.980253]\n",
            "[Epoch 0/10] [Batch 275/7121] [D loss: 0.615326] [G loss: 0.664997] [AE loss: 1.049145]\n",
            "[Epoch 0/10] [Batch 276/7121] [D loss: 0.612539] [G loss: 0.643846] [AE loss: 1.048053]\n",
            "[Epoch 0/10] [Batch 277/7121] [D loss: 0.609290] [G loss: 0.641650] [AE loss: 1.016202]\n",
            "[Epoch 0/10] [Batch 278/7121] [D loss: 0.613175] [G loss: 0.638659] [AE loss: 1.071545]\n",
            "[Epoch 0/10] [Batch 279/7121] [D loss: 0.629262] [G loss: 0.615991] [AE loss: 0.975060]\n",
            "[Epoch 0/10] [Batch 280/7121] [D loss: 0.628464] [G loss: 0.629899] [AE loss: 0.987447]\n",
            "[Epoch 0/10] [Batch 281/7121] [D loss: 0.588860] [G loss: 0.613798] [AE loss: 1.452551]\n",
            "[Epoch 0/10] [Batch 282/7121] [D loss: 0.621669] [G loss: 0.606318] [AE loss: 0.985656]\n",
            "[Epoch 0/10] [Batch 283/7121] [D loss: 0.624778] [G loss: 0.602693] [AE loss: 1.014854]\n",
            "[Epoch 0/10] [Batch 284/7121] [D loss: 0.587774] [G loss: 0.608531] [AE loss: 1.512427]\n",
            "[Epoch 0/10] [Batch 285/7121] [D loss: 0.593621] [G loss: 0.617521] [AE loss: 1.079408]\n",
            "[Epoch 0/10] [Batch 286/7121] [D loss: 0.606158] [G loss: 0.612873] [AE loss: 1.272035]\n",
            "[Epoch 0/10] [Batch 287/7121] [D loss: 0.606156] [G loss: 0.606889] [AE loss: 0.974755]\n",
            "[Epoch 0/10] [Batch 288/7121] [D loss: 0.597457] [G loss: 0.615998] [AE loss: 1.303671]\n",
            "[Epoch 0/10] [Batch 289/7121] [D loss: 0.617821] [G loss: 0.617891] [AE loss: 1.041913]\n",
            "[Epoch 0/10] [Batch 290/7121] [D loss: 0.597911] [G loss: 0.593713] [AE loss: 2.125332]\n",
            "[Epoch 0/10] [Batch 291/7121] [D loss: 0.598385] [G loss: 0.598004] [AE loss: 1.298502]\n",
            "[Epoch 0/10] [Batch 292/7121] [D loss: 0.602580] [G loss: 0.603012] [AE loss: 1.855100]\n",
            "[Epoch 0/10] [Batch 293/7121] [D loss: 0.635988] [G loss: 0.592294] [AE loss: 1.206114]\n",
            "[Epoch 0/10] [Batch 294/7121] [D loss: 0.670131] [G loss: 0.571501] [AE loss: 1.100115]\n",
            "[Epoch 0/10] [Batch 295/7121] [D loss: 0.694640] [G loss: 0.559399] [AE loss: 1.100754]\n",
            "[Epoch 0/10] [Batch 296/7121] [D loss: 0.633507] [G loss: 0.561484] [AE loss: 1.236977]\n",
            "[Epoch 0/10] [Batch 297/7121] [D loss: 0.670529] [G loss: 0.561570] [AE loss: 2.246346]\n",
            "[Epoch 0/10] [Batch 298/7121] [D loss: 0.673396] [G loss: 0.562896] [AE loss: 1.213359]\n",
            "[Epoch 0/10] [Batch 299/7121] [D loss: 0.690700] [G loss: 0.559590] [AE loss: 1.401606]\n",
            "[Epoch 0/10] [Batch 300/7121] [D loss: 0.708844] [G loss: 0.563031] [AE loss: 1.174324]\n",
            "[Epoch 0/10] [Batch 301/7121] [D loss: 0.706904] [G loss: 0.574292] [AE loss: 1.124844]\n",
            "[Epoch 0/10] [Batch 302/7121] [D loss: 0.676843] [G loss: 0.609380] [AE loss: 1.883718]\n",
            "[Epoch 0/10] [Batch 303/7121] [D loss: 0.700578] [G loss: 0.605375] [AE loss: 1.227767]\n",
            "[Epoch 0/10] [Batch 304/7121] [D loss: 0.697743] [G loss: 0.620486] [AE loss: 1.555965]\n",
            "[Epoch 0/10] [Batch 305/7121] [D loss: 0.635219] [G loss: 0.635572] [AE loss: 5.407351]\n",
            "[Epoch 0/10] [Batch 306/7121] [D loss: 0.663767] [G loss: 0.612935] [AE loss: 3.058896]\n",
            "[Epoch 0/10] [Batch 307/7121] [D loss: 0.720176] [G loss: 0.611148] [AE loss: 1.096272]\n",
            "[Epoch 0/10] [Batch 308/7121] [D loss: 0.676137] [G loss: 0.583761] [AE loss: 1.858698]\n",
            "[Epoch 0/10] [Batch 309/7121] [D loss: 0.746660] [G loss: 0.588883] [AE loss: 0.955543]\n",
            "[Epoch 0/10] [Batch 310/7121] [D loss: 0.762902] [G loss: 0.542380] [AE loss: 1.817580]\n",
            "[Epoch 0/10] [Batch 311/7121] [D loss: 0.791581] [G loss: 0.522180] [AE loss: 1.103128]\n",
            "[Epoch 0/10] [Batch 312/7121] [D loss: 0.877259] [G loss: 0.497020] [AE loss: 1.120909]\n",
            "[Epoch 0/10] [Batch 313/7121] [D loss: 0.893091] [G loss: 0.484371] [AE loss: 1.118075]\n",
            "[Epoch 0/10] [Batch 314/7121] [D loss: 0.886883] [G loss: 0.466436] [AE loss: 1.109175]\n",
            "[Epoch 0/10] [Batch 315/7121] [D loss: 0.973911] [G loss: 0.427818] [AE loss: 1.140157]\n",
            "[Epoch 0/10] [Batch 316/7121] [D loss: 0.968949] [G loss: 0.407864] [AE loss: 1.567179]\n",
            "[Epoch 0/10] [Batch 317/7121] [D loss: 0.949629] [G loss: 0.410289] [AE loss: 1.124078]\n",
            "[Epoch 0/10] [Batch 318/7121] [D loss: 0.911912] [G loss: 0.424850] [AE loss: 1.238636]\n",
            "[Epoch 0/10] [Batch 319/7121] [D loss: 0.978478] [G loss: 0.452837] [AE loss: 1.281567]\n",
            "[Epoch 0/10] [Batch 320/7121] [D loss: 0.871110] [G loss: 0.467197] [AE loss: 1.379101]\n",
            "[Epoch 0/10] [Batch 321/7121] [D loss: 0.964579] [G loss: 0.495364] [AE loss: 1.249892]\n",
            "[Epoch 0/10] [Batch 322/7121] [D loss: 0.960967] [G loss: 0.529516] [AE loss: 2.366446]\n",
            "[Epoch 0/10] [Batch 323/7121] [D loss: 0.929418] [G loss: 0.555020] [AE loss: 1.168930]\n",
            "[Epoch 0/10] [Batch 324/7121] [D loss: 0.808448] [G loss: 0.590977] [AE loss: 1.221087]\n",
            "[Epoch 0/10] [Batch 325/7121] [D loss: 0.839631] [G loss: 0.633430] [AE loss: 1.209629]\n",
            "[Epoch 0/10] [Batch 326/7121] [D loss: 0.827029] [G loss: 0.644303] [AE loss: 1.764349]\n",
            "[Epoch 0/10] [Batch 327/7121] [D loss: 0.788023] [G loss: 0.655039] [AE loss: 1.214320]\n",
            "[Epoch 0/10] [Batch 328/7121] [D loss: 0.808694] [G loss: 0.674350] [AE loss: 1.109874]\n",
            "[Epoch 0/10] [Batch 329/7121] [D loss: 0.781388] [G loss: 0.668545] [AE loss: 1.190321]\n",
            "[Epoch 0/10] [Batch 330/7121] [D loss: 0.820228] [G loss: 0.675179] [AE loss: 1.215794]\n",
            "[Epoch 0/10] [Batch 331/7121] [D loss: 0.773903] [G loss: 0.673781] [AE loss: 1.136108]\n",
            "[Epoch 0/10] [Batch 332/7121] [D loss: 0.791063] [G loss: 0.675124] [AE loss: 2.517608]\n",
            "[Epoch 0/10] [Batch 333/7121] [D loss: 0.838514] [G loss: 0.693018] [AE loss: 1.184673]\n",
            "[Epoch 0/10] [Batch 334/7121] [D loss: 0.813283] [G loss: 0.673413] [AE loss: 1.527336]\n",
            "[Epoch 0/10] [Batch 335/7121] [D loss: 0.781227] [G loss: 0.701961] [AE loss: 1.128254]\n",
            "[Epoch 0/10] [Batch 336/7121] [D loss: 0.822003] [G loss: 0.693844] [AE loss: 2.315609]\n",
            "[Epoch 0/10] [Batch 337/7121] [D loss: 0.858980] [G loss: 0.718248] [AE loss: 1.497970]\n",
            "[Epoch 0/10] [Batch 338/7121] [D loss: 0.813604] [G loss: 0.733984] [AE loss: 1.426024]\n",
            "[Epoch 0/10] [Batch 339/7121] [D loss: 0.820796] [G loss: 0.759421] [AE loss: 1.640385]\n",
            "[Epoch 0/10] [Batch 340/7121] [D loss: 0.781769] [G loss: 0.780170] [AE loss: 1.563225]\n",
            "[Epoch 0/10] [Batch 341/7121] [D loss: 0.775002] [G loss: 0.803153] [AE loss: 1.107558]\n",
            "[Epoch 0/10] [Batch 342/7121] [D loss: 0.732228] [G loss: 0.838276] [AE loss: 2.429643]\n",
            "[Epoch 0/10] [Batch 343/7121] [D loss: 0.699193] [G loss: 0.851285] [AE loss: 2.508159]\n",
            "[Epoch 0/10] [Batch 344/7121] [D loss: 0.661247] [G loss: 0.876201] [AE loss: 1.387107]\n",
            "[Epoch 0/10] [Batch 345/7121] [D loss: 0.744078] [G loss: 0.891911] [AE loss: 1.091197]\n",
            "[Epoch 0/10] [Batch 346/7121] [D loss: 0.676651] [G loss: 0.899398] [AE loss: 1.030346]\n",
            "[Epoch 0/10] [Batch 347/7121] [D loss: 0.712364] [G loss: 0.906063] [AE loss: 1.142849]\n",
            "[Epoch 0/10] [Batch 348/7121] [D loss: 0.661876] [G loss: 0.904537] [AE loss: 1.057500]\n",
            "[Epoch 0/10] [Batch 349/7121] [D loss: 0.668261] [G loss: 0.899911] [AE loss: 1.078478]\n",
            "[Epoch 0/10] [Batch 350/7121] [D loss: 0.686457] [G loss: 0.881493] [AE loss: 1.100727]\n",
            "[Epoch 0/10] [Batch 351/7121] [D loss: 0.687301] [G loss: 0.867489] [AE loss: 1.064730]\n",
            "[Epoch 0/10] [Batch 352/7121] [D loss: 0.698378] [G loss: 0.852157] [AE loss: 1.548252]\n",
            "[Epoch 0/10] [Batch 353/7121] [D loss: 0.713613] [G loss: 0.809198] [AE loss: 0.988660]\n",
            "[Epoch 0/10] [Batch 354/7121] [D loss: 0.657507] [G loss: 0.798594] [AE loss: 1.260145]\n",
            "[Epoch 0/10] [Batch 355/7121] [D loss: 0.709602] [G loss: 0.773966] [AE loss: 1.008880]\n",
            "[Epoch 0/10] [Batch 356/7121] [D loss: 0.667033] [G loss: 0.769282] [AE loss: 0.873320]\n",
            "[Epoch 0/10] [Batch 357/7121] [D loss: 0.674888] [G loss: 0.754047] [AE loss: 0.935766]\n",
            "[Epoch 0/10] [Batch 358/7121] [D loss: 0.670005] [G loss: 0.767562] [AE loss: 1.072396]\n",
            "[Epoch 0/10] [Batch 359/7121] [D loss: 0.658486] [G loss: 0.759080] [AE loss: 0.972727]\n",
            "[Epoch 0/10] [Batch 360/7121] [D loss: 0.664319] [G loss: 0.765863] [AE loss: 1.173724]\n",
            "[Epoch 0/10] [Batch 361/7121] [D loss: 0.677307] [G loss: 0.771893] [AE loss: 1.112533]\n",
            "[Epoch 0/10] [Batch 362/7121] [D loss: 0.660087] [G loss: 0.768981] [AE loss: 1.089933]\n",
            "[Epoch 0/10] [Batch 363/7121] [D loss: 0.611402] [G loss: 0.781828] [AE loss: 1.461534]\n",
            "[Epoch 0/10] [Batch 364/7121] [D loss: 0.618640] [G loss: 0.799838] [AE loss: 1.542776]\n",
            "[Epoch 0/10] [Batch 365/7121] [D loss: 0.588066] [G loss: 0.805515] [AE loss: 1.565831]\n",
            "[Epoch 0/10] [Batch 366/7121] [D loss: 0.609756] [G loss: 0.823103] [AE loss: 1.042999]\n",
            "[Epoch 0/10] [Batch 367/7121] [D loss: 0.624245] [G loss: 0.829908] [AE loss: 1.009546]\n",
            "[Epoch 0/10] [Batch 368/7121] [D loss: 0.595408] [G loss: 0.843736] [AE loss: 0.893335]\n",
            "[Epoch 0/10] [Batch 369/7121] [D loss: 0.581231] [G loss: 0.859665] [AE loss: 0.962293]\n",
            "[Epoch 0/10] [Batch 370/7121] [D loss: 0.562789] [G loss: 0.864968] [AE loss: 1.056247]\n",
            "[Epoch 0/10] [Batch 371/7121] [D loss: 0.577225] [G loss: 0.884380] [AE loss: 1.206387]\n",
            "[Epoch 0/10] [Batch 372/7121] [D loss: 0.570516] [G loss: 0.889326] [AE loss: 1.401074]\n",
            "[Epoch 0/10] [Batch 373/7121] [D loss: 0.537475] [G loss: 0.901669] [AE loss: 1.527525]\n",
            "[Epoch 0/10] [Batch 374/7121] [D loss: 0.567893] [G loss: 0.890451] [AE loss: 1.059947]\n",
            "[Epoch 0/10] [Batch 375/7121] [D loss: 0.594401] [G loss: 0.888252] [AE loss: 1.013450]\n",
            "[Epoch 0/10] [Batch 376/7121] [D loss: 0.574089] [G loss: 0.875064] [AE loss: 0.998536]\n",
            "[Epoch 0/10] [Batch 377/7121] [D loss: 0.580490] [G loss: 0.865446] [AE loss: 0.949492]\n",
            "[Epoch 0/10] [Batch 378/7121] [D loss: 0.548380] [G loss: 0.834887] [AE loss: 1.239199]\n",
            "[Epoch 0/10] [Batch 379/7121] [D loss: 0.521567] [G loss: 0.811941] [AE loss: 1.547931]\n",
            "[Epoch 0/10] [Batch 380/7121] [D loss: 0.588069] [G loss: 0.767092] [AE loss: 1.041340]\n",
            "[Epoch 0/10] [Batch 381/7121] [D loss: 0.593270] [G loss: 0.735768] [AE loss: 1.179466]\n",
            "[Epoch 0/10] [Batch 382/7121] [D loss: 0.593291] [G loss: 0.700633] [AE loss: 1.433695]\n",
            "[Epoch 0/10] [Batch 383/7121] [D loss: 0.658633] [G loss: 0.639012] [AE loss: 0.951248]\n",
            "[Epoch 0/10] [Batch 384/7121] [D loss: 0.595732] [G loss: 0.608661] [AE loss: 1.989363]\n",
            "[Epoch 0/10] [Batch 385/7121] [D loss: 0.709309] [G loss: 0.591256] [AE loss: 1.044949]\n",
            "[Epoch 0/10] [Batch 386/7121] [D loss: 0.706182] [G loss: 0.578514] [AE loss: 1.320902]\n",
            "[Epoch 0/10] [Batch 387/7121] [D loss: 0.664077] [G loss: 0.559542] [AE loss: 1.419072]\n",
            "[Epoch 0/10] [Batch 388/7121] [D loss: 0.679778] [G loss: 0.537656] [AE loss: 1.158270]\n",
            "[Epoch 0/10] [Batch 389/7121] [D loss: 0.662924] [G loss: 0.553862] [AE loss: 1.641533]\n",
            "[Epoch 0/10] [Batch 390/7121] [D loss: 0.660832] [G loss: 0.544839] [AE loss: 1.638392]\n",
            "[Epoch 0/10] [Batch 391/7121] [D loss: 0.679824] [G loss: 0.549266] [AE loss: 1.062219]\n",
            "[Epoch 0/10] [Batch 392/7121] [D loss: 0.646477] [G loss: 0.583312] [AE loss: 1.425911]\n",
            "[Epoch 0/10] [Batch 393/7121] [D loss: 0.659523] [G loss: 0.580277] [AE loss: 1.174418]\n",
            "[Epoch 0/10] [Batch 394/7121] [D loss: 0.639324] [G loss: 0.609689] [AE loss: 1.096432]\n",
            "[Epoch 0/10] [Batch 395/7121] [D loss: 0.635767] [G loss: 0.619331] [AE loss: 1.072273]\n",
            "[Epoch 0/10] [Batch 396/7121] [D loss: 0.638333] [G loss: 0.632690] [AE loss: 1.859580]\n",
            "[Epoch 0/10] [Batch 397/7121] [D loss: 0.610259] [G loss: 0.677626] [AE loss: 1.259232]\n",
            "[Epoch 0/10] [Batch 398/7121] [D loss: 0.581256] [G loss: 0.702650] [AE loss: 1.091705]\n",
            "[Epoch 0/10] [Batch 399/7121] [D loss: 0.557771] [G loss: 0.728946] [AE loss: 1.195938]\n",
            "[Epoch 0/10] [Batch 400/7121] [D loss: 0.590143] [G loss: 0.758785] [AE loss: 2.100320]\n",
            "[Epoch 0/10] [Batch 401/7121] [D loss: 0.552269] [G loss: 0.736434] [AE loss: 0.868806]\n",
            "[Epoch 0/10] [Batch 402/7121] [D loss: 0.635096] [G loss: 0.600326] [AE loss: 1.149863]\n",
            "[Epoch 0/10] [Batch 403/7121] [D loss: 0.757743] [G loss: 0.418811] [AE loss: 3.790897]\n",
            "[Epoch 0/10] [Batch 404/7121] [D loss: 0.823019] [G loss: 0.362488] [AE loss: 2.489651]\n",
            "[Epoch 0/10] [Batch 405/7121] [D loss: 0.848456] [G loss: 0.343914] [AE loss: 1.162384]\n",
            "[Epoch 0/10] [Batch 406/7121] [D loss: 0.915214] [G loss: 0.335825] [AE loss: 1.297043]\n",
            "[Epoch 0/10] [Batch 407/7121] [D loss: 0.896587] [G loss: 0.347737] [AE loss: 0.989046]\n",
            "[Epoch 0/10] [Batch 408/7121] [D loss: 0.914236] [G loss: 0.332628] [AE loss: 1.419401]\n",
            "[Epoch 0/10] [Batch 409/7121] [D loss: 0.833781] [G loss: 0.340205] [AE loss: 1.203181]\n",
            "[Epoch 0/10] [Batch 410/7121] [D loss: 0.889173] [G loss: 0.328692] [AE loss: 1.180508]\n",
            "[Epoch 0/10] [Batch 411/7121] [D loss: 0.890649] [G loss: 0.329176] [AE loss: 1.310924]\n",
            "[Epoch 0/10] [Batch 412/7121] [D loss: 0.912837] [G loss: 0.333171] [AE loss: 1.023456]\n",
            "[Epoch 0/10] [Batch 413/7121] [D loss: 0.894421] [G loss: 0.332494] [AE loss: 1.142504]\n",
            "[Epoch 0/10] [Batch 414/7121] [D loss: 0.903872] [G loss: 0.319908] [AE loss: 1.274872]\n",
            "[Epoch 0/10] [Batch 415/7121] [D loss: 0.889386] [G loss: 0.345160] [AE loss: 1.067259]\n",
            "[Epoch 0/10] [Batch 416/7121] [D loss: 0.871435] [G loss: 0.349279] [AE loss: 1.007050]\n",
            "[Epoch 0/10] [Batch 417/7121] [D loss: 0.841780] [G loss: 0.359747] [AE loss: 1.511926]\n",
            "[Epoch 0/10] [Batch 418/7121] [D loss: 0.847348] [G loss: 0.374797] [AE loss: 0.921746]\n",
            "[Epoch 0/10] [Batch 419/7121] [D loss: 0.807059] [G loss: 0.382757] [AE loss: 2.105377]\n",
            "[Epoch 0/10] [Batch 420/7121] [D loss: 0.831691] [G loss: 0.405870] [AE loss: 1.150823]\n",
            "[Epoch 0/10] [Batch 421/7121] [D loss: 0.827264] [G loss: 0.425708] [AE loss: 1.217186]\n",
            "[Epoch 0/10] [Batch 422/7121] [D loss: 0.779546] [G loss: 0.445301] [AE loss: 1.620396]\n",
            "[Epoch 0/10] [Batch 423/7121] [D loss: 0.762998] [G loss: 0.465101] [AE loss: 1.052239]\n",
            "[Epoch 0/10] [Batch 424/7121] [D loss: 0.738312] [G loss: 0.497349] [AE loss: 1.721669]\n",
            "[Epoch 0/10] [Batch 425/7121] [D loss: 0.738556] [G loss: 0.506552] [AE loss: 1.162449]\n",
            "[Epoch 0/10] [Batch 426/7121] [D loss: 0.700736] [G loss: 0.531396] [AE loss: 1.534807]\n",
            "[Epoch 0/10] [Batch 427/7121] [D loss: 0.725651] [G loss: 0.548881] [AE loss: 1.102347]\n",
            "[Epoch 0/10] [Batch 428/7121] [D loss: 0.680002] [G loss: 0.574250] [AE loss: 1.187343]\n",
            "[Epoch 0/10] [Batch 429/7121] [D loss: 0.667262] [G loss: 0.590287] [AE loss: 1.296697]\n",
            "[Epoch 0/10] [Batch 430/7121] [D loss: 0.692965] [G loss: 0.606631] [AE loss: 1.644872]\n",
            "[Epoch 0/10] [Batch 431/7121] [D loss: 0.639133] [G loss: 0.628876] [AE loss: 1.234814]\n",
            "[Epoch 0/10] [Batch 432/7121] [D loss: 0.663534] [G loss: 0.640853] [AE loss: 1.181439]\n",
            "[Epoch 0/10] [Batch 433/7121] [D loss: 0.600845] [G loss: 0.657048] [AE loss: 1.523563]\n",
            "[Epoch 0/10] [Batch 434/7121] [D loss: 0.640835] [G loss: 0.666203] [AE loss: 1.081259]\n",
            "[Epoch 0/10] [Batch 435/7121] [D loss: 0.623340] [G loss: 0.679115] [AE loss: 1.458611]\n",
            "[Epoch 0/10] [Batch 436/7121] [D loss: 0.646645] [G loss: 0.693132] [AE loss: 1.043993]\n",
            "[Epoch 0/10] [Batch 437/7121] [D loss: 0.614888] [G loss: 0.696074] [AE loss: 1.183874]\n",
            "[Epoch 0/10] [Batch 438/7121] [D loss: 0.597253] [G loss: 0.703525] [AE loss: 1.067765]\n",
            "[Epoch 0/10] [Batch 439/7121] [D loss: 0.619300] [G loss: 0.714214] [AE loss: 1.228075]\n",
            "[Epoch 0/10] [Batch 440/7121] [D loss: 0.607970] [G loss: 0.719010] [AE loss: 0.937359]\n",
            "[Epoch 0/10] [Batch 441/7121] [D loss: 0.624585] [G loss: 0.722703] [AE loss: 0.876669]\n",
            "[Epoch 0/10] [Batch 442/7121] [D loss: 0.615593] [G loss: 0.725931] [AE loss: 1.019370]\n",
            "[Epoch 0/10] [Batch 443/7121] [D loss: 0.570933] [G loss: 0.728326] [AE loss: 1.348476]\n",
            "[Epoch 0/10] [Batch 444/7121] [D loss: 0.569677] [G loss: 0.736251] [AE loss: 0.929923]\n",
            "[Epoch 0/10] [Batch 445/7121] [D loss: 0.582421] [G loss: 0.735635] [AE loss: 1.133236]\n",
            "[Epoch 0/10] [Batch 446/7121] [D loss: 0.583951] [G loss: 0.739296] [AE loss: 1.163116]\n",
            "[Epoch 0/10] [Batch 447/7121] [D loss: 0.576994] [G loss: 0.744193] [AE loss: 1.294158]\n",
            "[Epoch 0/10] [Batch 448/7121] [D loss: 0.583282] [G loss: 0.748711] [AE loss: 1.336242]\n",
            "[Epoch 0/10] [Batch 449/7121] [D loss: 0.594759] [G loss: 0.742544] [AE loss: 1.154898]\n",
            "[Epoch 0/10] [Batch 450/7121] [D loss: 0.561291] [G loss: 0.748068] [AE loss: 1.179682]\n",
            "[Epoch 0/10] [Batch 451/7121] [D loss: 0.598736] [G loss: 0.743406] [AE loss: 1.097433]\n",
            "[Epoch 0/10] [Batch 452/7121] [D loss: 0.574781] [G loss: 0.744210] [AE loss: 1.080569]\n",
            "[Epoch 0/10] [Batch 453/7121] [D loss: 0.587537] [G loss: 0.749933] [AE loss: 0.951799]\n",
            "[Epoch 0/10] [Batch 454/7121] [D loss: 0.570834] [G loss: 0.746803] [AE loss: 0.958839]\n",
            "[Epoch 0/10] [Batch 455/7121] [D loss: 0.592866] [G loss: 0.745209] [AE loss: 0.960241]\n",
            "[Epoch 0/10] [Batch 456/7121] [D loss: 0.579084] [G loss: 0.750545] [AE loss: 1.098558]\n",
            "[Epoch 0/10] [Batch 457/7121] [D loss: 0.551829] [G loss: 0.749345] [AE loss: 1.376118]\n",
            "[Epoch 0/10] [Batch 458/7121] [D loss: 0.567516] [G loss: 0.742183] [AE loss: 0.822212]\n",
            "[Epoch 0/10] [Batch 459/7121] [D loss: 0.536621] [G loss: 0.751329] [AE loss: 1.097800]\n",
            "[Epoch 0/10] [Batch 460/7121] [D loss: 0.531907] [G loss: 0.751762] [AE loss: 1.041740]\n",
            "[Epoch 0/10] [Batch 461/7121] [D loss: 0.528451] [G loss: 0.753028] [AE loss: 1.570038]\n",
            "[Epoch 0/10] [Batch 462/7121] [D loss: 0.568602] [G loss: 0.749735] [AE loss: 0.896521]\n",
            "[Epoch 0/10] [Batch 463/7121] [D loss: 0.529294] [G loss: 0.755441] [AE loss: 1.078853]\n",
            "[Epoch 0/10] [Batch 464/7121] [D loss: 0.546044] [G loss: 0.755208] [AE loss: 0.886757]\n",
            "[Epoch 0/10] [Batch 465/7121] [D loss: 0.538204] [G loss: 0.750727] [AE loss: 0.988681]\n",
            "[Epoch 0/10] [Batch 466/7121] [D loss: 0.543340] [G loss: 0.755773] [AE loss: 1.265473]\n",
            "[Epoch 0/10] [Batch 467/7121] [D loss: 0.531254] [G loss: 0.752931] [AE loss: 2.381317]\n",
            "[Epoch 0/10] [Batch 468/7121] [D loss: 0.558829] [G loss: 0.745124] [AE loss: 0.938147]\n",
            "[Epoch 0/10] [Batch 469/7121] [D loss: 0.550635] [G loss: 0.744968] [AE loss: 0.807136]\n",
            "[Epoch 0/10] [Batch 470/7121] [D loss: 0.512082] [G loss: 0.741361] [AE loss: 0.908171]\n",
            "[Epoch 0/10] [Batch 471/7121] [D loss: 0.520135] [G loss: 0.740846] [AE loss: 1.390622]\n",
            "[Epoch 0/10] [Batch 472/7121] [D loss: 0.553220] [G loss: 0.713951] [AE loss: 0.969918]\n",
            "[Epoch 0/10] [Batch 473/7121] [D loss: 0.527208] [G loss: 0.714288] [AE loss: 1.108919]\n",
            "[Epoch 0/10] [Batch 474/7121] [D loss: 0.578703] [G loss: 0.691724] [AE loss: 1.079621]\n",
            "[Epoch 0/10] [Batch 475/7121] [D loss: 0.561767] [G loss: 0.673670] [AE loss: 1.232194]\n",
            "[Epoch 0/10] [Batch 476/7121] [D loss: 0.605102] [G loss: 0.634888] [AE loss: 0.912169]\n",
            "[Epoch 0/10] [Batch 477/7121] [D loss: 0.609598] [G loss: 0.622080] [AE loss: 1.200354]\n",
            "[Epoch 0/10] [Batch 478/7121] [D loss: 0.573500] [G loss: 0.599788] [AE loss: 1.143236]\n",
            "[Epoch 0/10] [Batch 479/7121] [D loss: 0.587978] [G loss: 0.582364] [AE loss: 1.146157]\n",
            "[Epoch 0/10] [Batch 480/7121] [D loss: 0.648009] [G loss: 0.550844] [AE loss: 1.096487]\n",
            "[Epoch 0/10] [Batch 481/7121] [D loss: 0.669798] [G loss: 0.545338] [AE loss: 0.986574]\n",
            "[Epoch 0/10] [Batch 482/7121] [D loss: 0.657277] [G loss: 0.538860] [AE loss: 1.377432]\n",
            "[Epoch 0/10] [Batch 483/7121] [D loss: 0.683641] [G loss: 0.550195] [AE loss: 1.403464]\n",
            "[Epoch 0/10] [Batch 484/7121] [D loss: 0.655825] [G loss: 0.569505] [AE loss: 1.225769]\n",
            "[Epoch 0/10] [Batch 485/7121] [D loss: 0.662971] [G loss: 0.585043] [AE loss: 1.318556]\n",
            "[Epoch 0/10] [Batch 486/7121] [D loss: 0.653521] [G loss: 0.590747] [AE loss: 1.447702]\n",
            "[Epoch 0/10] [Batch 487/7121] [D loss: 0.649532] [G loss: 0.601683] [AE loss: 2.624725]\n",
            "[Epoch 0/10] [Batch 488/7121] [D loss: 0.651699] [G loss: 0.626406] [AE loss: 1.201951]\n",
            "[Epoch 0/10] [Batch 489/7121] [D loss: 0.613306] [G loss: 0.645506] [AE loss: 1.299984]\n",
            "[Epoch 0/10] [Batch 490/7121] [D loss: 0.682652] [G loss: 0.663631] [AE loss: 0.969806]\n",
            "[Epoch 0/10] [Batch 491/7121] [D loss: 0.629394] [G loss: 0.680892] [AE loss: 1.058607]\n",
            "[Epoch 0/10] [Batch 492/7121] [D loss: 0.622214] [G loss: 0.697243] [AE loss: 1.197886]\n",
            "[Epoch 0/10] [Batch 493/7121] [D loss: 0.603322] [G loss: 0.705883] [AE loss: 1.151158]\n",
            "[Epoch 0/10] [Batch 494/7121] [D loss: 0.676770] [G loss: 0.734698] [AE loss: 1.200891]\n",
            "[Epoch 0/10] [Batch 495/7121] [D loss: 0.628681] [G loss: 0.747484] [AE loss: 1.250190]\n",
            "[Epoch 0/10] [Batch 496/7121] [D loss: 0.657677] [G loss: 0.738995] [AE loss: 1.397808]\n",
            "[Epoch 0/10] [Batch 497/7121] [D loss: 0.618783] [G loss: 0.727573] [AE loss: 1.502734]\n",
            "[Epoch 0/10] [Batch 498/7121] [D loss: 0.637263] [G loss: 0.727961] [AE loss: 2.487927]\n",
            "[Epoch 0/10] [Batch 499/7121] [D loss: 0.666527] [G loss: 0.702675] [AE loss: 1.361419]\n",
            "[Epoch 0/10] [Batch 500/7121] [D loss: 0.720039] [G loss: 0.688453] [AE loss: 1.434931]\n",
            "[Epoch 0/10] [Batch 501/7121] [D loss: 0.719647] [G loss: 0.683335] [AE loss: 1.934770]\n",
            "[Epoch 0/10] [Batch 502/7121] [D loss: 0.707722] [G loss: 0.673920] [AE loss: 2.255454]\n",
            "[Epoch 0/10] [Batch 503/7121] [D loss: 0.706740] [G loss: 0.692275] [AE loss: 1.673775]\n",
            "[Epoch 0/10] [Batch 504/7121] [D loss: 0.723638] [G loss: 0.678393] [AE loss: 1.412876]\n",
            "[Epoch 0/10] [Batch 505/7121] [D loss: 0.675029] [G loss: 0.715098] [AE loss: 1.769028]\n",
            "[Epoch 0/10] [Batch 506/7121] [D loss: 0.694253] [G loss: 0.679521] [AE loss: 1.464672]\n",
            "[Epoch 0/10] [Batch 507/7121] [D loss: 0.721553] [G loss: 0.732423] [AE loss: 1.520898]\n",
            "[Epoch 0/10] [Batch 508/7121] [D loss: 0.700954] [G loss: 0.745183] [AE loss: 1.368876]\n",
            "[Epoch 0/10] [Batch 509/7121] [D loss: 0.741442] [G loss: 0.768629] [AE loss: 1.627261]\n",
            "[Epoch 0/10] [Batch 510/7121] [D loss: 0.725503] [G loss: 0.747821] [AE loss: 1.523535]\n",
            "[Epoch 0/10] [Batch 511/7121] [D loss: 0.680103] [G loss: 0.811539] [AE loss: 1.503700]\n",
            "[Epoch 0/10] [Batch 512/7121] [D loss: 0.715564] [G loss: 0.791199] [AE loss: 1.365770]\n",
            "[Epoch 0/10] [Batch 513/7121] [D loss: 0.718093] [G loss: 0.798719] [AE loss: 1.356457]\n",
            "[Epoch 0/10] [Batch 514/7121] [D loss: 0.714074] [G loss: 0.794675] [AE loss: 1.321310]\n",
            "[Epoch 0/10] [Batch 515/7121] [D loss: 0.718863] [G loss: 0.804063] [AE loss: 1.303587]\n",
            "[Epoch 0/10] [Batch 516/7121] [D loss: 0.718737] [G loss: 0.810274] [AE loss: 1.237248]\n",
            "[Epoch 0/10] [Batch 517/7121] [D loss: 0.702777] [G loss: 0.774975] [AE loss: 1.389495]\n",
            "[Epoch 0/10] [Batch 518/7121] [D loss: 0.655632] [G loss: 0.800480] [AE loss: 1.586741]\n",
            "[Epoch 0/10] [Batch 519/7121] [D loss: 0.659711] [G loss: 0.757260] [AE loss: 1.979589]\n",
            "[Epoch 0/10] [Batch 520/7121] [D loss: 0.730826] [G loss: 0.745444] [AE loss: 1.478881]\n",
            "[Epoch 0/10] [Batch 521/7121] [D loss: 0.713373] [G loss: 0.748989] [AE loss: 4.058306]\n",
            "[Epoch 0/10] [Batch 522/7121] [D loss: 0.751162] [G loss: 0.722081] [AE loss: 1.432911]\n",
            "[Epoch 0/10] [Batch 523/7121] [D loss: 0.761749] [G loss: 0.705425] [AE loss: 1.370868]\n",
            "[Epoch 0/10] [Batch 524/7121] [D loss: 0.814076] [G loss: 0.741119] [AE loss: 1.443447]\n",
            "[Epoch 0/10] [Batch 525/7121] [D loss: 0.757932] [G loss: 0.707862] [AE loss: 1.474175]\n",
            "[Epoch 0/10] [Batch 526/7121] [D loss: 0.707998] [G loss: 0.707525] [AE loss: 1.694958]\n",
            "[Epoch 0/10] [Batch 527/7121] [D loss: 0.878514] [G loss: 0.699927] [AE loss: 1.601952]\n",
            "[Epoch 0/10] [Batch 528/7121] [D loss: 0.750804] [G loss: 0.708557] [AE loss: 1.507308]\n",
            "[Epoch 0/10] [Batch 529/7121] [D loss: 0.676081] [G loss: 0.753157] [AE loss: 1.406685]\n",
            "[Epoch 0/10] [Batch 530/7121] [D loss: 0.656854] [G loss: 0.795744] [AE loss: 1.325882]\n",
            "[Epoch 0/10] [Batch 531/7121] [D loss: 0.691444] [G loss: 0.800843] [AE loss: 1.445025]\n",
            "[Epoch 0/10] [Batch 532/7121] [D loss: 0.685565] [G loss: 0.795351] [AE loss: 1.273286]\n",
            "[Epoch 0/10] [Batch 533/7121] [D loss: 0.574672] [G loss: 0.815268] [AE loss: 1.693793]\n",
            "[Epoch 0/10] [Batch 534/7121] [D loss: 0.639133] [G loss: 0.792068] [AE loss: 1.437267]\n",
            "[Epoch 0/10] [Batch 535/7121] [D loss: 0.679813] [G loss: 0.775806] [AE loss: 1.495738]\n",
            "[Epoch 0/10] [Batch 536/7121] [D loss: 0.722044] [G loss: 0.769831] [AE loss: 2.365618]\n",
            "[Epoch 0/10] [Batch 537/7121] [D loss: 0.758224] [G loss: 0.727363] [AE loss: 1.866582]\n",
            "[Epoch 0/10] [Batch 538/7121] [D loss: 0.719464] [G loss: 0.705780] [AE loss: 1.832434]\n",
            "[Epoch 0/10] [Batch 539/7121] [D loss: 0.710169] [G loss: 0.682404] [AE loss: 1.397194]\n",
            "[Epoch 0/10] [Batch 540/7121] [D loss: 0.759104] [G loss: 0.635019] [AE loss: 1.432632]\n",
            "[Epoch 0/10] [Batch 541/7121] [D loss: 0.772224] [G loss: 0.610350] [AE loss: 1.360657]\n",
            "[Epoch 0/10] [Batch 542/7121] [D loss: 0.828493] [G loss: 0.575491] [AE loss: 1.426562]\n",
            "[Epoch 0/10] [Batch 543/7121] [D loss: 0.746849] [G loss: 0.567436] [AE loss: 2.781228]\n",
            "[Epoch 0/10] [Batch 544/7121] [D loss: 0.758333] [G loss: 0.575150] [AE loss: 1.438865]\n",
            "[Epoch 0/10] [Batch 545/7121] [D loss: 0.783908] [G loss: 0.561830] [AE loss: 1.342555]\n",
            "[Epoch 0/10] [Batch 546/7121] [D loss: 0.813487] [G loss: 0.546856] [AE loss: 1.297696]\n",
            "[Epoch 0/10] [Batch 547/7121] [D loss: 0.730817] [G loss: 0.567431] [AE loss: 1.691863]\n",
            "[Epoch 0/10] [Batch 548/7121] [D loss: 0.782062] [G loss: 0.567540] [AE loss: 1.393414]\n",
            "[Epoch 0/10] [Batch 549/7121] [D loss: 0.683943] [G loss: 0.559518] [AE loss: 1.397514]\n",
            "[Epoch 0/10] [Batch 550/7121] [D loss: 0.749165] [G loss: 0.570763] [AE loss: 1.264120]\n",
            "[Epoch 0/10] [Batch 551/7121] [D loss: 0.773000] [G loss: 0.590822] [AE loss: 1.164193]\n",
            "[Epoch 0/10] [Batch 552/7121] [D loss: 0.736966] [G loss: 0.567240] [AE loss: 1.494285]\n",
            "[Epoch 0/10] [Batch 553/7121] [D loss: 0.740588] [G loss: 0.581523] [AE loss: 1.893152]\n",
            "[Epoch 0/10] [Batch 554/7121] [D loss: 0.750936] [G loss: 0.603993] [AE loss: 3.895508]\n",
            "[Epoch 0/10] [Batch 555/7121] [D loss: 0.681884] [G loss: 0.622987] [AE loss: 1.617976]\n",
            "[Epoch 0/10] [Batch 556/7121] [D loss: 0.729898] [G loss: 0.641414] [AE loss: 1.331806]\n",
            "[Epoch 0/10] [Batch 557/7121] [D loss: 0.715178] [G loss: 0.662350] [AE loss: 1.200161]\n",
            "[Epoch 0/10] [Batch 558/7121] [D loss: 0.687502] [G loss: 0.666444] [AE loss: 1.292298]\n",
            "[Epoch 0/10] [Batch 559/7121] [D loss: 0.678782] [G loss: 0.693235] [AE loss: 1.344674]\n",
            "[Epoch 0/10] [Batch 560/7121] [D loss: 0.660552] [G loss: 0.713788] [AE loss: 1.309742]\n",
            "[Epoch 0/10] [Batch 561/7121] [D loss: 0.672761] [G loss: 0.708990] [AE loss: 1.203874]\n",
            "[Epoch 0/10] [Batch 562/7121] [D loss: 0.639500] [G loss: 0.730486] [AE loss: 1.229540]\n",
            "[Epoch 0/10] [Batch 563/7121] [D loss: 0.634070] [G loss: 0.761032] [AE loss: 1.784119]\n",
            "[Epoch 0/10] [Batch 564/7121] [D loss: 0.639351] [G loss: 0.787564] [AE loss: 1.112014]\n",
            "[Epoch 0/10] [Batch 565/7121] [D loss: 0.608266] [G loss: 0.806927] [AE loss: 1.226366]\n",
            "[Epoch 0/10] [Batch 566/7121] [D loss: 0.582891] [G loss: 0.833450] [AE loss: 1.615472]\n",
            "[Epoch 0/10] [Batch 567/7121] [D loss: 0.554503] [G loss: 0.843744] [AE loss: 1.855709]\n",
            "[Epoch 0/10] [Batch 568/7121] [D loss: 0.537637] [G loss: 0.902572] [AE loss: 1.027037]\n",
            "[Epoch 0/10] [Batch 569/7121] [D loss: 0.553352] [G loss: 0.906623] [AE loss: 1.137630]\n",
            "[Epoch 0/10] [Batch 570/7121] [D loss: 0.537631] [G loss: 0.938699] [AE loss: 1.458614]\n",
            "[Epoch 0/10] [Batch 571/7121] [D loss: 0.503530] [G loss: 0.967737] [AE loss: 1.406911]\n",
            "[Epoch 0/10] [Batch 572/7121] [D loss: 0.553050] [G loss: 0.985485] [AE loss: 1.157854]\n",
            "[Epoch 0/10] [Batch 573/7121] [D loss: 0.515618] [G loss: 1.006465] [AE loss: 1.199573]\n",
            "[Epoch 0/10] [Batch 574/7121] [D loss: 0.507687] [G loss: 1.004352] [AE loss: 1.089053]\n",
            "[Epoch 0/10] [Batch 575/7121] [D loss: 0.515022] [G loss: 1.001066] [AE loss: 1.440921]\n",
            "[Epoch 0/10] [Batch 576/7121] [D loss: 0.576440] [G loss: 1.002794] [AE loss: 0.958299]\n",
            "[Epoch 0/10] [Batch 577/7121] [D loss: 0.516195] [G loss: 0.988586] [AE loss: 0.978420]\n",
            "[Epoch 0/10] [Batch 578/7121] [D loss: 0.563566] [G loss: 0.958677] [AE loss: 1.226511]\n",
            "[Epoch 0/10] [Batch 579/7121] [D loss: 0.501431] [G loss: 0.948816] [AE loss: 1.027915]\n",
            "[Epoch 0/10] [Batch 580/7121] [D loss: 0.527629] [G loss: 0.913443] [AE loss: 3.990877]\n",
            "[Epoch 0/10] [Batch 581/7121] [D loss: 0.497575] [G loss: 0.866129] [AE loss: 1.053332]\n",
            "[Epoch 0/10] [Batch 582/7121] [D loss: 0.510709] [G loss: 0.811163] [AE loss: 1.199766]\n",
            "[Epoch 0/10] [Batch 583/7121] [D loss: 0.526466] [G loss: 0.761705] [AE loss: 1.081762]\n",
            "[Epoch 0/10] [Batch 584/7121] [D loss: 0.530715] [G loss: 0.702253] [AE loss: 1.021673]\n",
            "[Epoch 0/10] [Batch 585/7121] [D loss: 0.624984] [G loss: 0.600394] [AE loss: 1.042696]\n",
            "[Epoch 0/10] [Batch 586/7121] [D loss: 0.685926] [G loss: 0.563887] [AE loss: 0.896442]\n",
            "[Epoch 0/10] [Batch 587/7121] [D loss: 0.615637] [G loss: 0.535639] [AE loss: 0.895641]\n",
            "[Epoch 0/10] [Batch 588/7121] [D loss: 0.739920] [G loss: 0.461363] [AE loss: 1.334808]\n",
            "[Epoch 0/10] [Batch 589/7121] [D loss: 0.704108] [G loss: 0.443914] [AE loss: 0.888462]\n",
            "[Epoch 0/10] [Batch 590/7121] [D loss: 0.837318] [G loss: 0.401879] [AE loss: 0.917793]\n",
            "[Epoch 0/10] [Batch 591/7121] [D loss: 0.812425] [G loss: 0.384998] [AE loss: 4.401199]\n",
            "[Epoch 0/10] [Batch 592/7121] [D loss: 0.816143] [G loss: 0.386270] [AE loss: 0.956564]\n",
            "[Epoch 0/10] [Batch 593/7121] [D loss: 0.762765] [G loss: 0.399135] [AE loss: 1.046816]\n",
            "[Epoch 0/10] [Batch 594/7121] [D loss: 0.807590] [G loss: 0.391373] [AE loss: 1.371722]\n",
            "[Epoch 0/10] [Batch 595/7121] [D loss: 0.754142] [G loss: 0.417555] [AE loss: 1.148868]\n",
            "[Epoch 0/10] [Batch 596/7121] [D loss: 0.775224] [G loss: 0.421169] [AE loss: 1.484371]\n",
            "[Epoch 0/10] [Batch 597/7121] [D loss: 0.692272] [G loss: 0.467010] [AE loss: 1.001851]\n",
            "[Epoch 0/10] [Batch 598/7121] [D loss: 0.741159] [G loss: 0.485309] [AE loss: 0.945955]\n",
            "[Epoch 0/10] [Batch 599/7121] [D loss: 0.712382] [G loss: 0.519331] [AE loss: 1.260882]\n",
            "[Epoch 0/10] [Batch 600/7121] [D loss: 0.707790] [G loss: 0.550075] [AE loss: 2.044233]\n",
            "[Epoch 0/10] [Batch 601/7121] [D loss: 0.597121] [G loss: 0.576163] [AE loss: 1.743314]\n",
            "[Epoch 0/10] [Batch 602/7121] [D loss: 0.672146] [G loss: 0.612872] [AE loss: 1.266534]\n",
            "[Epoch 0/10] [Batch 603/7121] [D loss: 0.601246] [G loss: 0.655754] [AE loss: 1.279996]\n",
            "[Epoch 0/10] [Batch 604/7121] [D loss: 0.573424] [G loss: 0.687641] [AE loss: 1.212428]\n",
            "[Epoch 0/10] [Batch 605/7121] [D loss: 0.634527] [G loss: 0.702113] [AE loss: 1.165893]\n",
            "[Epoch 0/10] [Batch 606/7121] [D loss: 0.576095] [G loss: 0.743673] [AE loss: 3.252407]\n",
            "[Epoch 0/10] [Batch 607/7121] [D loss: 0.599296] [G loss: 0.747129] [AE loss: 1.439670]\n",
            "[Epoch 0/10] [Batch 608/7121] [D loss: 0.551714] [G loss: 0.764095] [AE loss: 1.064237]\n",
            "[Epoch 0/10] [Batch 609/7121] [D loss: 0.575463] [G loss: 0.790521] [AE loss: 1.747413]\n",
            "[Epoch 0/10] [Batch 610/7121] [D loss: 0.500382] [G loss: 0.814995] [AE loss: 1.986016]\n",
            "[Epoch 0/10] [Batch 611/7121] [D loss: 0.570667] [G loss: 0.824157] [AE loss: 1.221044]\n",
            "[Epoch 0/10] [Batch 612/7121] [D loss: 0.536580] [G loss: 0.816701] [AE loss: 1.732816]\n",
            "[Epoch 0/10] [Batch 613/7121] [D loss: 0.524582] [G loss: 0.824486] [AE loss: 1.174007]\n",
            "[Epoch 0/10] [Batch 614/7121] [D loss: 0.571890] [G loss: 0.809180] [AE loss: 1.060783]\n",
            "[Epoch 0/10] [Batch 615/7121] [D loss: 0.530564] [G loss: 0.859566] [AE loss: 1.068670]\n",
            "[Epoch 0/10] [Batch 616/7121] [D loss: 0.523378] [G loss: 0.834718] [AE loss: 1.041731]\n",
            "[Epoch 0/10] [Batch 617/7121] [D loss: 0.531120] [G loss: 0.853113] [AE loss: 1.273730]\n",
            "[Epoch 0/10] [Batch 618/7121] [D loss: 0.539908] [G loss: 0.799390] [AE loss: 1.797604]\n",
            "[Epoch 0/10] [Batch 619/7121] [D loss: 0.534861] [G loss: 0.791378] [AE loss: 1.256464]\n",
            "[Epoch 0/10] [Batch 620/7121] [D loss: 0.532412] [G loss: 0.790204] [AE loss: 0.957328]\n",
            "[Epoch 0/10] [Batch 621/7121] [D loss: 0.538316] [G loss: 0.762346] [AE loss: 1.198630]\n",
            "[Epoch 0/10] [Batch 622/7121] [D loss: 0.565379] [G loss: 0.713064] [AE loss: 1.089754]\n",
            "[Epoch 0/10] [Batch 623/7121] [D loss: 0.570716] [G loss: 0.698571] [AE loss: 1.016402]\n",
            "[Epoch 0/10] [Batch 624/7121] [D loss: 0.550196] [G loss: 0.659334] [AE loss: 1.008378]\n",
            "[Epoch 0/10] [Batch 625/7121] [D loss: 0.675064] [G loss: 0.668754] [AE loss: 6.289392]\n",
            "[Epoch 0/10] [Batch 626/7121] [D loss: 0.546845] [G loss: 0.637569] [AE loss: 0.941507]\n",
            "[Epoch 0/10] [Batch 627/7121] [D loss: 0.570770] [G loss: 0.600666] [AE loss: 2.040276]\n",
            "[Epoch 0/10] [Batch 628/7121] [D loss: 0.624862] [G loss: 0.590900] [AE loss: 1.155998]\n",
            "[Epoch 0/10] [Batch 629/7121] [D loss: 0.661270] [G loss: 0.565681] [AE loss: 0.966671]\n",
            "[Epoch 0/10] [Batch 630/7121] [D loss: 0.655262] [G loss: 0.578463] [AE loss: 1.534522]\n",
            "[Epoch 0/10] [Batch 631/7121] [D loss: 0.642109] [G loss: 0.604949] [AE loss: 1.165653]\n",
            "[Epoch 0/10] [Batch 632/7121] [D loss: 0.604157] [G loss: 0.596608] [AE loss: 1.187686]\n",
            "[Epoch 0/10] [Batch 633/7121] [D loss: 0.644688] [G loss: 0.621483] [AE loss: 1.130838]\n",
            "[Epoch 0/10] [Batch 634/7121] [D loss: 0.631026] [G loss: 0.679528] [AE loss: 1.177125]\n",
            "[Epoch 0/10] [Batch 635/7121] [D loss: 0.634190] [G loss: 0.701993] [AE loss: 1.087389]\n",
            "[Epoch 0/10] [Batch 636/7121] [D loss: 0.536792] [G loss: 0.793896] [AE loss: 1.350834]\n",
            "[Epoch 0/10] [Batch 637/7121] [D loss: 0.547210] [G loss: 0.844769] [AE loss: 1.418173]\n",
            "[Epoch 0/10] [Batch 638/7121] [D loss: 0.489104] [G loss: 0.928809] [AE loss: 1.092124]\n",
            "[Epoch 0/10] [Batch 639/7121] [D loss: 0.549220] [G loss: 1.026264] [AE loss: 1.216878]\n",
            "[Epoch 0/10] [Batch 640/7121] [D loss: 0.524054] [G loss: 1.095043] [AE loss: 1.261255]\n",
            "[Epoch 0/10] [Batch 641/7121] [D loss: 0.489919] [G loss: 1.154388] [AE loss: 1.092304]\n",
            "[Epoch 0/10] [Batch 642/7121] [D loss: 0.536483] [G loss: 1.149159] [AE loss: 1.137393]\n",
            "[Epoch 0/10] [Batch 643/7121] [D loss: 0.466558] [G loss: 1.181790] [AE loss: 1.111036]\n",
            "[Epoch 0/10] [Batch 644/7121] [D loss: 0.578247] [G loss: 1.207291] [AE loss: 1.699356]\n",
            "[Epoch 0/10] [Batch 645/7121] [D loss: 0.498487] [G loss: 1.119986] [AE loss: 2.615784]\n",
            "[Epoch 0/10] [Batch 646/7121] [D loss: 0.451423] [G loss: 1.165895] [AE loss: 1.306053]\n",
            "[Epoch 0/10] [Batch 647/7121] [D loss: 0.466891] [G loss: 1.111331] [AE loss: 3.613022]\n",
            "[Epoch 0/10] [Batch 648/7121] [D loss: 0.513285] [G loss: 1.077973] [AE loss: 1.364991]\n",
            "[Epoch 0/10] [Batch 649/7121] [D loss: 0.517201] [G loss: 0.991318] [AE loss: 1.237652]\n",
            "[Epoch 0/10] [Batch 650/7121] [D loss: 0.553039] [G loss: 0.978367] [AE loss: 1.752758]\n",
            "[Epoch 0/10] [Batch 651/7121] [D loss: 0.525553] [G loss: 0.942580] [AE loss: 1.683909]\n",
            "[Epoch 0/10] [Batch 652/7121] [D loss: 0.518915] [G loss: 0.886226] [AE loss: 1.651078]\n",
            "[Epoch 0/10] [Batch 653/7121] [D loss: 0.543581] [G loss: 0.852819] [AE loss: 1.182755]\n",
            "[Epoch 0/10] [Batch 654/7121] [D loss: 0.553343] [G loss: 0.818879] [AE loss: 1.087597]\n",
            "[Epoch 0/10] [Batch 655/7121] [D loss: 0.549463] [G loss: 0.860851] [AE loss: 3.061239]\n",
            "[Epoch 0/10] [Batch 656/7121] [D loss: 0.611667] [G loss: 0.833952] [AE loss: 1.294671]\n",
            "[Epoch 0/10] [Batch 657/7121] [D loss: 0.543500] [G loss: 0.823075] [AE loss: 1.207239]\n",
            "[Epoch 0/10] [Batch 658/7121] [D loss: 0.712427] [G loss: 0.835515] [AE loss: 1.076190]\n",
            "[Epoch 0/10] [Batch 659/7121] [D loss: 0.683376] [G loss: 0.893865] [AE loss: 1.392691]\n",
            "[Epoch 0/10] [Batch 660/7121] [D loss: 0.619251] [G loss: 0.836252] [AE loss: 3.904671]\n",
            "[Epoch 0/10] [Batch 661/7121] [D loss: 0.581499] [G loss: 0.865178] [AE loss: 2.070690]\n",
            "[Epoch 0/10] [Batch 662/7121] [D loss: 0.750245] [G loss: 0.811069] [AE loss: 1.361242]\n",
            "[Epoch 0/10] [Batch 663/7121] [D loss: 0.715570] [G loss: 0.758782] [AE loss: 1.339980]\n",
            "[Epoch 0/10] [Batch 664/7121] [D loss: 0.684464] [G loss: 0.784552] [AE loss: 1.675539]\n",
            "[Epoch 0/10] [Batch 665/7121] [D loss: 0.695302] [G loss: 0.812049] [AE loss: 1.306052]\n",
            "[Epoch 0/10] [Batch 666/7121] [D loss: 0.789464] [G loss: 0.683406] [AE loss: 1.434532]\n",
            "[Epoch 0/10] [Batch 667/7121] [D loss: 0.576759] [G loss: 0.667312] [AE loss: 2.317171]\n",
            "[Epoch 0/10] [Batch 668/7121] [D loss: 0.762990] [G loss: 0.700752] [AE loss: 1.511535]\n",
            "[Epoch 0/10] [Batch 669/7121] [D loss: 0.655860] [G loss: 0.672285] [AE loss: 1.377976]\n",
            "[Epoch 0/10] [Batch 670/7121] [D loss: 0.669680] [G loss: 0.673091] [AE loss: 1.322364]\n",
            "[Epoch 0/10] [Batch 671/7121] [D loss: 0.676646] [G loss: 0.685748] [AE loss: 1.228729]\n",
            "[Epoch 0/10] [Batch 672/7121] [D loss: 0.834116] [G loss: 0.723013] [AE loss: 1.390874]\n",
            "[Epoch 0/10] [Batch 673/7121] [D loss: 0.790077] [G loss: 0.701143] [AE loss: 1.175553]\n",
            "[Epoch 0/10] [Batch 674/7121] [D loss: 0.670873] [G loss: 0.740525] [AE loss: 1.278175]\n",
            "[Epoch 0/10] [Batch 675/7121] [D loss: 0.666080] [G loss: 0.693051] [AE loss: 2.761022]\n",
            "[Epoch 0/10] [Batch 676/7121] [D loss: 0.708690] [G loss: 0.695022] [AE loss: 1.849017]\n",
            "[Epoch 0/10] [Batch 677/7121] [D loss: 0.842328] [G loss: 0.652601] [AE loss: 1.283716]\n",
            "[Epoch 0/10] [Batch 678/7121] [D loss: 0.722242] [G loss: 0.619311] [AE loss: 1.444768]\n",
            "[Epoch 0/10] [Batch 679/7121] [D loss: 0.697442] [G loss: 0.592764] [AE loss: 1.360185]\n",
            "[Epoch 0/10] [Batch 680/7121] [D loss: 0.866194] [G loss: 0.530754] [AE loss: 6.235634]\n",
            "[Epoch 0/10] [Batch 681/7121] [D loss: 0.854759] [G loss: 0.502416] [AE loss: 1.320755]\n",
            "[Epoch 0/10] [Batch 682/7121] [D loss: 0.841272] [G loss: 0.443958] [AE loss: 1.944991]\n",
            "[Epoch 0/10] [Batch 683/7121] [D loss: 0.964123] [G loss: 0.439126] [AE loss: 1.407430]\n",
            "[Epoch 0/10] [Batch 684/7121] [D loss: 0.871998] [G loss: 0.398687] [AE loss: 1.224711]\n",
            "[Epoch 0/10] [Batch 685/7121] [D loss: 0.830134] [G loss: 0.395391] [AE loss: 1.546823]\n",
            "[Epoch 0/10] [Batch 686/7121] [D loss: 0.788094] [G loss: 0.405918] [AE loss: 1.381059]\n",
            "[Epoch 0/10] [Batch 687/7121] [D loss: 0.844857] [G loss: 0.455167] [AE loss: 1.225928]\n",
            "[Epoch 0/10] [Batch 688/7121] [D loss: 0.656836] [G loss: 0.506918] [AE loss: 1.335523]\n",
            "[Epoch 0/10] [Batch 689/7121] [D loss: 0.737193] [G loss: 0.575477] [AE loss: 1.431385]\n",
            "[Epoch 0/10] [Batch 690/7121] [D loss: 0.649104] [G loss: 0.668923] [AE loss: 1.531653]\n",
            "[Epoch 0/10] [Batch 691/7121] [D loss: 0.752057] [G loss: 0.715165] [AE loss: 0.906804]\n",
            "[Epoch 0/10] [Batch 692/7121] [D loss: 0.762635] [G loss: 0.747379] [AE loss: 1.120182]\n",
            "[Epoch 0/10] [Batch 693/7121] [D loss: 0.562065] [G loss: 0.733837] [AE loss: 1.249044]\n",
            "[Epoch 0/10] [Batch 694/7121] [D loss: 0.731610] [G loss: 0.717319] [AE loss: 1.937189]\n",
            "[Epoch 0/10] [Batch 695/7121] [D loss: 0.660823] [G loss: 0.656410] [AE loss: 1.307544]\n",
            "[Epoch 0/10] [Batch 696/7121] [D loss: 0.770441] [G loss: 0.635809] [AE loss: 1.169804]\n",
            "[Epoch 0/10] [Batch 697/7121] [D loss: 0.681750] [G loss: 0.618215] [AE loss: 1.238590]\n",
            "[Epoch 0/10] [Batch 698/7121] [D loss: 0.696666] [G loss: 0.620271] [AE loss: 1.517962]\n",
            "[Epoch 0/10] [Batch 699/7121] [D loss: 0.720585] [G loss: 0.646766] [AE loss: 1.450146]\n",
            "[Epoch 0/10] [Batch 700/7121] [D loss: 0.747355] [G loss: 0.662564] [AE loss: 1.125919]\n",
            "[Epoch 0/10] [Batch 701/7121] [D loss: 0.743522] [G loss: 0.650349] [AE loss: 1.048834]\n",
            "[Epoch 0/10] [Batch 702/7121] [D loss: 0.658106] [G loss: 0.668030] [AE loss: 1.382062]\n",
            "[Epoch 0/10] [Batch 703/7121] [D loss: 0.646798] [G loss: 0.657671] [AE loss: 1.149225]\n",
            "[Epoch 0/10] [Batch 704/7121] [D loss: 0.800990] [G loss: 0.682093] [AE loss: 2.030032]\n",
            "[Epoch 0/10] [Batch 705/7121] [D loss: 0.756345] [G loss: 0.683206] [AE loss: 1.081471]\n",
            "[Epoch 0/10] [Batch 706/7121] [D loss: 0.735913] [G loss: 0.705269] [AE loss: 1.033110]\n",
            "[Epoch 0/10] [Batch 707/7121] [D loss: 0.625703] [G loss: 0.655375] [AE loss: 1.085718]\n",
            "[Epoch 0/10] [Batch 708/7121] [D loss: 0.622712] [G loss: 0.688450] [AE loss: 1.725942]\n",
            "[Epoch 0/10] [Batch 709/7121] [D loss: 0.716901] [G loss: 0.687523] [AE loss: 0.988907]\n",
            "[Epoch 0/10] [Batch 710/7121] [D loss: 0.769677] [G loss: 0.626145] [AE loss: 1.212158]\n",
            "[Epoch 0/10] [Batch 711/7121] [D loss: 0.744106] [G loss: 0.541146] [AE loss: 1.167297]\n",
            "[Epoch 0/10] [Batch 712/7121] [D loss: 0.818616] [G loss: 0.479822] [AE loss: 1.886739]\n",
            "[Epoch 0/10] [Batch 713/7121] [D loss: 0.742262] [G loss: 0.444828] [AE loss: 2.059864]\n",
            "[Epoch 0/10] [Batch 714/7121] [D loss: 0.918504] [G loss: 0.391979] [AE loss: 1.224751]\n",
            "[Epoch 0/10] [Batch 715/7121] [D loss: 0.912125] [G loss: 0.345553] [AE loss: 1.426783]\n",
            "[Epoch 0/10] [Batch 716/7121] [D loss: 1.025468] [G loss: 0.338095] [AE loss: 1.653172]\n",
            "[Epoch 0/10] [Batch 717/7121] [D loss: 0.923394] [G loss: 0.373485] [AE loss: 1.074570]\n",
            "[Epoch 0/10] [Batch 718/7121] [D loss: 0.862065] [G loss: 0.409145] [AE loss: 1.137163]\n",
            "[Epoch 0/10] [Batch 719/7121] [D loss: 0.809528] [G loss: 0.406868] [AE loss: 0.916315]\n",
            "[Epoch 0/10] [Batch 720/7121] [D loss: 0.833151] [G loss: 0.442905] [AE loss: 1.023098]\n",
            "[Epoch 0/10] [Batch 721/7121] [D loss: 0.834930] [G loss: 0.462864] [AE loss: 1.332302]\n",
            "[Epoch 0/10] [Batch 722/7121] [D loss: 0.816860] [G loss: 0.473513] [AE loss: 1.200555]\n",
            "[Epoch 0/10] [Batch 723/7121] [D loss: 0.798798] [G loss: 0.504805] [AE loss: 1.285641]\n",
            "[Epoch 0/10] [Batch 724/7121] [D loss: 0.779727] [G loss: 0.499382] [AE loss: 0.903612]\n",
            "[Epoch 0/10] [Batch 725/7121] [D loss: 0.728679] [G loss: 0.519967] [AE loss: 1.263240]\n",
            "[Epoch 0/10] [Batch 726/7121] [D loss: 0.814696] [G loss: 0.506422] [AE loss: 0.897441]\n",
            "[Epoch 0/10] [Batch 727/7121] [D loss: 0.811784] [G loss: 0.510004] [AE loss: 1.269791]\n",
            "[Epoch 0/10] [Batch 728/7121] [D loss: 0.771658] [G loss: 0.504837] [AE loss: 1.163154]\n",
            "[Epoch 0/10] [Batch 729/7121] [D loss: 0.779245] [G loss: 0.500488] [AE loss: 2.073660]\n",
            "[Epoch 0/10] [Batch 730/7121] [D loss: 0.771351] [G loss: 0.483417] [AE loss: 1.092081]\n",
            "[Epoch 0/10] [Batch 731/7121] [D loss: 0.854802] [G loss: 0.504028] [AE loss: 1.504134]\n",
            "[Epoch 0/10] [Batch 732/7121] [D loss: 0.794651] [G loss: 0.485139] [AE loss: 1.054953]\n",
            "[Epoch 0/10] [Batch 733/7121] [D loss: 0.768606] [G loss: 0.494731] [AE loss: 1.058854]\n",
            "[Epoch 0/10] [Batch 734/7121] [D loss: 0.784396] [G loss: 0.512371] [AE loss: 2.435355]\n",
            "[Epoch 0/10] [Batch 735/7121] [D loss: 0.814746] [G loss: 0.530426] [AE loss: 2.751803]\n",
            "[Epoch 0/10] [Batch 736/7121] [D loss: 0.772562] [G loss: 0.569822] [AE loss: 1.379598]\n",
            "[Epoch 0/10] [Batch 737/7121] [D loss: 0.768154] [G loss: 0.578859] [AE loss: 1.006000]\n",
            "[Epoch 0/10] [Batch 738/7121] [D loss: 0.763680] [G loss: 0.606144] [AE loss: 2.870233]\n",
            "[Epoch 0/10] [Batch 739/7121] [D loss: 0.733525] [G loss: 0.627165] [AE loss: 1.241848]\n",
            "[Epoch 0/10] [Batch 740/7121] [D loss: 0.711289] [G loss: 0.667453] [AE loss: 1.005436]\n",
            "[Epoch 0/10] [Batch 741/7121] [D loss: 0.748514] [G loss: 0.695335] [AE loss: 1.341373]\n",
            "[Epoch 0/10] [Batch 742/7121] [D loss: 0.711832] [G loss: 0.727999] [AE loss: 0.942004]\n",
            "[Epoch 0/10] [Batch 743/7121] [D loss: 0.663477] [G loss: 0.767449] [AE loss: 1.118869]\n",
            "[Epoch 0/10] [Batch 744/7121] [D loss: 0.648150] [G loss: 0.798599] [AE loss: 1.071119]\n",
            "[Epoch 0/10] [Batch 745/7121] [D loss: 0.660886] [G loss: 0.844710] [AE loss: 0.940643]\n",
            "[Epoch 0/10] [Batch 746/7121] [D loss: 0.627761] [G loss: 0.883214] [AE loss: 1.204459]\n",
            "[Epoch 0/10] [Batch 747/7121] [D loss: 0.644161] [G loss: 0.915067] [AE loss: 1.020401]\n",
            "[Epoch 0/10] [Batch 748/7121] [D loss: 0.623850] [G loss: 0.940469] [AE loss: 2.234862]\n",
            "[Epoch 0/10] [Batch 749/7121] [D loss: 0.591494] [G loss: 0.971464] [AE loss: 2.399440]\n",
            "[Epoch 0/10] [Batch 750/7121] [D loss: 0.638443] [G loss: 0.983060] [AE loss: 0.955860]\n",
            "[Epoch 0/10] [Batch 751/7121] [D loss: 0.615613] [G loss: 0.992470] [AE loss: 1.142661]\n",
            "[Epoch 0/10] [Batch 752/7121] [D loss: 0.599594] [G loss: 1.001698] [AE loss: 1.130352]\n",
            "[Epoch 0/10] [Batch 753/7121] [D loss: 0.577349] [G loss: 1.013189] [AE loss: 1.925595]\n",
            "[Epoch 0/10] [Batch 754/7121] [D loss: 0.584244] [G loss: 0.998632] [AE loss: 1.068022]\n",
            "[Epoch 0/10] [Batch 755/7121] [D loss: 0.599702] [G loss: 0.970871] [AE loss: 1.175354]\n",
            "[Epoch 0/10] [Batch 756/7121] [D loss: 0.602549] [G loss: 0.963460] [AE loss: 0.948728]\n",
            "[Epoch 0/10] [Batch 757/7121] [D loss: 0.592310] [G loss: 0.932003] [AE loss: 2.335990]\n",
            "[Epoch 0/10] [Batch 758/7121] [D loss: 0.599066] [G loss: 0.910246] [AE loss: 2.598140]\n",
            "[Epoch 0/10] [Batch 759/7121] [D loss: 0.564256] [G loss: 0.899602] [AE loss: 1.215069]\n",
            "[Epoch 0/10] [Batch 760/7121] [D loss: 0.603837] [G loss: 0.888222] [AE loss: 1.085400]\n",
            "[Epoch 0/10] [Batch 761/7121] [D loss: 0.630978] [G loss: 0.889584] [AE loss: 0.934515]\n",
            "[Epoch 0/10] [Batch 762/7121] [D loss: 0.595053] [G loss: 0.885853] [AE loss: 0.861511]\n",
            "[Epoch 0/10] [Batch 763/7121] [D loss: 0.577586] [G loss: 0.899749] [AE loss: 1.396987]\n",
            "[Epoch 0/10] [Batch 764/7121] [D loss: 0.589976] [G loss: 0.904912] [AE loss: 0.972539]\n",
            "[Epoch 0/10] [Batch 765/7121] [D loss: 0.508008] [G loss: 0.918683] [AE loss: 2.438058]\n",
            "[Epoch 0/10] [Batch 766/7121] [D loss: 0.571056] [G loss: 0.922389] [AE loss: 0.905790]\n",
            "[Epoch 0/10] [Batch 767/7121] [D loss: 0.569252] [G loss: 0.924201] [AE loss: 1.240660]\n",
            "[Epoch 0/10] [Batch 768/7121] [D loss: 0.523701] [G loss: 0.923529] [AE loss: 1.045895]\n",
            "[Epoch 0/10] [Batch 769/7121] [D loss: 0.564852] [G loss: 0.922810] [AE loss: 1.424868]\n",
            "[Epoch 0/10] [Batch 770/7121] [D loss: 0.511946] [G loss: 0.928593] [AE loss: 0.903388]\n",
            "[Epoch 0/10] [Batch 771/7121] [D loss: 0.499589] [G loss: 0.929686] [AE loss: 1.880782]\n",
            "[Epoch 0/10] [Batch 772/7121] [D loss: 0.525899] [G loss: 0.926439] [AE loss: 0.868941]\n",
            "[Epoch 0/10] [Batch 773/7121] [D loss: 0.559674] [G loss: 0.900185] [AE loss: 0.813723]\n",
            "[Epoch 0/10] [Batch 774/7121] [D loss: 0.506756] [G loss: 0.897431] [AE loss: 1.418415]\n",
            "[Epoch 0/10] [Batch 775/7121] [D loss: 0.489745] [G loss: 0.877707] [AE loss: 1.187870]\n",
            "[Epoch 0/10] [Batch 776/7121] [D loss: 0.509682] [G loss: 0.867494] [AE loss: 0.897205]\n",
            "[Epoch 0/10] [Batch 777/7121] [D loss: 0.561620] [G loss: 0.848831] [AE loss: 1.129382]\n",
            "[Epoch 0/10] [Batch 778/7121] [D loss: 0.512841] [G loss: 0.838212] [AE loss: 1.416103]\n",
            "[Epoch 0/10] [Batch 779/7121] [D loss: 0.521738] [G loss: 0.824494] [AE loss: 0.885567]\n",
            "[Epoch 0/10] [Batch 780/7121] [D loss: 0.541208] [G loss: 0.796284] [AE loss: 0.936225]\n",
            "[Epoch 0/10] [Batch 781/7121] [D loss: 0.515648] [G loss: 0.801647] [AE loss: 0.940051]\n",
            "[Epoch 0/10] [Batch 782/7121] [D loss: 0.537226] [G loss: 0.786030] [AE loss: 0.845798]\n",
            "[Epoch 0/10] [Batch 783/7121] [D loss: 0.505788] [G loss: 0.777717] [AE loss: 2.695584]\n",
            "[Epoch 0/10] [Batch 784/7121] [D loss: 0.538037] [G loss: 0.767947] [AE loss: 0.942495]\n",
            "[Epoch 0/10] [Batch 785/7121] [D loss: 0.557468] [G loss: 0.764914] [AE loss: 0.834955]\n",
            "[Epoch 0/10] [Batch 786/7121] [D loss: 0.555305] [G loss: 0.771357] [AE loss: 0.915581]\n",
            "[Epoch 0/10] [Batch 787/7121] [D loss: 0.508749] [G loss: 0.774776] [AE loss: 1.702143]\n",
            "[Epoch 0/10] [Batch 788/7121] [D loss: 0.484074] [G loss: 0.782426] [AE loss: 1.046511]\n",
            "[Epoch 0/10] [Batch 789/7121] [D loss: 0.541045] [G loss: 0.780925] [AE loss: 0.899260]\n",
            "[Epoch 0/10] [Batch 790/7121] [D loss: 0.509836] [G loss: 0.793928] [AE loss: 1.045978]\n",
            "[Epoch 0/10] [Batch 791/7121] [D loss: 0.573287] [G loss: 0.793337] [AE loss: 0.848803]\n",
            "[Epoch 0/10] [Batch 792/7121] [D loss: 0.484227] [G loss: 0.792001] [AE loss: 1.414480]\n",
            "[Epoch 0/10] [Batch 793/7121] [D loss: 0.489626] [G loss: 0.820725] [AE loss: 1.104337]\n",
            "[Epoch 0/10] [Batch 794/7121] [D loss: 0.525168] [G loss: 0.805454] [AE loss: 0.877660]\n",
            "[Epoch 0/10] [Batch 795/7121] [D loss: 0.467605] [G loss: 0.817833] [AE loss: 2.079337]\n",
            "[Epoch 0/10] [Batch 796/7121] [D loss: 0.494496] [G loss: 0.820103] [AE loss: 0.954827]\n",
            "[Epoch 0/10] [Batch 797/7121] [D loss: 0.522392] [G loss: 0.821294] [AE loss: 1.194799]\n",
            "[Epoch 0/10] [Batch 798/7121] [D loss: 0.468884] [G loss: 0.829808] [AE loss: 1.051893]\n",
            "[Epoch 0/10] [Batch 799/7121] [D loss: 0.470688] [G loss: 0.832824] [AE loss: 1.265217]\n",
            "[Epoch 0/10] [Batch 800/7121] [D loss: 0.519272] [G loss: 0.818050] [AE loss: 0.958647]\n",
            "[Epoch 0/10] [Batch 801/7121] [D loss: 0.460855] [G loss: 0.860471] [AE loss: 1.211702]\n",
            "[Epoch 0/10] [Batch 802/7121] [D loss: 0.453153] [G loss: 0.849285] [AE loss: 1.310406]\n",
            "[Epoch 0/10] [Batch 803/7121] [D loss: 0.489359] [G loss: 0.890226] [AE loss: 1.040673]\n",
            "[Epoch 0/10] [Batch 804/7121] [D loss: 0.489823] [G loss: 0.892079] [AE loss: 1.140675]\n",
            "[Epoch 0/10] [Batch 805/7121] [D loss: 0.503973] [G loss: 0.886118] [AE loss: 1.957282]\n",
            "[Epoch 0/10] [Batch 806/7121] [D loss: 0.497596] [G loss: 0.865694] [AE loss: 0.978580]\n",
            "[Epoch 0/10] [Batch 807/7121] [D loss: 0.495647] [G loss: 0.836575] [AE loss: 1.011470]\n",
            "[Epoch 0/10] [Batch 808/7121] [D loss: 0.538768] [G loss: 0.836815] [AE loss: 1.030685]\n",
            "[Epoch 0/10] [Batch 809/7121] [D loss: 0.482330] [G loss: 0.844494] [AE loss: 1.171323]\n",
            "[Epoch 0/10] [Batch 810/7121] [D loss: 0.479353] [G loss: 0.805017] [AE loss: 3.130752]\n",
            "[Epoch 0/10] [Batch 811/7121] [D loss: 0.492526] [G loss: 0.765779] [AE loss: 1.072716]\n",
            "[Epoch 0/10] [Batch 812/7121] [D loss: 0.588833] [G loss: 0.763300] [AE loss: 1.310088]\n",
            "[Epoch 0/10] [Batch 813/7121] [D loss: 0.508348] [G loss: 0.747351] [AE loss: 1.170664]\n",
            "[Epoch 0/10] [Batch 814/7121] [D loss: 0.531371] [G loss: 0.711921] [AE loss: 1.124018]\n",
            "[Epoch 0/10] [Batch 815/7121] [D loss: 0.502866] [G loss: 0.712061] [AE loss: 1.148580]\n",
            "[Epoch 0/10] [Batch 816/7121] [D loss: 0.607055] [G loss: 0.661146] [AE loss: 1.057905]\n",
            "[Epoch 0/10] [Batch 817/7121] [D loss: 0.645458] [G loss: 0.639195] [AE loss: 1.195962]\n",
            "[Epoch 0/10] [Batch 818/7121] [D loss: 0.620484] [G loss: 0.591361] [AE loss: 1.333344]\n",
            "[Epoch 0/10] [Batch 819/7121] [D loss: 0.668243] [G loss: 0.523638] [AE loss: 1.024667]\n",
            "[Epoch 0/10] [Batch 820/7121] [D loss: 0.694444] [G loss: 0.464159] [AE loss: 1.274940]\n",
            "[Epoch 0/10] [Batch 821/7121] [D loss: 0.785383] [G loss: 0.438184] [AE loss: 1.078842]\n",
            "[Epoch 0/10] [Batch 822/7121] [D loss: 0.666707] [G loss: 0.449156] [AE loss: 1.314313]\n",
            "[Epoch 0/10] [Batch 823/7121] [D loss: 0.801273] [G loss: 0.394122] [AE loss: 1.255343]\n",
            "[Epoch 0/10] [Batch 824/7121] [D loss: 0.780212] [G loss: 0.401624] [AE loss: 1.162857]\n",
            "[Epoch 0/10] [Batch 825/7121] [D loss: 0.839064] [G loss: 0.401846] [AE loss: 1.271268]\n",
            "[Epoch 0/10] [Batch 826/7121] [D loss: 0.796274] [G loss: 0.410047] [AE loss: 1.291245]\n",
            "[Epoch 0/10] [Batch 827/7121] [D loss: 0.858370] [G loss: 0.416580] [AE loss: 1.369971]\n",
            "[Epoch 0/10] [Batch 828/7121] [D loss: 0.757502] [G loss: 0.424000] [AE loss: 1.093073]\n",
            "[Epoch 0/10] [Batch 829/7121] [D loss: 0.715810] [G loss: 0.494146] [AE loss: 1.531229]\n",
            "[Epoch 0/10] [Batch 830/7121] [D loss: 0.691238] [G loss: 0.543728] [AE loss: 1.289914]\n",
            "[Epoch 0/10] [Batch 831/7121] [D loss: 0.673579] [G loss: 0.604946] [AE loss: 1.568724]\n",
            "[Epoch 0/10] [Batch 832/7121] [D loss: 0.625554] [G loss: 0.645112] [AE loss: 1.942310]\n",
            "[Epoch 0/10] [Batch 833/7121] [D loss: 0.496827] [G loss: 0.739607] [AE loss: 1.570801]\n",
            "[Epoch 0/10] [Batch 834/7121] [D loss: 0.610319] [G loss: 0.770081] [AE loss: 2.488182]\n",
            "[Epoch 0/10] [Batch 835/7121] [D loss: 0.588938] [G loss: 0.866203] [AE loss: 4.879683]\n",
            "[Epoch 0/10] [Batch 836/7121] [D loss: 0.479140] [G loss: 0.959665] [AE loss: 1.310193]\n",
            "[Epoch 0/10] [Batch 837/7121] [D loss: 0.528102] [G loss: 1.033683] [AE loss: 1.229720]\n",
            "[Epoch 0/10] [Batch 838/7121] [D loss: 0.473589] [G loss: 1.081133] [AE loss: 1.809783]\n",
            "[Epoch 0/10] [Batch 839/7121] [D loss: 0.430902] [G loss: 1.145499] [AE loss: 1.843204]\n",
            "[Epoch 0/10] [Batch 840/7121] [D loss: 0.446691] [G loss: 1.139776] [AE loss: 1.269464]\n",
            "[Epoch 0/10] [Batch 841/7121] [D loss: 0.496716] [G loss: 1.140351] [AE loss: 1.341945]\n",
            "[Epoch 0/10] [Batch 842/7121] [D loss: 0.463134] [G loss: 1.148896] [AE loss: 1.595606]\n",
            "[Epoch 0/10] [Batch 843/7121] [D loss: 0.483094] [G loss: 1.122009] [AE loss: 1.455097]\n",
            "[Epoch 0/10] [Batch 844/7121] [D loss: 0.515786] [G loss: 1.075914] [AE loss: 1.333290]\n",
            "[Epoch 0/10] [Batch 845/7121] [D loss: 0.475923] [G loss: 1.068760] [AE loss: 1.271809]\n",
            "[Epoch 0/10] [Batch 846/7121] [D loss: 0.502068] [G loss: 1.053048] [AE loss: 1.215707]\n",
            "[Epoch 0/10] [Batch 847/7121] [D loss: 0.499331] [G loss: 1.048124] [AE loss: 1.216351]\n",
            "[Epoch 0/10] [Batch 848/7121] [D loss: 0.491615] [G loss: 0.984125] [AE loss: 1.784044]\n",
            "[Epoch 0/10] [Batch 849/7121] [D loss: 0.512045] [G loss: 0.966543] [AE loss: 1.262498]\n",
            "[Epoch 0/10] [Batch 850/7121] [D loss: 0.460759] [G loss: 0.881330] [AE loss: 1.938196]\n",
            "[Epoch 0/10] [Batch 851/7121] [D loss: 0.525153] [G loss: 0.866086] [AE loss: 1.385680]\n",
            "[Epoch 0/10] [Batch 852/7121] [D loss: 0.478812] [G loss: 0.842891] [AE loss: 1.196243]\n",
            "[Epoch 0/10] [Batch 853/7121] [D loss: 0.508997] [G loss: 0.885632] [AE loss: 1.862069]\n",
            "[Epoch 0/10] [Batch 854/7121] [D loss: 0.520313] [G loss: 0.851808] [AE loss: 1.351631]\n",
            "[Epoch 0/10] [Batch 855/7121] [D loss: 0.474413] [G loss: 0.894991] [AE loss: 1.647698]\n",
            "[Epoch 0/10] [Batch 856/7121] [D loss: 0.452275] [G loss: 0.882884] [AE loss: 1.264652]\n",
            "[Epoch 0/10] [Batch 857/7121] [D loss: 0.523910] [G loss: 0.886547] [AE loss: 1.224359]\n",
            "[Epoch 0/10] [Batch 858/7121] [D loss: 0.501109] [G loss: 0.872899] [AE loss: 1.348480]\n",
            "[Epoch 0/10] [Batch 859/7121] [D loss: 0.537387] [G loss: 0.892838] [AE loss: 1.040850]\n",
            "[Epoch 0/10] [Batch 860/7121] [D loss: 0.668828] [G loss: 0.773654] [AE loss: 1.193096]\n",
            "[Epoch 0/10] [Batch 861/7121] [D loss: 0.705682] [G loss: 0.713515] [AE loss: 1.243271]\n",
            "[Epoch 0/10] [Batch 862/7121] [D loss: 0.693457] [G loss: 0.655855] [AE loss: 1.342835]\n",
            "[Epoch 0/10] [Batch 863/7121] [D loss: 0.845259] [G loss: 0.601696] [AE loss: 1.187120]\n",
            "[Epoch 0/10] [Batch 864/7121] [D loss: 0.721372] [G loss: 0.605792] [AE loss: 1.053217]\n",
            "[Epoch 0/10] [Batch 865/7121] [D loss: 0.786419] [G loss: 0.619098] [AE loss: 1.300296]\n",
            "[Epoch 0/10] [Batch 866/7121] [D loss: 0.760835] [G loss: 0.628578] [AE loss: 1.490251]\n",
            "[Epoch 0/10] [Batch 867/7121] [D loss: 0.707361] [G loss: 0.711284] [AE loss: 1.524783]\n",
            "[Epoch 0/10] [Batch 868/7121] [D loss: 0.713718] [G loss: 0.769793] [AE loss: 1.358966]\n",
            "[Epoch 0/10] [Batch 869/7121] [D loss: 0.618573] [G loss: 0.848002] [AE loss: 1.236161]\n",
            "[Epoch 0/10] [Batch 870/7121] [D loss: 0.647584] [G loss: 0.932532] [AE loss: 1.150788]\n",
            "[Epoch 0/10] [Batch 871/7121] [D loss: 0.522293] [G loss: 1.014415] [AE loss: 1.678442]\n",
            "[Epoch 0/10] [Batch 872/7121] [D loss: 0.633071] [G loss: 1.104583] [AE loss: 1.251000]\n",
            "[Epoch 0/10] [Batch 873/7121] [D loss: 0.667744] [G loss: 1.172087] [AE loss: 1.269269]\n",
            "[Epoch 0/10] [Batch 874/7121] [D loss: 0.738226] [G loss: 1.263803] [AE loss: 1.112074]\n",
            "[Epoch 0/10] [Batch 875/7121] [D loss: 0.613054] [G loss: 1.250549] [AE loss: 1.146878]\n",
            "[Epoch 0/10] [Batch 876/7121] [D loss: 0.844649] [G loss: 1.218529] [AE loss: 1.282580]\n",
            "[Epoch 0/10] [Batch 877/7121] [D loss: 0.634595] [G loss: 1.150214] [AE loss: 1.165193]\n",
            "[Epoch 0/10] [Batch 878/7121] [D loss: 0.766569] [G loss: 1.004745] [AE loss: 1.274557]\n",
            "[Epoch 0/10] [Batch 879/7121] [D loss: 1.000082] [G loss: 0.889725] [AE loss: 1.577650]\n",
            "[Epoch 0/10] [Batch 880/7121] [D loss: 0.944524] [G loss: 0.751551] [AE loss: 1.498126]\n",
            "[Epoch 0/10] [Batch 881/7121] [D loss: 0.783770] [G loss: 0.693763] [AE loss: 1.091576]\n",
            "[Epoch 0/10] [Batch 882/7121] [D loss: 0.978483] [G loss: 0.641598] [AE loss: 1.128265]\n",
            "[Epoch 0/10] [Batch 883/7121] [D loss: 0.938859] [G loss: 0.475441] [AE loss: 1.389675]\n",
            "[Epoch 0/10] [Batch 884/7121] [D loss: 0.989414] [G loss: 0.421088] [AE loss: 1.087362]\n",
            "[Epoch 0/10] [Batch 885/7121] [D loss: 1.175113] [G loss: 0.454754] [AE loss: 1.039421]\n",
            "[Epoch 0/10] [Batch 886/7121] [D loss: 1.173265] [G loss: 0.430925] [AE loss: 1.116854]\n",
            "[Epoch 0/10] [Batch 887/7121] [D loss: 0.946048] [G loss: 0.442013] [AE loss: 1.047943]\n",
            "[Epoch 0/10] [Batch 888/7121] [D loss: 1.022439] [G loss: 0.430342] [AE loss: 1.224138]\n",
            "[Epoch 0/10] [Batch 889/7121] [D loss: 1.056701] [G loss: 0.447377] [AE loss: 0.911620]\n",
            "[Epoch 0/10] [Batch 890/7121] [D loss: 1.033069] [G loss: 0.456522] [AE loss: 0.961843]\n",
            "[Epoch 0/10] [Batch 891/7121] [D loss: 0.870930] [G loss: 0.451827] [AE loss: 1.365899]\n",
            "[Epoch 0/10] [Batch 892/7121] [D loss: 0.870266] [G loss: 0.460341] [AE loss: 1.207772]\n",
            "[Epoch 0/10] [Batch 893/7121] [D loss: 0.899716] [G loss: 0.461076] [AE loss: 1.106642]\n",
            "[Epoch 0/10] [Batch 894/7121] [D loss: 0.891546] [G loss: 0.483735] [AE loss: 0.988291]\n",
            "[Epoch 0/10] [Batch 895/7121] [D loss: 0.739748] [G loss: 0.515820] [AE loss: 4.464139]\n",
            "[Epoch 0/10] [Batch 896/7121] [D loss: 0.816275] [G loss: 0.517347] [AE loss: 2.296580]\n",
            "[Epoch 0/10] [Batch 897/7121] [D loss: 0.824020] [G loss: 0.548195] [AE loss: 1.647194]\n",
            "[Epoch 0/10] [Batch 898/7121] [D loss: 0.757216] [G loss: 0.555720] [AE loss: 0.833977]\n",
            "[Epoch 0/10] [Batch 899/7121] [D loss: 0.877806] [G loss: 0.549673] [AE loss: 1.038639]\n",
            "[Epoch 0/10] [Batch 900/7121] [D loss: 0.777401] [G loss: 0.558863] [AE loss: 0.992457]\n",
            "[Epoch 0/10] [Batch 901/7121] [D loss: 0.806629] [G loss: 0.568921] [AE loss: 1.240146]\n",
            "[Epoch 0/10] [Batch 902/7121] [D loss: 0.695483] [G loss: 0.564146] [AE loss: 1.471604]\n",
            "[Epoch 0/10] [Batch 903/7121] [D loss: 0.731825] [G loss: 0.573048] [AE loss: 1.176347]\n",
            "[Epoch 0/10] [Batch 904/7121] [D loss: 0.662226] [G loss: 0.573429] [AE loss: 1.621763]\n",
            "[Epoch 0/10] [Batch 905/7121] [D loss: 0.786160] [G loss: 0.587673] [AE loss: 1.175202]\n",
            "[Epoch 0/10] [Batch 906/7121] [D loss: 0.711563] [G loss: 0.612324] [AE loss: 0.833696]\n",
            "[Epoch 0/10] [Batch 907/7121] [D loss: 0.619398] [G loss: 0.609292] [AE loss: 0.989556]\n",
            "[Epoch 0/10] [Batch 908/7121] [D loss: 0.658911] [G loss: 0.599157] [AE loss: 1.239784]\n",
            "[Epoch 0/10] [Batch 909/7121] [D loss: 0.724076] [G loss: 0.601320] [AE loss: 1.150059]\n",
            "[Epoch 0/10] [Batch 910/7121] [D loss: 0.661790] [G loss: 0.640095] [AE loss: 1.057953]\n",
            "[Epoch 0/10] [Batch 911/7121] [D loss: 0.650529] [G loss: 0.625116] [AE loss: 0.971400]\n",
            "[Epoch 0/10] [Batch 912/7121] [D loss: 0.651311] [G loss: 0.659733] [AE loss: 1.098535]\n",
            "[Epoch 0/10] [Batch 913/7121] [D loss: 0.678296] [G loss: 0.648518] [AE loss: 1.005563]\n",
            "[Epoch 0/10] [Batch 914/7121] [D loss: 0.726835] [G loss: 0.651705] [AE loss: 0.802871]\n",
            "[Epoch 0/10] [Batch 915/7121] [D loss: 0.642409] [G loss: 0.676737] [AE loss: 1.122273]\n",
            "[Epoch 0/10] [Batch 916/7121] [D loss: 0.626293] [G loss: 0.673160] [AE loss: 1.350581]\n",
            "[Epoch 0/10] [Batch 917/7121] [D loss: 0.666482] [G loss: 0.699040] [AE loss: 0.936486]\n",
            "[Epoch 0/10] [Batch 918/7121] [D loss: 0.576250] [G loss: 0.707496] [AE loss: 1.521599]\n",
            "[Epoch 0/10] [Batch 919/7121] [D loss: 0.653038] [G loss: 0.720516] [AE loss: 0.887996]\n",
            "[Epoch 0/10] [Batch 920/7121] [D loss: 0.632793] [G loss: 0.719608] [AE loss: 1.975452]\n",
            "[Epoch 0/10] [Batch 921/7121] [D loss: 0.646904] [G loss: 0.725334] [AE loss: 0.788157]\n",
            "[Epoch 0/10] [Batch 922/7121] [D loss: 0.654818] [G loss: 0.735327] [AE loss: 1.154472]\n",
            "[Epoch 0/10] [Batch 923/7121] [D loss: 0.608836] [G loss: 0.738818] [AE loss: 1.347284]\n",
            "[Epoch 0/10] [Batch 924/7121] [D loss: 0.637346] [G loss: 0.747038] [AE loss: 1.091962]\n",
            "[Epoch 0/10] [Batch 925/7121] [D loss: 0.585745] [G loss: 0.746562] [AE loss: 1.383843]\n",
            "[Epoch 0/10] [Batch 926/7121] [D loss: 0.627305] [G loss: 0.763385] [AE loss: 1.453436]\n",
            "[Epoch 0/10] [Batch 927/7121] [D loss: 0.580100] [G loss: 0.787641] [AE loss: 0.997031]\n",
            "[Epoch 0/10] [Batch 928/7121] [D loss: 0.581195] [G loss: 0.798029] [AE loss: 2.095558]\n",
            "[Epoch 0/10] [Batch 929/7121] [D loss: 0.703636] [G loss: 0.823989] [AE loss: 0.789043]\n",
            "[Epoch 0/10] [Batch 930/7121] [D loss: 0.654033] [G loss: 0.845389] [AE loss: 0.854866]\n",
            "[Epoch 0/10] [Batch 931/7121] [D loss: 0.626735] [G loss: 0.871855] [AE loss: 1.034532]\n",
            "[Epoch 0/10] [Batch 932/7121] [D loss: 0.556019] [G loss: 0.901292] [AE loss: 5.518889]\n",
            "[Epoch 0/10] [Batch 933/7121] [D loss: 0.608393] [G loss: 0.942416] [AE loss: 1.241997]\n",
            "[Epoch 0/10] [Batch 934/7121] [D loss: 0.539450] [G loss: 0.982892] [AE loss: 1.382509]\n",
            "[Epoch 0/10] [Batch 935/7121] [D loss: 0.605446] [G loss: 0.996342] [AE loss: 0.906988]\n",
            "[Epoch 0/10] [Batch 936/7121] [D loss: 0.522004] [G loss: 1.009881] [AE loss: 0.953022]\n",
            "[Epoch 0/10] [Batch 937/7121] [D loss: 0.494478] [G loss: 1.027223] [AE loss: 1.149234]\n",
            "[Epoch 0/10] [Batch 938/7121] [D loss: 0.537642] [G loss: 1.036601] [AE loss: 1.056284]\n",
            "[Epoch 0/10] [Batch 939/7121] [D loss: 0.566344] [G loss: 1.039779] [AE loss: 2.622699]\n",
            "[Epoch 0/10] [Batch 940/7121] [D loss: 0.513130] [G loss: 1.003841] [AE loss: 1.324834]\n",
            "[Epoch 0/10] [Batch 941/7121] [D loss: 0.529563] [G loss: 0.955494] [AE loss: 3.082561]\n",
            "[Epoch 0/10] [Batch 942/7121] [D loss: 0.481395] [G loss: 0.920699] [AE loss: 1.935138]\n",
            "[Epoch 0/10] [Batch 943/7121] [D loss: 0.534731] [G loss: 0.838434] [AE loss: 0.985293]\n",
            "[Epoch 0/10] [Batch 944/7121] [D loss: 0.552246] [G loss: 0.788829] [AE loss: 1.049487]\n",
            "[Epoch 0/10] [Batch 945/7121] [D loss: 0.602682] [G loss: 0.728459] [AE loss: 1.263285]\n",
            "[Epoch 0/10] [Batch 946/7121] [D loss: 0.568212] [G loss: 0.692325] [AE loss: 1.112832]\n",
            "[Epoch 0/10] [Batch 947/7121] [D loss: 0.547476] [G loss: 0.669401] [AE loss: 1.231282]\n",
            "[Epoch 0/10] [Batch 948/7121] [D loss: 0.649685] [G loss: 0.582033] [AE loss: 1.116271]\n",
            "[Epoch 0/10] [Batch 949/7121] [D loss: 0.731246] [G loss: 0.560144] [AE loss: 1.554271]\n",
            "[Epoch 0/10] [Batch 950/7121] [D loss: 0.746239] [G loss: 0.522266] [AE loss: 1.186306]\n",
            "[Epoch 0/10] [Batch 951/7121] [D loss: 0.688790] [G loss: 0.534126] [AE loss: 1.134314]\n",
            "[Epoch 0/10] [Batch 952/7121] [D loss: 0.718648] [G loss: 0.532781] [AE loss: 6.040151]\n",
            "[Epoch 0/10] [Batch 953/7121] [D loss: 0.760276] [G loss: 0.503536] [AE loss: 1.483442]\n",
            "[Epoch 0/10] [Batch 954/7121] [D loss: 0.800605] [G loss: 0.549694] [AE loss: 1.149741]\n",
            "[Epoch 0/10] [Batch 955/7121] [D loss: 0.694157] [G loss: 0.562525] [AE loss: 1.167776]\n",
            "[Epoch 0/10] [Batch 956/7121] [D loss: 0.702310] [G loss: 0.547949] [AE loss: 1.129599]\n",
            "[Epoch 0/10] [Batch 957/7121] [D loss: 0.641119] [G loss: 0.562561] [AE loss: 1.330530]\n",
            "[Epoch 0/10] [Batch 958/7121] [D loss: 0.683372] [G loss: 0.629680] [AE loss: 1.100422]\n",
            "[Epoch 0/10] [Batch 959/7121] [D loss: 0.727068] [G loss: 0.610510] [AE loss: 1.116711]\n",
            "[Epoch 0/10] [Batch 960/7121] [D loss: 0.700163] [G loss: 0.626784] [AE loss: 1.588768]\n",
            "[Epoch 0/10] [Batch 961/7121] [D loss: 0.650285] [G loss: 0.584717] [AE loss: 1.012133]\n",
            "[Epoch 0/10] [Batch 962/7121] [D loss: 0.638724] [G loss: 0.604350] [AE loss: 1.136294]\n",
            "[Epoch 0/10] [Batch 963/7121] [D loss: 0.718660] [G loss: 0.623633] [AE loss: 1.669085]\n",
            "[Epoch 0/10] [Batch 964/7121] [D loss: 0.650180] [G loss: 0.594051] [AE loss: 1.184971]\n",
            "[Epoch 0/10] [Batch 965/7121] [D loss: 0.697681] [G loss: 0.565995] [AE loss: 1.175189]\n",
            "[Epoch 0/10] [Batch 966/7121] [D loss: 0.670316] [G loss: 0.576583] [AE loss: 1.116836]\n",
            "[Epoch 0/10] [Batch 967/7121] [D loss: 0.704049] [G loss: 0.598829] [AE loss: 1.190215]\n",
            "[Epoch 0/10] [Batch 968/7121] [D loss: 0.684030] [G loss: 0.586674] [AE loss: 2.078763]\n",
            "[Epoch 0/10] [Batch 969/7121] [D loss: 0.619808] [G loss: 0.571704] [AE loss: 1.488171]\n",
            "[Epoch 0/10] [Batch 970/7121] [D loss: 0.777163] [G loss: 0.570340] [AE loss: 1.121081]\n",
            "[Epoch 0/10] [Batch 971/7121] [D loss: 0.701135] [G loss: 0.563198] [AE loss: 1.771702]\n",
            "[Epoch 0/10] [Batch 972/7121] [D loss: 0.742306] [G loss: 0.574214] [AE loss: 1.105204]\n",
            "[Epoch 0/10] [Batch 973/7121] [D loss: 0.756186] [G loss: 0.543614] [AE loss: 1.233886]\n",
            "[Epoch 0/10] [Batch 974/7121] [D loss: 0.719836] [G loss: 0.541357] [AE loss: 1.934779]\n",
            "[Epoch 0/10] [Batch 975/7121] [D loss: 0.667988] [G loss: 0.559657] [AE loss: 1.409979]\n",
            "[Epoch 0/10] [Batch 976/7121] [D loss: 0.712825] [G loss: 0.572407] [AE loss: 3.149285]\n",
            "[Epoch 0/10] [Batch 977/7121] [D loss: 0.720434] [G loss: 0.570009] [AE loss: 1.204878]\n",
            "[Epoch 0/10] [Batch 978/7121] [D loss: 0.732672] [G loss: 0.628942] [AE loss: 1.122674]\n",
            "[Epoch 0/10] [Batch 979/7121] [D loss: 0.775300] [G loss: 0.600715] [AE loss: 1.088475]\n",
            "[Epoch 0/10] [Batch 980/7121] [D loss: 0.744769] [G loss: 0.617481] [AE loss: 2.341488]\n",
            "[Epoch 0/10] [Batch 981/7121] [D loss: 0.706156] [G loss: 0.666884] [AE loss: 1.588331]\n",
            "[Epoch 0/10] [Batch 982/7121] [D loss: 0.698363] [G loss: 0.715483] [AE loss: 1.335406]\n",
            "[Epoch 0/10] [Batch 983/7121] [D loss: 0.615432] [G loss: 0.762671] [AE loss: 1.618896]\n",
            "[Epoch 0/10] [Batch 984/7121] [D loss: 0.568441] [G loss: 0.797037] [AE loss: 1.560476]\n",
            "[Epoch 0/10] [Batch 985/7121] [D loss: 0.503052] [G loss: 0.830039] [AE loss: 2.238133]\n",
            "[Epoch 0/10] [Batch 986/7121] [D loss: 0.514394] [G loss: 0.918929] [AE loss: 1.787510]\n",
            "[Epoch 0/10] [Batch 987/7121] [D loss: 0.501853] [G loss: 0.941214] [AE loss: 1.265741]\n",
            "[Epoch 0/10] [Batch 988/7121] [D loss: 0.507615] [G loss: 1.018518] [AE loss: 1.519004]\n",
            "[Epoch 0/10] [Batch 989/7121] [D loss: 0.554756] [G loss: 1.039649] [AE loss: 1.323164]\n",
            "[Epoch 0/10] [Batch 990/7121] [D loss: 0.481198] [G loss: 1.075497] [AE loss: 1.495809]\n",
            "[Epoch 0/10] [Batch 991/7121] [D loss: 0.596123] [G loss: 1.062761] [AE loss: 1.322310]\n",
            "[Epoch 0/10] [Batch 992/7121] [D loss: 0.533103] [G loss: 1.035161] [AE loss: 1.410350]\n",
            "[Epoch 0/10] [Batch 993/7121] [D loss: 0.545519] [G loss: 1.023014] [AE loss: 1.335514]\n",
            "[Epoch 0/10] [Batch 994/7121] [D loss: 0.509973] [G loss: 1.006367] [AE loss: 1.480499]\n",
            "[Epoch 0/10] [Batch 995/7121] [D loss: 0.592940] [G loss: 0.915956] [AE loss: 1.199178]\n",
            "[Epoch 0/10] [Batch 996/7121] [D loss: 0.536422] [G loss: 0.901419] [AE loss: 1.236443]\n",
            "[Epoch 0/10] [Batch 997/7121] [D loss: 0.601983] [G loss: 0.826527] [AE loss: 1.183278]\n",
            "[Epoch 0/10] [Batch 998/7121] [D loss: 0.666416] [G loss: 0.813582] [AE loss: 1.148022]\n",
            "[Epoch 0/10] [Batch 999/7121] [D loss: 0.571765] [G loss: 0.769964] [AE loss: 1.271699]\n",
            "[Epoch 0/10] [Batch 1000/7121] [D loss: 0.631254] [G loss: 0.718839] [AE loss: 1.156467]\n",
            "[Epoch 0/10] [Batch 1001/7121] [D loss: 0.658598] [G loss: 0.673487] [AE loss: 2.118811]\n",
            "[Epoch 0/10] [Batch 1002/7121] [D loss: 0.657452] [G loss: 0.645644] [AE loss: 1.074777]\n",
            "[Epoch 0/10] [Batch 1003/7121] [D loss: 0.659148] [G loss: 0.640456] [AE loss: 1.456906]\n",
            "[Epoch 0/10] [Batch 1004/7121] [D loss: 0.643328] [G loss: 0.642743] [AE loss: 2.942516]\n",
            "[Epoch 0/10] [Batch 1005/7121] [D loss: 0.704632] [G loss: 0.629252] [AE loss: 1.273879]\n",
            "[Epoch 0/10] [Batch 1006/7121] [D loss: 0.648659] [G loss: 0.601829] [AE loss: 1.622630]\n",
            "[Epoch 0/10] [Batch 1007/7121] [D loss: 0.630725] [G loss: 0.602863] [AE loss: 0.954349]\n",
            "[Epoch 0/10] [Batch 1008/7121] [D loss: 0.746141] [G loss: 0.612318] [AE loss: 2.230334]\n",
            "[Epoch 0/10] [Batch 1009/7121] [D loss: 0.583946] [G loss: 0.625499] [AE loss: 1.498926]\n",
            "[Epoch 0/10] [Batch 1010/7121] [D loss: 0.689778] [G loss: 0.627354] [AE loss: 0.948770]\n",
            "[Epoch 0/10] [Batch 1011/7121] [D loss: 0.775250] [G loss: 0.619009] [AE loss: 1.054447]\n",
            "[Epoch 0/10] [Batch 1012/7121] [D loss: 0.606785] [G loss: 0.626475] [AE loss: 2.072345]\n",
            "[Epoch 0/10] [Batch 1013/7121] [D loss: 0.812953] [G loss: 0.614614] [AE loss: 1.357465]\n",
            "[Epoch 0/10] [Batch 1014/7121] [D loss: 0.691985] [G loss: 0.624968] [AE loss: 1.225260]\n",
            "[Epoch 0/10] [Batch 1015/7121] [D loss: 0.836999] [G loss: 0.620735] [AE loss: 1.229521]\n",
            "[Epoch 0/10] [Batch 1016/7121] [D loss: 0.689228] [G loss: 0.621898] [AE loss: 0.935247]\n",
            "[Epoch 0/10] [Batch 1017/7121] [D loss: 0.683211] [G loss: 0.613047] [AE loss: 1.079373]\n",
            "[Epoch 0/10] [Batch 1018/7121] [D loss: 0.644300] [G loss: 0.627568] [AE loss: 1.025136]\n",
            "[Epoch 0/10] [Batch 1019/7121] [D loss: 0.765878] [G loss: 0.613225] [AE loss: 1.108619]\n",
            "[Epoch 0/10] [Batch 1020/7121] [D loss: 0.686706] [G loss: 0.638151] [AE loss: 0.850610]\n",
            "[Epoch 0/10] [Batch 1021/7121] [D loss: 0.698782] [G loss: 0.637521] [AE loss: 1.482197]\n",
            "[Epoch 0/10] [Batch 1022/7121] [D loss: 0.734690] [G loss: 0.620924] [AE loss: 0.826615]\n",
            "[Epoch 0/10] [Batch 1023/7121] [D loss: 0.662638] [G loss: 0.637224] [AE loss: 1.123357]\n",
            "[Epoch 0/10] [Batch 1024/7121] [D loss: 0.818754] [G loss: 0.642388] [AE loss: 0.967952]\n",
            "[Epoch 0/10] [Batch 1025/7121] [D loss: 0.759629] [G loss: 0.644713] [AE loss: 1.041116]\n",
            "[Epoch 0/10] [Batch 1026/7121] [D loss: 0.598380] [G loss: 0.677455] [AE loss: 1.530799]\n",
            "[Epoch 0/10] [Batch 1027/7121] [D loss: 0.630515] [G loss: 0.668124] [AE loss: 1.054651]\n",
            "[Epoch 0/10] [Batch 1028/7121] [D loss: 0.671854] [G loss: 0.679363] [AE loss: 0.894098]\n",
            "[Epoch 0/10] [Batch 1029/7121] [D loss: 0.745488] [G loss: 0.697770] [AE loss: 1.340268]\n",
            "[Epoch 0/10] [Batch 1030/7121] [D loss: 0.766264] [G loss: 0.715296] [AE loss: 1.025743]\n",
            "[Epoch 0/10] [Batch 1031/7121] [D loss: 0.700407] [G loss: 0.731486] [AE loss: 1.011895]\n",
            "[Epoch 0/10] [Batch 1032/7121] [D loss: 0.622553] [G loss: 0.751497] [AE loss: 1.217218]\n",
            "[Epoch 0/10] [Batch 1033/7121] [D loss: 0.610092] [G loss: 0.793474] [AE loss: 1.925541]\n",
            "[Epoch 0/10] [Batch 1034/7121] [D loss: 0.634963] [G loss: 0.779462] [AE loss: 0.902894]\n",
            "[Epoch 0/10] [Batch 1035/7121] [D loss: 0.641384] [G loss: 0.771153] [AE loss: 1.299015]\n",
            "[Epoch 0/10] [Batch 1036/7121] [D loss: 0.679834] [G loss: 0.790152] [AE loss: 0.848890]\n",
            "[Epoch 0/10] [Batch 1037/7121] [D loss: 0.655330] [G loss: 0.774528] [AE loss: 0.907248]\n",
            "[Epoch 0/10] [Batch 1038/7121] [D loss: 0.656122] [G loss: 0.721376] [AE loss: 0.978820]\n",
            "[Epoch 0/10] [Batch 1039/7121] [D loss: 0.627022] [G loss: 0.707628] [AE loss: 1.488870]\n",
            "[Epoch 0/10] [Batch 1040/7121] [D loss: 0.686784] [G loss: 0.693518] [AE loss: 1.042214]\n",
            "[Epoch 0/10] [Batch 1041/7121] [D loss: 0.763094] [G loss: 0.659597] [AE loss: 1.052325]\n",
            "[Epoch 0/10] [Batch 1042/7121] [D loss: 0.667081] [G loss: 0.681628] [AE loss: 1.249787]\n",
            "[Epoch 0/10] [Batch 1043/7121] [D loss: 0.701075] [G loss: 0.663780] [AE loss: 1.087511]\n",
            "[Epoch 0/10] [Batch 1044/7121] [D loss: 0.715683] [G loss: 0.669819] [AE loss: 1.164262]\n",
            "[Epoch 0/10] [Batch 1045/7121] [D loss: 0.625556] [G loss: 0.688511] [AE loss: 1.601409]\n",
            "[Epoch 0/10] [Batch 1046/7121] [D loss: 0.697557] [G loss: 0.686404] [AE loss: 0.984839]\n",
            "[Epoch 0/10] [Batch 1047/7121] [D loss: 0.706406] [G loss: 0.711348] [AE loss: 1.227052]\n",
            "[Epoch 0/10] [Batch 1048/7121] [D loss: 0.604647] [G loss: 0.748715] [AE loss: 2.655921]\n",
            "[Epoch 0/10] [Batch 1049/7121] [D loss: 0.653987] [G loss: 0.753212] [AE loss: 1.412653]\n",
            "[Epoch 0/10] [Batch 1050/7121] [D loss: 0.653540] [G loss: 0.789779] [AE loss: 1.068716]\n",
            "[Epoch 0/10] [Batch 1051/7121] [D loss: 0.679321] [G loss: 0.829726] [AE loss: 1.054323]\n",
            "[Epoch 0/10] [Batch 1052/7121] [D loss: 0.558791] [G loss: 0.840350] [AE loss: 1.892763]\n",
            "[Epoch 0/10] [Batch 1053/7121] [D loss: 0.598381] [G loss: 0.859231] [AE loss: 1.872873]\n",
            "[Epoch 0/10] [Batch 1054/7121] [D loss: 0.592072] [G loss: 0.916217] [AE loss: 1.643942]\n",
            "[Epoch 0/10] [Batch 1055/7121] [D loss: 0.610168] [G loss: 0.896649] [AE loss: 1.397077]\n",
            "[Epoch 0/10] [Batch 1056/7121] [D loss: 0.660209] [G loss: 0.893857] [AE loss: 1.191671]\n",
            "[Epoch 0/10] [Batch 1057/7121] [D loss: 0.648291] [G loss: 0.917158] [AE loss: 1.222503]\n",
            "[Epoch 0/10] [Batch 1058/7121] [D loss: 0.616179] [G loss: 0.910905] [AE loss: 1.318436]\n",
            "[Epoch 0/10] [Batch 1059/7121] [D loss: 0.614303] [G loss: 0.886230] [AE loss: 2.962719]\n",
            "[Epoch 0/10] [Batch 1060/7121] [D loss: 0.568420] [G loss: 0.915130] [AE loss: 1.318172]\n",
            "[Epoch 0/10] [Batch 1061/7121] [D loss: 0.525211] [G loss: 0.903910] [AE loss: 1.250646]\n",
            "[Epoch 0/10] [Batch 1062/7121] [D loss: 0.552450] [G loss: 0.925590] [AE loss: 1.278060]\n",
            "[Epoch 0/10] [Batch 1063/7121] [D loss: 0.580774] [G loss: 0.912048] [AE loss: 1.542426]\n",
            "[Epoch 0/10] [Batch 1064/7121] [D loss: 0.639752] [G loss: 0.914995] [AE loss: 1.863681]\n",
            "[Epoch 0/10] [Batch 1065/7121] [D loss: 0.605927] [G loss: 0.879256] [AE loss: 1.174276]\n",
            "[Epoch 0/10] [Batch 1066/7121] [D loss: 0.616428] [G loss: 0.906057] [AE loss: 1.164150]\n",
            "[Epoch 0/10] [Batch 1067/7121] [D loss: 0.588468] [G loss: 0.921370] [AE loss: 1.347058]\n",
            "[Epoch 0/10] [Batch 1068/7121] [D loss: 0.626594] [G loss: 0.929457] [AE loss: 1.162466]\n",
            "[Epoch 0/10] [Batch 1069/7121] [D loss: 0.649918] [G loss: 0.949511] [AE loss: 1.164516]\n",
            "[Epoch 0/10] [Batch 1070/7121] [D loss: 0.526573] [G loss: 0.943924] [AE loss: 1.583877]\n",
            "[Epoch 0/10] [Batch 1071/7121] [D loss: 0.564660] [G loss: 0.949405] [AE loss: 1.695454]\n",
            "[Epoch 0/10] [Batch 1072/7121] [D loss: 0.610073] [G loss: 0.934889] [AE loss: 1.239659]\n",
            "[Epoch 0/10] [Batch 1073/7121] [D loss: 0.648364] [G loss: 0.946973] [AE loss: 1.284530]\n",
            "[Epoch 0/10] [Batch 1074/7121] [D loss: 0.610947] [G loss: 0.944605] [AE loss: 1.410281]\n",
            "[Epoch 0/10] [Batch 1075/7121] [D loss: 0.624121] [G loss: 0.911813] [AE loss: 1.244732]\n",
            "[Epoch 0/10] [Batch 1076/7121] [D loss: 0.592101] [G loss: 0.881980] [AE loss: 1.267233]\n",
            "[Epoch 0/10] [Batch 1077/7121] [D loss: 0.457373] [G loss: 0.882392] [AE loss: 1.937006]\n",
            "[Epoch 0/10] [Batch 1078/7121] [D loss: 0.500239] [G loss: 0.864589] [AE loss: 1.534745]\n",
            "[Epoch 0/10] [Batch 1079/7121] [D loss: 0.568373] [G loss: 0.886574] [AE loss: 1.382706]\n",
            "[Epoch 0/10] [Batch 1080/7121] [D loss: 0.689622] [G loss: 0.838092] [AE loss: 1.169191]\n",
            "[Epoch 0/10] [Batch 1081/7121] [D loss: 0.595577] [G loss: 0.840826] [AE loss: 1.272104]\n",
            "[Epoch 0/10] [Batch 1082/7121] [D loss: 0.559799] [G loss: 0.895188] [AE loss: 1.638512]\n",
            "[Epoch 0/10] [Batch 1083/7121] [D loss: 0.739633] [G loss: 0.826138] [AE loss: 1.232554]\n",
            "[Epoch 0/10] [Batch 1084/7121] [D loss: 0.560018] [G loss: 0.815080] [AE loss: 1.426733]\n",
            "[Epoch 0/10] [Batch 1085/7121] [D loss: 0.576307] [G loss: 0.800419] [AE loss: 1.366515]\n",
            "[Epoch 0/10] [Batch 1086/7121] [D loss: 0.628424] [G loss: 0.810856] [AE loss: 1.503263]\n",
            "[Epoch 0/10] [Batch 1087/7121] [D loss: 0.562904] [G loss: 0.814943] [AE loss: 1.551464]\n",
            "[Epoch 0/10] [Batch 1088/7121] [D loss: 0.615384] [G loss: 0.808968] [AE loss: 1.302246]\n",
            "[Epoch 0/10] [Batch 1089/7121] [D loss: 0.619013] [G loss: 0.781812] [AE loss: 1.346483]\n",
            "[Epoch 0/10] [Batch 1090/7121] [D loss: 0.610551] [G loss: 0.830027] [AE loss: 1.140966]\n",
            "[Epoch 0/10] [Batch 1091/7121] [D loss: 0.606474] [G loss: 0.795950] [AE loss: 1.748149]\n",
            "[Epoch 0/10] [Batch 1092/7121] [D loss: 0.626512] [G loss: 0.852514] [AE loss: 1.262305]\n",
            "[Epoch 0/10] [Batch 1093/7121] [D loss: 0.488749] [G loss: 0.883111] [AE loss: 2.254939]\n",
            "[Epoch 0/10] [Batch 1094/7121] [D loss: 0.606475] [G loss: 0.878815] [AE loss: 1.222810]\n",
            "[Epoch 0/10] [Batch 1095/7121] [D loss: 0.629642] [G loss: 0.864122] [AE loss: 1.450303]\n",
            "[Epoch 0/10] [Batch 1096/7121] [D loss: 0.527411] [G loss: 0.896138] [AE loss: 1.981282]\n",
            "[Epoch 0/10] [Batch 1097/7121] [D loss: 0.645768] [G loss: 0.916584] [AE loss: 1.639673]\n",
            "[Epoch 0/10] [Batch 1098/7121] [D loss: 0.649866] [G loss: 0.940482] [AE loss: 1.202993]\n",
            "[Epoch 0/10] [Batch 1099/7121] [D loss: 0.645830] [G loss: 0.933968] [AE loss: 1.404136]\n",
            "[Epoch 0/10] [Batch 1100/7121] [D loss: 0.540222] [G loss: 0.928328] [AE loss: 1.464068]\n",
            "[Epoch 0/10] [Batch 1101/7121] [D loss: 0.650085] [G loss: 0.936469] [AE loss: 1.441103]\n",
            "[Epoch 0/10] [Batch 1102/7121] [D loss: 0.537975] [G loss: 0.916293] [AE loss: 3.073576]\n",
            "[Epoch 0/10] [Batch 1103/7121] [D loss: 0.571651] [G loss: 0.906078] [AE loss: 1.463722]\n",
            "[Epoch 0/10] [Batch 1104/7121] [D loss: 0.616835] [G loss: 0.914035] [AE loss: 1.238076]\n",
            "[Epoch 0/10] [Batch 1105/7121] [D loss: 0.684654] [G loss: 0.893445] [AE loss: 1.247389]\n",
            "[Epoch 0/10] [Batch 1106/7121] [D loss: 0.592855] [G loss: 0.881435] [AE loss: 1.254970]\n",
            "[Epoch 0/10] [Batch 1107/7121] [D loss: 0.638478] [G loss: 0.886589] [AE loss: 1.168766]\n",
            "[Epoch 0/10] [Batch 1108/7121] [D loss: 0.562005] [G loss: 0.906466] [AE loss: 1.240179]\n",
            "[Epoch 0/10] [Batch 1109/7121] [D loss: 0.601967] [G loss: 0.895356] [AE loss: 1.454028]\n",
            "[Epoch 0/10] [Batch 1110/7121] [D loss: 0.629520] [G loss: 0.893778] [AE loss: 1.098179]\n",
            "[Epoch 0/10] [Batch 1111/7121] [D loss: 0.560298] [G loss: 0.939788] [AE loss: 1.321272]\n",
            "[Epoch 0/10] [Batch 1112/7121] [D loss: 0.626013] [G loss: 0.887120] [AE loss: 1.161467]\n",
            "[Epoch 0/10] [Batch 1113/7121] [D loss: 0.552626] [G loss: 0.924296] [AE loss: 1.552681]\n",
            "[Epoch 0/10] [Batch 1114/7121] [D loss: 0.684748] [G loss: 0.877535] [AE loss: 1.155183]\n",
            "[Epoch 0/10] [Batch 1115/7121] [D loss: 0.669500] [G loss: 0.888699] [AE loss: 1.704409]\n",
            "[Epoch 0/10] [Batch 1116/7121] [D loss: 0.610757] [G loss: 0.858319] [AE loss: 1.773847]\n",
            "[Epoch 0/10] [Batch 1117/7121] [D loss: 0.640237] [G loss: 0.887548] [AE loss: 1.496788]\n",
            "[Epoch 0/10] [Batch 1118/7121] [D loss: 0.542265] [G loss: 0.899006] [AE loss: 1.215199]\n",
            "[Epoch 0/10] [Batch 1119/7121] [D loss: 0.732286] [G loss: 0.856258] [AE loss: 1.171423]\n",
            "[Epoch 0/10] [Batch 1120/7121] [D loss: 0.629673] [G loss: 0.884827] [AE loss: 2.102010]\n",
            "[Epoch 0/10] [Batch 1121/7121] [D loss: 0.685897] [G loss: 0.889499] [AE loss: 3.592592]\n",
            "[Epoch 0/10] [Batch 1122/7121] [D loss: 0.694420] [G loss: 0.872537] [AE loss: 2.274343]\n",
            "[Epoch 0/10] [Batch 1123/7121] [D loss: 0.696298] [G loss: 0.919117] [AE loss: 1.182917]\n",
            "[Epoch 0/10] [Batch 1124/7121] [D loss: 0.635665] [G loss: 0.945015] [AE loss: 1.244690]\n",
            "[Epoch 0/10] [Batch 1125/7121] [D loss: 0.622983] [G loss: 0.947032] [AE loss: 1.329713]\n",
            "[Epoch 0/10] [Batch 1126/7121] [D loss: 0.595940] [G loss: 1.000681] [AE loss: 1.139001]\n",
            "[Epoch 0/10] [Batch 1127/7121] [D loss: 0.618369] [G loss: 1.056094] [AE loss: 1.124980]\n",
            "[Epoch 0/10] [Batch 1128/7121] [D loss: 0.565325] [G loss: 1.108530] [AE loss: 1.156982]\n",
            "[Epoch 0/10] [Batch 1129/7121] [D loss: 0.576997] [G loss: 1.147368] [AE loss: 1.242039]\n",
            "[Epoch 0/10] [Batch 1130/7121] [D loss: 0.555556] [G loss: 1.075243] [AE loss: 1.353949]\n",
            "[Epoch 0/10] [Batch 1131/7121] [D loss: 0.548391] [G loss: 1.072590] [AE loss: 1.256338]\n",
            "[Epoch 0/10] [Batch 1132/7121] [D loss: 0.579463] [G loss: 1.062140] [AE loss: 1.066574]\n",
            "[Epoch 0/10] [Batch 1133/7121] [D loss: 0.549809] [G loss: 1.027001] [AE loss: 1.111662]\n",
            "[Epoch 0/10] [Batch 1134/7121] [D loss: 0.564595] [G loss: 0.994864] [AE loss: 1.513876]\n",
            "[Epoch 0/10] [Batch 1135/7121] [D loss: 0.619297] [G loss: 0.960224] [AE loss: 1.271681]\n",
            "[Epoch 0/10] [Batch 1136/7121] [D loss: 0.471414] [G loss: 0.970122] [AE loss: 1.372296]\n",
            "[Epoch 0/10] [Batch 1137/7121] [D loss: 0.555299] [G loss: 0.956104] [AE loss: 1.344702]\n",
            "[Epoch 0/10] [Batch 1138/7121] [D loss: 0.613713] [G loss: 0.941162] [AE loss: 1.263072]\n",
            "[Epoch 0/10] [Batch 1139/7121] [D loss: 0.510240] [G loss: 0.933016] [AE loss: 1.305931]\n",
            "[Epoch 0/10] [Batch 1140/7121] [D loss: 0.604888] [G loss: 0.962868] [AE loss: 1.388451]\n",
            "[Epoch 0/10] [Batch 1141/7121] [D loss: 0.599211] [G loss: 0.923851] [AE loss: 1.286696]\n",
            "[Epoch 0/10] [Batch 1142/7121] [D loss: 0.528128] [G loss: 0.910163] [AE loss: 1.605173]\n",
            "[Epoch 0/10] [Batch 1143/7121] [D loss: 0.663486] [G loss: 0.934018] [AE loss: 1.223756]\n",
            "[Epoch 0/10] [Batch 1144/7121] [D loss: 0.548319] [G loss: 0.891819] [AE loss: 1.407828]\n",
            "[Epoch 0/10] [Batch 1145/7121] [D loss: 0.637750] [G loss: 0.898145] [AE loss: 1.106728]\n",
            "[Epoch 0/10] [Batch 1146/7121] [D loss: 0.580546] [G loss: 0.896094] [AE loss: 1.310244]\n",
            "[Epoch 0/10] [Batch 1147/7121] [D loss: 0.715965] [G loss: 0.896888] [AE loss: 0.960567]\n",
            "[Epoch 0/10] [Batch 1148/7121] [D loss: 0.635166] [G loss: 0.860733] [AE loss: 0.984517]\n",
            "[Epoch 0/10] [Batch 1149/7121] [D loss: 0.601287] [G loss: 0.855608] [AE loss: 1.174988]\n",
            "[Epoch 0/10] [Batch 1150/7121] [D loss: 0.579356] [G loss: 0.846113] [AE loss: 1.012933]\n",
            "[Epoch 0/10] [Batch 1151/7121] [D loss: 0.649754] [G loss: 0.793299] [AE loss: 1.143184]\n",
            "[Epoch 0/10] [Batch 1152/7121] [D loss: 0.573506] [G loss: 0.824032] [AE loss: 1.393751]\n",
            "[Epoch 0/10] [Batch 1153/7121] [D loss: 0.559498] [G loss: 0.748457] [AE loss: 1.283640]\n",
            "[Epoch 0/10] [Batch 1154/7121] [D loss: 0.627182] [G loss: 0.729610] [AE loss: 1.049992]\n",
            "[Epoch 0/10] [Batch 1155/7121] [D loss: 0.687667] [G loss: 0.710601] [AE loss: 1.085673]\n",
            "[Epoch 0/10] [Batch 1156/7121] [D loss: 0.634734] [G loss: 0.682984] [AE loss: 1.072811]\n",
            "[Epoch 0/10] [Batch 1157/7121] [D loss: 0.681218] [G loss: 0.684495] [AE loss: 1.068559]\n",
            "[Epoch 0/10] [Batch 1158/7121] [D loss: 0.628430] [G loss: 0.671565] [AE loss: 1.540313]\n",
            "[Epoch 0/10] [Batch 1159/7121] [D loss: 0.717980] [G loss: 0.629335] [AE loss: 1.583338]\n",
            "[Epoch 0/10] [Batch 1160/7121] [D loss: 0.713574] [G loss: 0.631835] [AE loss: 1.245277]\n",
            "[Epoch 0/10] [Batch 1161/7121] [D loss: 0.692979] [G loss: 0.629529] [AE loss: 1.324744]\n",
            "[Epoch 0/10] [Batch 1162/7121] [D loss: 0.767987] [G loss: 0.594381] [AE loss: 3.065555]\n",
            "[Epoch 0/10] [Batch 1163/7121] [D loss: 0.693174] [G loss: 0.594063] [AE loss: 1.225737]\n",
            "[Epoch 0/10] [Batch 1164/7121] [D loss: 0.752466] [G loss: 0.604970] [AE loss: 0.977178]\n",
            "[Epoch 0/10] [Batch 1165/7121] [D loss: 0.746756] [G loss: 0.572172] [AE loss: 0.929426]\n",
            "[Epoch 0/10] [Batch 1166/7121] [D loss: 0.847897] [G loss: 0.566688] [AE loss: 0.836493]\n",
            "[Epoch 0/10] [Batch 1167/7121] [D loss: 0.710533] [G loss: 0.569545] [AE loss: 1.157835]\n",
            "[Epoch 0/10] [Batch 1168/7121] [D loss: 0.733094] [G loss: 0.575171] [AE loss: 1.404954]\n",
            "[Epoch 0/10] [Batch 1169/7121] [D loss: 0.735695] [G loss: 0.600698] [AE loss: 1.051306]\n",
            "[Epoch 0/10] [Batch 1170/7121] [D loss: 0.737077] [G loss: 0.565044] [AE loss: 1.183230]\n",
            "[Epoch 0/10] [Batch 1171/7121] [D loss: 0.759107] [G loss: 0.562462] [AE loss: 1.012991]\n",
            "[Epoch 0/10] [Batch 1172/7121] [D loss: 0.752632] [G loss: 0.575004] [AE loss: 1.241342]\n",
            "[Epoch 0/10] [Batch 1173/7121] [D loss: 0.772792] [G loss: 0.577181] [AE loss: 1.096184]\n",
            "[Epoch 0/10] [Batch 1174/7121] [D loss: 0.683731] [G loss: 0.586062] [AE loss: 1.362783]\n",
            "[Epoch 0/10] [Batch 1175/7121] [D loss: 0.806057] [G loss: 0.592112] [AE loss: 0.895263]\n",
            "[Epoch 0/10] [Batch 1176/7121] [D loss: 0.742878] [G loss: 0.583000] [AE loss: 0.977291]\n",
            "[Epoch 0/10] [Batch 1177/7121] [D loss: 0.823279] [G loss: 0.580964] [AE loss: 1.543885]\n",
            "[Epoch 0/10] [Batch 1178/7121] [D loss: 0.847489] [G loss: 0.552669] [AE loss: 4.343727]\n",
            "[Epoch 0/10] [Batch 1179/7121] [D loss: 0.720073] [G loss: 0.540021] [AE loss: 1.034514]\n",
            "[Epoch 0/10] [Batch 1180/7121] [D loss: 0.842222] [G loss: 0.552783] [AE loss: 1.000110]\n",
            "[Epoch 0/10] [Batch 1181/7121] [D loss: 0.735611] [G loss: 0.574455] [AE loss: 1.267507]\n",
            "[Epoch 0/10] [Batch 1182/7121] [D loss: 0.823151] [G loss: 0.549275] [AE loss: 0.956038]\n",
            "[Epoch 0/10] [Batch 1183/7121] [D loss: 0.679578] [G loss: 0.543269] [AE loss: 1.504627]\n",
            "[Epoch 0/10] [Batch 1184/7121] [D loss: 0.677578] [G loss: 0.546103] [AE loss: 1.378237]\n",
            "[Epoch 0/10] [Batch 1185/7121] [D loss: 0.738239] [G loss: 0.537268] [AE loss: 0.936418]\n",
            "[Epoch 0/10] [Batch 1186/7121] [D loss: 0.855447] [G loss: 0.553741] [AE loss: 1.086194]\n",
            "[Epoch 0/10] [Batch 1187/7121] [D loss: 0.739559] [G loss: 0.548651] [AE loss: 1.039569]\n",
            "[Epoch 0/10] [Batch 1188/7121] [D loss: 0.811251] [G loss: 0.549757] [AE loss: 1.107471]\n",
            "[Epoch 0/10] [Batch 1189/7121] [D loss: 0.794730] [G loss: 0.554363] [AE loss: 0.907467]\n",
            "[Epoch 0/10] [Batch 1190/7121] [D loss: 0.721803] [G loss: 0.577183] [AE loss: 2.699776]\n",
            "[Epoch 0/10] [Batch 1191/7121] [D loss: 0.776234] [G loss: 0.575291] [AE loss: 1.234843]\n",
            "[Epoch 0/10] [Batch 1192/7121] [D loss: 0.887265] [G loss: 0.569536] [AE loss: 0.979304]\n",
            "[Epoch 0/10] [Batch 1193/7121] [D loss: 0.696413] [G loss: 0.578934] [AE loss: 1.041363]\n",
            "[Epoch 0/10] [Batch 1194/7121] [D loss: 0.758759] [G loss: 0.590934] [AE loss: 1.008150]\n",
            "[Epoch 0/10] [Batch 1195/7121] [D loss: 0.694299] [G loss: 0.604561] [AE loss: 1.475418]\n",
            "[Epoch 0/10] [Batch 1196/7121] [D loss: 0.706744] [G loss: 0.611977] [AE loss: 3.579516]\n",
            "[Epoch 0/10] [Batch 1197/7121] [D loss: 0.726177] [G loss: 0.617609] [AE loss: 1.025921]\n",
            "[Epoch 0/10] [Batch 1198/7121] [D loss: 0.683148] [G loss: 0.634428] [AE loss: 2.223114]\n",
            "[Epoch 0/10] [Batch 1199/7121] [D loss: 0.747252] [G loss: 0.634180] [AE loss: 1.039299]\n",
            "[Epoch 0/10] [Batch 1200/7121] [D loss: 0.762851] [G loss: 0.633329] [AE loss: 1.069635]\n",
            "[Epoch 0/10] [Batch 1201/7121] [D loss: 0.780935] [G loss: 0.658579] [AE loss: 1.145065]\n",
            "[Epoch 0/10] [Batch 1202/7121] [D loss: 0.795606] [G loss: 0.653977] [AE loss: 0.930850]\n",
            "[Epoch 0/10] [Batch 1203/7121] [D loss: 0.754027] [G loss: 0.658983] [AE loss: 0.919783]\n",
            "[Epoch 0/10] [Batch 1204/7121] [D loss: 0.685783] [G loss: 0.664700] [AE loss: 2.003829]\n",
            "[Epoch 0/10] [Batch 1205/7121] [D loss: 0.691774] [G loss: 0.686149] [AE loss: 1.068600]\n",
            "[Epoch 0/10] [Batch 1206/7121] [D loss: 0.685855] [G loss: 0.688534] [AE loss: 1.336689]\n",
            "[Epoch 0/10] [Batch 1207/7121] [D loss: 0.700941] [G loss: 0.682091] [AE loss: 1.264653]\n",
            "[Epoch 0/10] [Batch 1208/7121] [D loss: 0.619807] [G loss: 0.703160] [AE loss: 1.487484]\n",
            "[Epoch 0/10] [Batch 1209/7121] [D loss: 0.712961] [G loss: 0.704040] [AE loss: 1.441123]\n",
            "[Epoch 0/10] [Batch 1210/7121] [D loss: 0.699854] [G loss: 0.705549] [AE loss: 0.867545]\n",
            "[Epoch 0/10] [Batch 1211/7121] [D loss: 0.657426] [G loss: 0.715184] [AE loss: 1.150171]\n",
            "[Epoch 0/10] [Batch 1212/7121] [D loss: 0.674236] [G loss: 0.713211] [AE loss: 1.565347]\n",
            "[Epoch 0/10] [Batch 1213/7121] [D loss: 0.655635] [G loss: 0.727449] [AE loss: 1.276822]\n",
            "[Epoch 0/10] [Batch 1214/7121] [D loss: 0.681976] [G loss: 0.749431] [AE loss: 1.038486]\n",
            "[Epoch 0/10] [Batch 1215/7121] [D loss: 0.689252] [G loss: 0.726109] [AE loss: 1.073013]\n",
            "[Epoch 0/10] [Batch 1216/7121] [D loss: 0.629432] [G loss: 0.721734] [AE loss: 1.011395]\n",
            "[Epoch 0/10] [Batch 1217/7121] [D loss: 0.553928] [G loss: 0.741083] [AE loss: 1.468353]\n",
            "[Epoch 0/10] [Batch 1218/7121] [D loss: 0.607257] [G loss: 0.755527] [AE loss: 0.845192]\n",
            "[Epoch 0/10] [Batch 1219/7121] [D loss: 0.668377] [G loss: 0.760405] [AE loss: 1.104773]\n",
            "[Epoch 0/10] [Batch 1220/7121] [D loss: 0.648360] [G loss: 0.754323] [AE loss: 1.143983]\n",
            "[Epoch 0/10] [Batch 1221/7121] [D loss: 0.556086] [G loss: 0.769500] [AE loss: 1.027457]\n",
            "[Epoch 0/10] [Batch 1222/7121] [D loss: 0.658350] [G loss: 0.763041] [AE loss: 1.004661]\n",
            "[Epoch 0/10] [Batch 1223/7121] [D loss: 0.607100] [G loss: 0.758735] [AE loss: 1.058543]\n",
            "[Epoch 0/10] [Batch 1224/7121] [D loss: 0.590149] [G loss: 0.777589] [AE loss: 1.452801]\n",
            "[Epoch 0/10] [Batch 1225/7121] [D loss: 0.625363] [G loss: 0.810908] [AE loss: 1.317479]\n",
            "[Epoch 0/10] [Batch 1226/7121] [D loss: 0.579436] [G loss: 0.813656] [AE loss: 1.538980]\n",
            "[Epoch 0/10] [Batch 1227/7121] [D loss: 0.621409] [G loss: 0.790289] [AE loss: 1.009923]\n",
            "[Epoch 0/10] [Batch 1228/7121] [D loss: 0.573534] [G loss: 0.801684] [AE loss: 1.005461]\n",
            "[Epoch 0/10] [Batch 1229/7121] [D loss: 0.633017] [G loss: 0.812625] [AE loss: 0.871210]\n",
            "[Epoch 0/10] [Batch 1230/7121] [D loss: 0.607463] [G loss: 0.810730] [AE loss: 0.930238]\n",
            "[Epoch 0/10] [Batch 1231/7121] [D loss: 0.596187] [G loss: 0.839980] [AE loss: 0.876125]\n",
            "[Epoch 0/10] [Batch 1232/7121] [D loss: 0.539880] [G loss: 0.826042] [AE loss: 1.068787]\n",
            "[Epoch 0/10] [Batch 1233/7121] [D loss: 0.571252] [G loss: 0.800536] [AE loss: 0.898684]\n",
            "[Epoch 0/10] [Batch 1234/7121] [D loss: 0.595024] [G loss: 0.805430] [AE loss: 0.933215]\n",
            "[Epoch 0/10] [Batch 1235/7121] [D loss: 0.565738] [G loss: 0.819760] [AE loss: 0.948523]\n",
            "[Epoch 0/10] [Batch 1236/7121] [D loss: 0.571606] [G loss: 0.795849] [AE loss: 1.585003]\n",
            "[Epoch 0/10] [Batch 1237/7121] [D loss: 0.579320] [G loss: 0.792511] [AE loss: 0.874140]\n",
            "[Epoch 0/10] [Batch 1238/7121] [D loss: 0.543511] [G loss: 0.754268] [AE loss: 1.061825]\n",
            "[Epoch 0/10] [Batch 1239/7121] [D loss: 0.559229] [G loss: 0.775599] [AE loss: 1.267218]\n",
            "[Epoch 0/10] [Batch 1240/7121] [D loss: 0.560375] [G loss: 0.751863] [AE loss: 1.175521]\n",
            "[Epoch 0/10] [Batch 1241/7121] [D loss: 0.534564] [G loss: 0.735396] [AE loss: 1.080603]\n",
            "[Epoch 0/10] [Batch 1242/7121] [D loss: 0.544635] [G loss: 0.723085] [AE loss: 1.456514]\n",
            "[Epoch 0/10] [Batch 1243/7121] [D loss: 0.558310] [G loss: 0.733354] [AE loss: 1.144443]\n",
            "[Epoch 0/10] [Batch 1244/7121] [D loss: 0.520432] [G loss: 0.732409] [AE loss: 1.368113]\n",
            "[Epoch 0/10] [Batch 1245/7121] [D loss: 0.608287] [G loss: 0.729427] [AE loss: 0.927769]\n",
            "[Epoch 0/10] [Batch 1246/7121] [D loss: 0.582135] [G loss: 0.695604] [AE loss: 0.866435]\n",
            "[Epoch 0/10] [Batch 1247/7121] [D loss: 0.572782] [G loss: 0.703035] [AE loss: 1.662091]\n",
            "[Epoch 0/10] [Batch 1248/7121] [D loss: 0.614205] [G loss: 0.694288] [AE loss: 0.937201]\n",
            "[Epoch 0/10] [Batch 1249/7121] [D loss: 0.654313] [G loss: 0.676257] [AE loss: 0.902749]\n",
            "[Epoch 0/10] [Batch 1250/7121] [D loss: 0.604201] [G loss: 0.672953] [AE loss: 0.970486]\n",
            "[Epoch 0/10] [Batch 1251/7121] [D loss: 0.585203] [G loss: 0.684836] [AE loss: 1.158246]\n",
            "[Epoch 0/10] [Batch 1252/7121] [D loss: 0.603022] [G loss: 0.689642] [AE loss: 1.429217]\n",
            "[Epoch 0/10] [Batch 1253/7121] [D loss: 0.598370] [G loss: 0.712001] [AE loss: 0.939043]\n",
            "[Epoch 0/10] [Batch 1254/7121] [D loss: 0.593728] [G loss: 0.682740] [AE loss: 0.954521]\n",
            "[Epoch 0/10] [Batch 1255/7121] [D loss: 0.607745] [G loss: 0.667681] [AE loss: 0.850953]\n",
            "[Epoch 0/10] [Batch 1256/7121] [D loss: 0.566985] [G loss: 0.674235] [AE loss: 1.178381]\n",
            "[Epoch 0/10] [Batch 1257/7121] [D loss: 0.586574] [G loss: 0.701601] [AE loss: 0.942548]\n",
            "[Epoch 0/10] [Batch 1258/7121] [D loss: 0.572582] [G loss: 0.735732] [AE loss: 1.070584]\n",
            "[Epoch 0/10] [Batch 1259/7121] [D loss: 0.568265] [G loss: 0.751029] [AE loss: 0.964132]\n",
            "[Epoch 0/10] [Batch 1260/7121] [D loss: 0.542852] [G loss: 0.779730] [AE loss: 1.177681]\n",
            "[Epoch 0/10] [Batch 1261/7121] [D loss: 0.595925] [G loss: 0.824048] [AE loss: 0.816933]\n",
            "[Epoch 0/10] [Batch 1262/7121] [D loss: 0.551510] [G loss: 0.818107] [AE loss: 0.844287]\n",
            "[Epoch 0/10] [Batch 1263/7121] [D loss: 0.527537] [G loss: 0.849253] [AE loss: 1.139742]\n",
            "[Epoch 0/10] [Batch 1264/7121] [D loss: 0.460532] [G loss: 0.896628] [AE loss: 1.583928]\n",
            "[Epoch 0/10] [Batch 1265/7121] [D loss: 0.464115] [G loss: 0.895488] [AE loss: 1.174469]\n",
            "[Epoch 0/10] [Batch 1266/7121] [D loss: 0.522798] [G loss: 0.905393] [AE loss: 1.199627]\n",
            "[Epoch 0/10] [Batch 1267/7121] [D loss: 0.463362] [G loss: 0.902027] [AE loss: 2.165039]\n",
            "[Epoch 0/10] [Batch 1268/7121] [D loss: 0.466053] [G loss: 0.925456] [AE loss: 1.065192]\n",
            "[Epoch 0/10] [Batch 1269/7121] [D loss: 0.439718] [G loss: 0.918645] [AE loss: 1.136905]\n",
            "[Epoch 0/10] [Batch 1270/7121] [D loss: 0.486813] [G loss: 0.913271] [AE loss: 1.632509]\n",
            "[Epoch 0/10] [Batch 1271/7121] [D loss: 0.482731] [G loss: 0.882890] [AE loss: 1.141252]\n",
            "[Epoch 0/10] [Batch 1272/7121] [D loss: 0.481129] [G loss: 0.886508] [AE loss: 1.207280]\n",
            "[Epoch 0/10] [Batch 1273/7121] [D loss: 0.479355] [G loss: 0.860473] [AE loss: 1.266970]\n",
            "[Epoch 0/10] [Batch 1274/7121] [D loss: 0.523817] [G loss: 0.829823] [AE loss: 1.031413]\n",
            "[Epoch 0/10] [Batch 1275/7121] [D loss: 0.523143] [G loss: 0.805026] [AE loss: 1.290261]\n",
            "[Epoch 0/10] [Batch 1276/7121] [D loss: 0.506498] [G loss: 0.769508] [AE loss: 1.297608]\n",
            "[Epoch 0/10] [Batch 1277/7121] [D loss: 0.528920] [G loss: 0.746053] [AE loss: 0.985515]\n",
            "[Epoch 0/10] [Batch 1278/7121] [D loss: 0.519343] [G loss: 0.724856] [AE loss: 2.151612]\n",
            "[Epoch 0/10] [Batch 1279/7121] [D loss: 0.604110] [G loss: 0.698164] [AE loss: 1.793985]\n",
            "[Epoch 0/10] [Batch 1280/7121] [D loss: 0.618688] [G loss: 0.672954] [AE loss: 0.796910]\n",
            "[Epoch 0/10] [Batch 1281/7121] [D loss: 0.618341] [G loss: 0.656856] [AE loss: 1.239755]\n",
            "[Epoch 0/10] [Batch 1282/7121] [D loss: 0.616065] [G loss: 0.633623] [AE loss: 1.013174]\n",
            "[Epoch 0/10] [Batch 1283/7121] [D loss: 0.576951] [G loss: 0.641352] [AE loss: 0.878101]\n",
            "[Epoch 0/10] [Batch 1284/7121] [D loss: 0.593927] [G loss: 0.642047] [AE loss: 0.855058]\n",
            "[Epoch 0/10] [Batch 1285/7121] [D loss: 0.607007] [G loss: 0.633546] [AE loss: 0.916436]\n",
            "[Epoch 0/10] [Batch 1286/7121] [D loss: 0.607494] [G loss: 0.635472] [AE loss: 1.394983]\n",
            "[Epoch 0/10] [Batch 1287/7121] [D loss: 0.582471] [G loss: 0.660247] [AE loss: 1.047057]\n",
            "[Epoch 0/10] [Batch 1288/7121] [D loss: 0.580134] [G loss: 0.662554] [AE loss: 1.135284]\n",
            "[Epoch 0/10] [Batch 1289/7121] [D loss: 0.558658] [G loss: 0.680193] [AE loss: 0.928856]\n",
            "[Epoch 0/10] [Batch 1290/7121] [D loss: 0.552961] [G loss: 0.681450] [AE loss: 1.154185]\n",
            "[Epoch 0/10] [Batch 1291/7121] [D loss: 0.538010] [G loss: 0.680950] [AE loss: 1.183495]\n",
            "[Epoch 0/10] [Batch 1292/7121] [D loss: 0.505950] [G loss: 0.707619] [AE loss: 0.962537]\n",
            "[Epoch 0/10] [Batch 1293/7121] [D loss: 0.531123] [G loss: 0.724210] [AE loss: 0.968518]\n",
            "[Epoch 0/10] [Batch 1294/7121] [D loss: 0.490347] [G loss: 0.719887] [AE loss: 3.234623]\n",
            "[Epoch 0/10] [Batch 1295/7121] [D loss: 0.486694] [G loss: 0.747027] [AE loss: 1.001319]\n",
            "[Epoch 0/10] [Batch 1296/7121] [D loss: 0.528095] [G loss: 0.737079] [AE loss: 1.338748]\n",
            "[Epoch 0/10] [Batch 1297/7121] [D loss: 0.471306] [G loss: 0.749237] [AE loss: 2.086570]\n",
            "[Epoch 0/10] [Batch 1298/7121] [D loss: 0.526675] [G loss: 0.762515] [AE loss: 1.025672]\n",
            "[Epoch 0/10] [Batch 1299/7121] [D loss: 0.494543] [G loss: 0.771665] [AE loss: 0.863981]\n",
            "[Epoch 0/10] [Batch 1300/7121] [D loss: 0.483215] [G loss: 0.759287] [AE loss: 1.151587]\n",
            "[Epoch 0/10] [Batch 1301/7121] [D loss: 0.478086] [G loss: 0.766702] [AE loss: 0.999248]\n",
            "[Epoch 0/10] [Batch 1302/7121] [D loss: 0.470702] [G loss: 0.749451] [AE loss: 0.958044]\n",
            "[Epoch 0/10] [Batch 1303/7121] [D loss: 0.466978] [G loss: 0.757788] [AE loss: 0.875316]\n",
            "[Epoch 0/10] [Batch 1304/7121] [D loss: 0.486402] [G loss: 0.719146] [AE loss: 0.871971]\n",
            "[Epoch 0/10] [Batch 1305/7121] [D loss: 0.460193] [G loss: 0.718717] [AE loss: 1.447825]\n",
            "[Epoch 0/10] [Batch 1306/7121] [D loss: 0.538961] [G loss: 0.709762] [AE loss: 1.083152]\n",
            "[Epoch 0/10] [Batch 1307/7121] [D loss: 0.548274] [G loss: 0.652579] [AE loss: 0.874934]\n",
            "[Epoch 0/10] [Batch 1308/7121] [D loss: 0.502224] [G loss: 0.704833] [AE loss: 1.089159]\n",
            "[Epoch 0/10] [Batch 1309/7121] [D loss: 0.539168] [G loss: 0.670730] [AE loss: 1.421201]\n",
            "[Epoch 0/10] [Batch 1310/7121] [D loss: 0.500704] [G loss: 0.691686] [AE loss: 3.848160]\n",
            "[Epoch 0/10] [Batch 1311/7121] [D loss: 0.551402] [G loss: 0.616234] [AE loss: 0.952797]\n",
            "[Epoch 0/10] [Batch 1312/7121] [D loss: 0.564577] [G loss: 0.592092] [AE loss: 0.954524]\n",
            "[Epoch 0/10] [Batch 1313/7121] [D loss: 0.471932] [G loss: 0.729813] [AE loss: 1.226284]\n",
            "[Epoch 0/10] [Batch 1314/7121] [D loss: 0.466644] [G loss: 0.833334] [AE loss: 1.532355]\n",
            "[Epoch 0/10] [Batch 1315/7121] [D loss: 0.542830] [G loss: 0.720860] [AE loss: 1.120938]\n",
            "[Epoch 0/10] [Batch 1316/7121] [D loss: 0.612220] [G loss: 0.550209] [AE loss: 1.138599]\n",
            "[Epoch 0/10] [Batch 1317/7121] [D loss: 0.738037] [G loss: 0.405839] [AE loss: 1.156049]\n",
            "[Epoch 0/10] [Batch 1318/7121] [D loss: 0.745819] [G loss: 0.436884] [AE loss: 1.235106]\n",
            "[Epoch 0/10] [Batch 1319/7121] [D loss: 0.697766] [G loss: 0.503309] [AE loss: 1.451155]\n",
            "[Epoch 0/10] [Batch 1320/7121] [D loss: 0.612916] [G loss: 0.609686] [AE loss: 1.897542]\n",
            "[Epoch 0/10] [Batch 1321/7121] [D loss: 0.589654] [G loss: 0.699823] [AE loss: 1.006703]\n",
            "[Epoch 0/10] [Batch 1322/7121] [D loss: 0.520491] [G loss: 0.803557] [AE loss: 1.491077]\n",
            "[Epoch 0/10] [Batch 1323/7121] [D loss: 0.558560] [G loss: 0.895365] [AE loss: 0.990994]\n",
            "[Epoch 0/10] [Batch 1324/7121] [D loss: 0.459002] [G loss: 0.985189] [AE loss: 1.145391]\n",
            "[Epoch 0/10] [Batch 1325/7121] [D loss: 0.445698] [G loss: 1.068561] [AE loss: 1.210529]\n",
            "[Epoch 0/10] [Batch 1326/7121] [D loss: 0.459235] [G loss: 1.132830] [AE loss: 1.313910]\n",
            "[Epoch 0/10] [Batch 1327/7121] [D loss: 0.484339] [G loss: 1.176802] [AE loss: 0.993812]\n",
            "[Epoch 0/10] [Batch 1328/7121] [D loss: 0.382189] [G loss: 1.208433] [AE loss: 1.737166]\n",
            "[Epoch 0/10] [Batch 1329/7121] [D loss: 0.483236] [G loss: 1.192939] [AE loss: 0.815701]\n",
            "[Epoch 0/10] [Batch 1330/7121] [D loss: 0.406297] [G loss: 1.211039] [AE loss: 1.014410]\n",
            "[Epoch 0/10] [Batch 1331/7121] [D loss: 0.439893] [G loss: 1.190528] [AE loss: 1.187224]\n",
            "[Epoch 0/10] [Batch 1332/7121] [D loss: 0.401883] [G loss: 1.168587] [AE loss: 1.618229]\n",
            "[Epoch 0/10] [Batch 1333/7121] [D loss: 0.453271] [G loss: 1.135106] [AE loss: 1.146547]\n",
            "[Epoch 0/10] [Batch 1334/7121] [D loss: 0.409225] [G loss: 1.085571] [AE loss: 1.233750]\n",
            "[Epoch 0/10] [Batch 1335/7121] [D loss: 0.469903] [G loss: 1.030312] [AE loss: 1.235499]\n",
            "[Epoch 0/10] [Batch 1336/7121] [D loss: 0.473516] [G loss: 0.976759] [AE loss: 1.098767]\n",
            "[Epoch 0/10] [Batch 1337/7121] [D loss: 0.496343] [G loss: 0.929588] [AE loss: 1.872820]\n",
            "[Epoch 0/10] [Batch 1338/7121] [D loss: 0.469909] [G loss: 0.890683] [AE loss: 1.123653]\n",
            "[Epoch 0/10] [Batch 1339/7121] [D loss: 0.507538] [G loss: 0.867395] [AE loss: 0.943966]\n",
            "[Epoch 0/10] [Batch 1340/7121] [D loss: 0.455803] [G loss: 0.836285] [AE loss: 1.032098]\n",
            "[Epoch 0/10] [Batch 1341/7121] [D loss: 0.469048] [G loss: 0.832915] [AE loss: 0.999416]\n",
            "[Epoch 0/10] [Batch 1342/7121] [D loss: 0.489174] [G loss: 0.783043] [AE loss: 1.156231]\n",
            "[Epoch 0/10] [Batch 1343/7121] [D loss: 0.518828] [G loss: 0.781858] [AE loss: 0.975101]\n",
            "[Epoch 0/10] [Batch 1344/7121] [D loss: 0.505126] [G loss: 0.763530] [AE loss: 1.546016]\n",
            "[Epoch 0/10] [Batch 1345/7121] [D loss: 0.562072] [G loss: 0.746621] [AE loss: 1.035569]\n",
            "[Epoch 0/10] [Batch 1346/7121] [D loss: 0.488360] [G loss: 0.717749] [AE loss: 2.105584]\n",
            "[Epoch 0/10] [Batch 1347/7121] [D loss: 0.535318] [G loss: 0.702674] [AE loss: 1.011301]\n",
            "[Epoch 0/10] [Batch 1348/7121] [D loss: 0.541394] [G loss: 0.696066] [AE loss: 1.201668]\n",
            "[Epoch 0/10] [Batch 1349/7121] [D loss: 0.544813] [G loss: 0.675367] [AE loss: 1.159322]\n",
            "[Epoch 0/10] [Batch 1350/7121] [D loss: 0.535024] [G loss: 0.668496] [AE loss: 0.939388]\n",
            "[Epoch 0/10] [Batch 1351/7121] [D loss: 0.537922] [G loss: 0.658205] [AE loss: 1.214820]\n",
            "[Epoch 0/10] [Batch 1352/7121] [D loss: 0.522463] [G loss: 0.643132] [AE loss: 2.690739]\n",
            "[Epoch 0/10] [Batch 1353/7121] [D loss: 0.484164] [G loss: 0.651351] [AE loss: 1.174477]\n",
            "[Epoch 0/10] [Batch 1354/7121] [D loss: 0.519746] [G loss: 0.646217] [AE loss: 1.011202]\n",
            "[Epoch 0/10] [Batch 1355/7121] [D loss: 0.559202] [G loss: 0.647087] [AE loss: 1.341862]\n",
            "[Epoch 0/10] [Batch 1356/7121] [D loss: 0.524220] [G loss: 0.651933] [AE loss: 1.027616]\n",
            "[Epoch 0/10] [Batch 1357/7121] [D loss: 0.507619] [G loss: 0.651412] [AE loss: 1.512629]\n",
            "[Epoch 0/10] [Batch 1358/7121] [D loss: 0.513985] [G loss: 0.662749] [AE loss: 0.995578]\n",
            "[Epoch 0/10] [Batch 1359/7121] [D loss: 0.521519] [G loss: 0.651838] [AE loss: 1.179892]\n",
            "[Epoch 0/10] [Batch 1360/7121] [D loss: 0.508330] [G loss: 0.670261] [AE loss: 1.071798]\n",
            "[Epoch 0/10] [Batch 1361/7121] [D loss: 0.504224] [G loss: 0.670872] [AE loss: 0.964712]\n",
            "[Epoch 0/10] [Batch 1362/7121] [D loss: 0.497337] [G loss: 0.686632] [AE loss: 0.981563]\n",
            "[Epoch 0/10] [Batch 1363/7121] [D loss: 0.498907] [G loss: 0.690269] [AE loss: 1.891281]\n",
            "[Epoch 0/10] [Batch 1364/7121] [D loss: 0.496785] [G loss: 0.709155] [AE loss: 1.022115]\n",
            "[Epoch 0/10] [Batch 1365/7121] [D loss: 0.458803] [G loss: 0.721153] [AE loss: 1.589413]\n",
            "[Epoch 0/10] [Batch 1366/7121] [D loss: 0.473642] [G loss: 0.743627] [AE loss: 0.933117]\n",
            "[Epoch 0/10] [Batch 1367/7121] [D loss: 0.463439] [G loss: 0.746679] [AE loss: 1.254539]\n",
            "[Epoch 0/10] [Batch 1368/7121] [D loss: 0.481788] [G loss: 0.751178] [AE loss: 1.023786]\n",
            "[Epoch 0/10] [Batch 1369/7121] [D loss: 0.456326] [G loss: 0.788090] [AE loss: 0.954937]\n",
            "[Epoch 0/10] [Batch 1370/7121] [D loss: 0.426725] [G loss: 0.801929] [AE loss: 1.039434]\n",
            "[Epoch 0/10] [Batch 1371/7121] [D loss: 0.413749] [G loss: 0.786907] [AE loss: 1.178649]\n",
            "[Epoch 0/10] [Batch 1372/7121] [D loss: 0.442879] [G loss: 0.809766] [AE loss: 0.934123]\n",
            "[Epoch 0/10] [Batch 1373/7121] [D loss: 0.475463] [G loss: 0.826327] [AE loss: 0.955477]\n",
            "[Epoch 0/10] [Batch 1374/7121] [D loss: 0.427739] [G loss: 0.839316] [AE loss: 0.982237]\n",
            "[Epoch 0/10] [Batch 1375/7121] [D loss: 0.436601] [G loss: 0.854961] [AE loss: 1.208339]\n",
            "[Epoch 0/10] [Batch 1376/7121] [D loss: 0.449118] [G loss: 0.828176] [AE loss: 4.591340]\n",
            "[Epoch 0/10] [Batch 1377/7121] [D loss: 0.377997] [G loss: 0.875974] [AE loss: 1.127185]\n",
            "[Epoch 0/10] [Batch 1378/7121] [D loss: 0.420201] [G loss: 0.887697] [AE loss: 1.186370]\n",
            "[Epoch 0/10] [Batch 1379/7121] [D loss: 0.409078] [G loss: 0.870822] [AE loss: 5.233842]\n",
            "[Epoch 0/10] [Batch 1380/7121] [D loss: 0.422045] [G loss: 0.896156] [AE loss: 1.070116]\n",
            "[Epoch 0/10] [Batch 1381/7121] [D loss: 0.398303] [G loss: 0.907297] [AE loss: 1.609707]\n",
            "[Epoch 0/10] [Batch 1382/7121] [D loss: 0.503464] [G loss: 0.922092] [AE loss: 0.723666]\n",
            "[Epoch 0/10] [Batch 1383/7121] [D loss: 0.434423] [G loss: 0.923827] [AE loss: 1.055563]\n",
            "[Epoch 0/10] [Batch 1384/7121] [D loss: 0.394269] [G loss: 0.997041] [AE loss: 1.010216]\n",
            "[Epoch 0/10] [Batch 1385/7121] [D loss: 0.370369] [G loss: 0.976831] [AE loss: 1.146906]\n",
            "[Epoch 0/10] [Batch 1386/7121] [D loss: 0.387508] [G loss: 1.001973] [AE loss: 1.009402]\n",
            "[Epoch 0/10] [Batch 1387/7121] [D loss: 0.399084] [G loss: 0.988472] [AE loss: 1.107204]\n",
            "[Epoch 0/10] [Batch 1388/7121] [D loss: 0.392973] [G loss: 1.010756] [AE loss: 2.381785]\n",
            "[Epoch 0/10] [Batch 1389/7121] [D loss: 0.384885] [G loss: 1.040193] [AE loss: 1.007682]\n",
            "[Epoch 0/10] [Batch 1390/7121] [D loss: 0.355605] [G loss: 1.023373] [AE loss: 1.282535]\n",
            "[Epoch 0/10] [Batch 1391/7121] [D loss: 0.418917] [G loss: 1.010673] [AE loss: 1.001644]\n",
            "[Epoch 0/10] [Batch 1392/7121] [D loss: 0.417888] [G loss: 1.006016] [AE loss: 1.065658]\n",
            "[Epoch 0/10] [Batch 1393/7121] [D loss: 0.398585] [G loss: 1.007930] [AE loss: 0.908439]\n",
            "[Epoch 0/10] [Batch 1394/7121] [D loss: 0.371487] [G loss: 1.016834] [AE loss: 1.211063]\n",
            "[Epoch 0/10] [Batch 1395/7121] [D loss: 0.393650] [G loss: 0.998946] [AE loss: 1.213870]\n",
            "[Epoch 0/10] [Batch 1396/7121] [D loss: 0.344815] [G loss: 1.001687] [AE loss: 1.185415]\n",
            "[Epoch 0/10] [Batch 1397/7121] [D loss: 0.392926] [G loss: 0.997748] [AE loss: 1.310369]\n",
            "[Epoch 0/10] [Batch 1398/7121] [D loss: 0.430438] [G loss: 0.960227] [AE loss: 0.920584]\n",
            "[Epoch 0/10] [Batch 1399/7121] [D loss: 0.478499] [G loss: 0.975523] [AE loss: 0.816430]\n",
            "[Epoch 0/10] [Batch 1400/7121] [D loss: 0.387959] [G loss: 0.875117] [AE loss: 2.155049]\n",
            "[Epoch 0/10] [Batch 1401/7121] [D loss: 0.465814] [G loss: 0.871521] [AE loss: 1.474451]\n",
            "[Epoch 0/10] [Batch 1402/7121] [D loss: 0.412107] [G loss: 0.823393] [AE loss: 0.979973]\n",
            "[Epoch 0/10] [Batch 1403/7121] [D loss: 0.526457] [G loss: 0.787184] [AE loss: 0.897163]\n",
            "[Epoch 0/10] [Batch 1404/7121] [D loss: 0.456614] [G loss: 0.781436] [AE loss: 3.085473]\n",
            "[Epoch 0/10] [Batch 1405/7121] [D loss: 0.464862] [G loss: 0.783308] [AE loss: 1.209986]\n",
            "[Epoch 0/10] [Batch 1406/7121] [D loss: 0.502980] [G loss: 0.721113] [AE loss: 1.033367]\n",
            "[Epoch 0/10] [Batch 1407/7121] [D loss: 0.510579] [G loss: 0.743284] [AE loss: 1.206513]\n",
            "[Epoch 0/10] [Batch 1408/7121] [D loss: 0.543179] [G loss: 0.745544] [AE loss: 0.947137]\n",
            "[Epoch 0/10] [Batch 1409/7121] [D loss: 0.507169] [G loss: 0.665793] [AE loss: 1.233078]\n",
            "[Epoch 0/10] [Batch 1410/7121] [D loss: 0.578029] [G loss: 0.609290] [AE loss: 0.824451]\n",
            "[Epoch 0/10] [Batch 1411/7121] [D loss: 0.516124] [G loss: 0.602887] [AE loss: 1.287402]\n",
            "[Epoch 0/10] [Batch 1412/7121] [D loss: 0.564651] [G loss: 0.579199] [AE loss: 1.158980]\n",
            "[Epoch 0/10] [Batch 1413/7121] [D loss: 0.578820] [G loss: 0.517258] [AE loss: 3.470197]\n",
            "[Epoch 0/10] [Batch 1414/7121] [D loss: 0.586348] [G loss: 0.516529] [AE loss: 1.121512]\n",
            "[Epoch 0/10] [Batch 1415/7121] [D loss: 0.662625] [G loss: 0.501947] [AE loss: 1.075857]\n",
            "[Epoch 0/10] [Batch 1416/7121] [D loss: 0.662824] [G loss: 0.550479] [AE loss: 1.032462]\n",
            "[Epoch 0/10] [Batch 1417/7121] [D loss: 0.656894] [G loss: 0.527641] [AE loss: 1.065637]\n",
            "[Epoch 0/10] [Batch 1418/7121] [D loss: 0.603261] [G loss: 0.567082] [AE loss: 1.382427]\n",
            "[Epoch 0/10] [Batch 1419/7121] [D loss: 0.662535] [G loss: 0.590624] [AE loss: 1.210058]\n",
            "[Epoch 0/10] [Batch 1420/7121] [D loss: 0.668065] [G loss: 0.599865] [AE loss: 1.073779]\n",
            "[Epoch 0/10] [Batch 1421/7121] [D loss: 0.648045] [G loss: 0.679229] [AE loss: 1.010930]\n",
            "[Epoch 0/10] [Batch 1422/7121] [D loss: 0.598741] [G loss: 0.681222] [AE loss: 1.076574]\n",
            "[Epoch 0/10] [Batch 1423/7121] [D loss: 0.571699] [G loss: 0.701265] [AE loss: 1.295258]\n",
            "[Epoch 0/10] [Batch 1424/7121] [D loss: 0.514201] [G loss: 0.756406] [AE loss: 0.966601]\n",
            "[Epoch 0/10] [Batch 1425/7121] [D loss: 0.569574] [G loss: 0.778451] [AE loss: 1.197177]\n",
            "[Epoch 0/10] [Batch 1426/7121] [D loss: 0.566784] [G loss: 0.791912] [AE loss: 1.248909]\n",
            "[Epoch 0/10] [Batch 1427/7121] [D loss: 0.511511] [G loss: 0.789686] [AE loss: 0.894499]\n",
            "[Epoch 0/10] [Batch 1428/7121] [D loss: 0.563148] [G loss: 0.797547] [AE loss: 2.737460]\n",
            "[Epoch 0/10] [Batch 1429/7121] [D loss: 0.546119] [G loss: 0.806663] [AE loss: 1.159825]\n",
            "[Epoch 0/10] [Batch 1430/7121] [D loss: 0.587223] [G loss: 0.774359] [AE loss: 1.002182]\n",
            "[Epoch 0/10] [Batch 1431/7121] [D loss: 0.606511] [G loss: 0.697960] [AE loss: 1.124275]\n",
            "[Epoch 0/10] [Batch 1432/7121] [D loss: 0.644163] [G loss: 0.654322] [AE loss: 0.937437]\n",
            "[Epoch 0/10] [Batch 1433/7121] [D loss: 0.678073] [G loss: 0.580634] [AE loss: 0.977280]\n",
            "[Epoch 0/10] [Batch 1434/7121] [D loss: 0.691169] [G loss: 0.513938] [AE loss: 1.366553]\n",
            "[Epoch 0/10] [Batch 1435/7121] [D loss: 0.778947] [G loss: 0.466010] [AE loss: 1.138288]\n",
            "[Epoch 0/10] [Batch 1436/7121] [D loss: 0.758952] [G loss: 0.445187] [AE loss: 1.290042]\n",
            "[Epoch 0/10] [Batch 1437/7121] [D loss: 0.881914] [G loss: 0.403883] [AE loss: 0.922767]\n",
            "[Epoch 0/10] [Batch 1438/7121] [D loss: 0.899894] [G loss: 0.440781] [AE loss: 1.128000]\n",
            "[Epoch 0/10] [Batch 1439/7121] [D loss: 0.860528] [G loss: 0.425984] [AE loss: 1.118824]\n",
            "[Epoch 0/10] [Batch 1440/7121] [D loss: 0.837947] [G loss: 0.439061] [AE loss: 1.266029]\n",
            "[Epoch 0/10] [Batch 1441/7121] [D loss: 0.763751] [G loss: 0.480483] [AE loss: 1.258385]\n",
            "[Epoch 0/10] [Batch 1442/7121] [D loss: 0.827733] [G loss: 0.497848] [AE loss: 1.378811]\n",
            "[Epoch 0/10] [Batch 1443/7121] [D loss: 0.705106] [G loss: 0.569086] [AE loss: 1.148648]\n",
            "[Epoch 0/10] [Batch 1444/7121] [D loss: 0.761011] [G loss: 0.626989] [AE loss: 1.074136]\n",
            "[Epoch 0/10] [Batch 1445/7121] [D loss: 0.621378] [G loss: 0.746855] [AE loss: 1.092916]\n",
            "[Epoch 0/10] [Batch 1446/7121] [D loss: 0.529977] [G loss: 0.808003] [AE loss: 1.598324]\n",
            "[Epoch 0/10] [Batch 1447/7121] [D loss: 0.671366] [G loss: 0.863904] [AE loss: 1.310328]\n",
            "[Epoch 0/10] [Batch 1448/7121] [D loss: 0.617755] [G loss: 0.909713] [AE loss: 1.264348]\n",
            "[Epoch 0/10] [Batch 1449/7121] [D loss: 0.676642] [G loss: 0.932811] [AE loss: 1.077897]\n",
            "[Epoch 0/10] [Batch 1450/7121] [D loss: 0.655267] [G loss: 1.012076] [AE loss: 1.045174]\n",
            "[Epoch 0/10] [Batch 1451/7121] [D loss: 0.672422] [G loss: 1.042794] [AE loss: 1.116118]\n",
            "[Epoch 0/10] [Batch 1452/7121] [D loss: 0.651995] [G loss: 1.060842] [AE loss: 1.110136]\n",
            "[Epoch 0/10] [Batch 1453/7121] [D loss: 0.558606] [G loss: 1.061115] [AE loss: 1.460130]\n",
            "[Epoch 0/10] [Batch 1454/7121] [D loss: 0.554996] [G loss: 1.030199] [AE loss: 1.589281]\n",
            "[Epoch 0/10] [Batch 1455/7121] [D loss: 0.653886] [G loss: 1.012809] [AE loss: 1.078047]\n",
            "[Epoch 0/10] [Batch 1456/7121] [D loss: 0.613148] [G loss: 1.008461] [AE loss: 1.147345]\n",
            "[Epoch 0/10] [Batch 1457/7121] [D loss: 0.634091] [G loss: 0.934974] [AE loss: 0.993893]\n",
            "[Epoch 0/10] [Batch 1458/7121] [D loss: 0.678367] [G loss: 0.955956] [AE loss: 1.324673]\n",
            "[Epoch 0/10] [Batch 1459/7121] [D loss: 0.596106] [G loss: 0.928705] [AE loss: 1.184894]\n",
            "[Epoch 0/10] [Batch 1460/7121] [D loss: 0.640449] [G loss: 0.927624] [AE loss: 1.032413]\n",
            "[Epoch 0/10] [Batch 1461/7121] [D loss: 0.599223] [G loss: 0.914745] [AE loss: 1.087956]\n",
            "[Epoch 0/10] [Batch 1462/7121] [D loss: 0.568994] [G loss: 0.940950] [AE loss: 1.256665]\n",
            "[Epoch 0/10] [Batch 1463/7121] [D loss: 0.635540] [G loss: 0.910557] [AE loss: 1.044988]\n",
            "[Epoch 0/10] [Batch 1464/7121] [D loss: 0.592771] [G loss: 0.913480] [AE loss: 1.432066]\n",
            "[Epoch 0/10] [Batch 1465/7121] [D loss: 0.608244] [G loss: 0.902910] [AE loss: 1.129856]\n",
            "[Epoch 0/10] [Batch 1466/7121] [D loss: 0.551413] [G loss: 0.906597] [AE loss: 1.037064]\n",
            "[Epoch 0/10] [Batch 1467/7121] [D loss: 0.579186] [G loss: 0.878911] [AE loss: 0.942107]\n",
            "[Epoch 0/10] [Batch 1468/7121] [D loss: 0.579915] [G loss: 0.880288] [AE loss: 1.229543]\n",
            "[Epoch 0/10] [Batch 1469/7121] [D loss: 0.707098] [G loss: 0.853467] [AE loss: 1.003486]\n",
            "[Epoch 0/10] [Batch 1470/7121] [D loss: 0.560179] [G loss: 0.833098] [AE loss: 1.041172]\n",
            "[Epoch 0/10] [Batch 1471/7121] [D loss: 0.630589] [G loss: 0.822516] [AE loss: 1.295307]\n",
            "[Epoch 0/10] [Batch 1472/7121] [D loss: 0.527890] [G loss: 0.803754] [AE loss: 1.854400]\n",
            "[Epoch 0/10] [Batch 1473/7121] [D loss: 0.575155] [G loss: 0.783986] [AE loss: 1.128378]\n",
            "[Epoch 0/10] [Batch 1474/7121] [D loss: 0.555112] [G loss: 0.752774] [AE loss: 1.214013]\n",
            "[Epoch 0/10] [Batch 1475/7121] [D loss: 0.659562] [G loss: 0.726214] [AE loss: 1.186497]\n",
            "[Epoch 0/10] [Batch 1476/7121] [D loss: 0.579780] [G loss: 0.721012] [AE loss: 1.067891]\n",
            "[Epoch 0/10] [Batch 1477/7121] [D loss: 0.633013] [G loss: 0.700192] [AE loss: 0.982143]\n",
            "[Epoch 0/10] [Batch 1478/7121] [D loss: 0.637575] [G loss: 0.698602] [AE loss: 0.889344]\n",
            "[Epoch 0/10] [Batch 1479/7121] [D loss: 0.665898] [G loss: 0.680276] [AE loss: 0.936018]\n",
            "[Epoch 0/10] [Batch 1480/7121] [D loss: 0.632148] [G loss: 0.664976] [AE loss: 1.222271]\n",
            "[Epoch 0/10] [Batch 1481/7121] [D loss: 0.594650] [G loss: 0.655308] [AE loss: 1.238881]\n",
            "[Epoch 0/10] [Batch 1482/7121] [D loss: 0.585667] [G loss: 0.674386] [AE loss: 1.296000]\n",
            "[Epoch 0/10] [Batch 1483/7121] [D loss: 0.576807] [G loss: 0.677353] [AE loss: 1.058626]\n",
            "[Epoch 0/10] [Batch 1484/7121] [D loss: 0.564649] [G loss: 0.702283] [AE loss: 1.584072]\n",
            "[Epoch 0/10] [Batch 1485/7121] [D loss: 0.550013] [G loss: 0.740323] [AE loss: 0.978018]\n",
            "[Epoch 0/10] [Batch 1486/7121] [D loss: 0.556629] [G loss: 0.729859] [AE loss: 0.919587]\n",
            "[Epoch 0/10] [Batch 1487/7121] [D loss: 0.563687] [G loss: 0.782018] [AE loss: 1.242878]\n",
            "[Epoch 0/10] [Batch 1488/7121] [D loss: 0.507663] [G loss: 0.833593] [AE loss: 1.559944]\n",
            "[Epoch 0/10] [Batch 1489/7121] [D loss: 0.531448] [G loss: 0.852812] [AE loss: 0.955206]\n",
            "[Epoch 0/10] [Batch 1490/7121] [D loss: 0.467865] [G loss: 0.915554] [AE loss: 1.257061]\n",
            "[Epoch 0/10] [Batch 1491/7121] [D loss: 0.421571] [G loss: 0.977132] [AE loss: 1.068339]\n",
            "[Epoch 0/10] [Batch 1492/7121] [D loss: 0.458540] [G loss: 1.012605] [AE loss: 0.932050]\n",
            "[Epoch 0/10] [Batch 1493/7121] [D loss: 0.455340] [G loss: 1.066263] [AE loss: 1.615223]\n",
            "[Epoch 0/10] [Batch 1494/7121] [D loss: 0.397340] [G loss: 1.099930] [AE loss: 0.964893]\n",
            "[Epoch 0/10] [Batch 1495/7121] [D loss: 0.410811] [G loss: 1.119272] [AE loss: 0.942513]\n",
            "[Epoch 0/10] [Batch 1496/7121] [D loss: 0.379670] [G loss: 1.138834] [AE loss: 1.111142]\n",
            "[Epoch 0/10] [Batch 1497/7121] [D loss: 0.388523] [G loss: 1.175046] [AE loss: 1.081162]\n",
            "[Epoch 0/10] [Batch 1498/7121] [D loss: 0.391789] [G loss: 1.177309] [AE loss: 1.012675]\n",
            "[Epoch 0/10] [Batch 1499/7121] [D loss: 0.363575] [G loss: 1.198580] [AE loss: 1.170868]\n",
            "[Epoch 0/10] [Batch 1500/7121] [D loss: 0.368773] [G loss: 1.216781] [AE loss: 1.472054]\n",
            "[Epoch 0/10] [Batch 1501/7121] [D loss: 0.404872] [G loss: 1.194044] [AE loss: 1.059796]\n",
            "[Epoch 0/10] [Batch 1502/7121] [D loss: 0.342435] [G loss: 1.184599] [AE loss: 1.799384]\n",
            "[Epoch 0/10] [Batch 1503/7121] [D loss: 0.420983] [G loss: 1.164190] [AE loss: 0.985835]\n",
            "[Epoch 0/10] [Batch 1504/7121] [D loss: 0.362805] [G loss: 1.152221] [AE loss: 1.305992]\n",
            "[Epoch 0/10] [Batch 1505/7121] [D loss: 0.325745] [G loss: 1.083416] [AE loss: 2.084842]\n",
            "[Epoch 0/10] [Batch 1506/7121] [D loss: 0.378915] [G loss: 1.073724] [AE loss: 1.244319]\n",
            "[Epoch 0/10] [Batch 1507/7121] [D loss: 0.379400] [G loss: 1.026718] [AE loss: 0.945494]\n",
            "[Epoch 0/10] [Batch 1508/7121] [D loss: 0.360540] [G loss: 0.989751] [AE loss: 2.262051]\n",
            "[Epoch 0/10] [Batch 1509/7121] [D loss: 0.398026] [G loss: 0.910997] [AE loss: 1.883796]\n",
            "[Epoch 0/10] [Batch 1510/7121] [D loss: 0.431414] [G loss: 0.868694] [AE loss: 1.108487]\n",
            "[Epoch 0/10] [Batch 1511/7121] [D loss: 0.486691] [G loss: 0.806727] [AE loss: 1.177285]\n",
            "[Epoch 0/10] [Batch 1512/7121] [D loss: 0.454113] [G loss: 0.757889] [AE loss: 1.124348]\n",
            "[Epoch 0/10] [Batch 1513/7121] [D loss: 0.479066] [G loss: 0.747435] [AE loss: 0.986282]\n",
            "[Epoch 0/10] [Batch 1514/7121] [D loss: 0.524916] [G loss: 0.709061] [AE loss: 0.942323]\n",
            "[Epoch 0/10] [Batch 1515/7121] [D loss: 0.492685] [G loss: 0.744799] [AE loss: 1.423164]\n",
            "[Epoch 0/10] [Batch 1516/7121] [D loss: 0.529951] [G loss: 0.693544] [AE loss: 0.988229]\n",
            "[Epoch 0/10] [Batch 1517/7121] [D loss: 0.556522] [G loss: 0.670804] [AE loss: 1.561472]\n",
            "[Epoch 0/10] [Batch 1518/7121] [D loss: 0.544425] [G loss: 0.669328] [AE loss: 1.174337]\n",
            "[Epoch 0/10] [Batch 1519/7121] [D loss: 0.569288] [G loss: 0.639211] [AE loss: 1.185503]\n",
            "[Epoch 0/10] [Batch 1520/7121] [D loss: 0.598989] [G loss: 0.681530] [AE loss: 0.769293]\n",
            "[Epoch 0/10] [Batch 1521/7121] [D loss: 0.550176] [G loss: 0.669612] [AE loss: 1.441764]\n",
            "[Epoch 0/10] [Batch 1522/7121] [D loss: 0.583992] [G loss: 0.659108] [AE loss: 1.720438]\n",
            "[Epoch 0/10] [Batch 1523/7121] [D loss: 0.553823] [G loss: 0.644165] [AE loss: 1.081471]\n",
            "[Epoch 0/10] [Batch 1524/7121] [D loss: 0.556892] [G loss: 0.628167] [AE loss: 0.923237]\n",
            "[Epoch 0/10] [Batch 1525/7121] [D loss: 0.623877] [G loss: 0.612124] [AE loss: 1.122184]\n",
            "[Epoch 0/10] [Batch 1526/7121] [D loss: 0.598596] [G loss: 0.620955] [AE loss: 3.901046]\n",
            "[Epoch 0/10] [Batch 1527/7121] [D loss: 0.547974] [G loss: 0.571872] [AE loss: 1.102887]\n",
            "[Epoch 0/10] [Batch 1528/7121] [D loss: 0.582998] [G loss: 0.571795] [AE loss: 0.972091]\n",
            "[Epoch 0/10] [Batch 1529/7121] [D loss: 0.609468] [G loss: 0.543028] [AE loss: 1.294224]\n",
            "[Epoch 0/10] [Batch 1530/7121] [D loss: 0.584612] [G loss: 0.565332] [AE loss: 1.340688]\n",
            "[Epoch 0/10] [Batch 1531/7121] [D loss: 0.587274] [G loss: 0.537541] [AE loss: 0.955030]\n",
            "[Epoch 0/10] [Batch 1532/7121] [D loss: 0.594152] [G loss: 0.568621] [AE loss: 0.964183]\n",
            "[Epoch 0/10] [Batch 1533/7121] [D loss: 0.697832] [G loss: 0.510345] [AE loss: 1.028227]\n",
            "[Epoch 0/10] [Batch 1534/7121] [D loss: 0.678033] [G loss: 0.535653] [AE loss: 0.946584]\n",
            "[Epoch 0/10] [Batch 1535/7121] [D loss: 0.620610] [G loss: 0.526659] [AE loss: 1.337936]\n",
            "[Epoch 0/10] [Batch 1536/7121] [D loss: 0.791529] [G loss: 0.594219] [AE loss: 0.879049]\n",
            "[Epoch 0/10] [Batch 1537/7121] [D loss: 0.693903] [G loss: 0.586234] [AE loss: 1.073430]\n",
            "[Epoch 0/10] [Batch 1538/7121] [D loss: 0.721819] [G loss: 0.620123] [AE loss: 1.017636]\n",
            "[Epoch 0/10] [Batch 1539/7121] [D loss: 0.640632] [G loss: 0.646295] [AE loss: 1.786680]\n",
            "[Epoch 0/10] [Batch 1540/7121] [D loss: 0.694656] [G loss: 0.666635] [AE loss: 1.205246]\n",
            "[Epoch 0/10] [Batch 1541/7121] [D loss: 0.645408] [G loss: 0.719209] [AE loss: 1.067927]\n",
            "[Epoch 0/10] [Batch 1542/7121] [D loss: 0.631112] [G loss: 0.726463] [AE loss: 1.144300]\n",
            "[Epoch 0/10] [Batch 1543/7121] [D loss: 0.642842] [G loss: 0.724144] [AE loss: 1.377497]\n",
            "[Epoch 0/10] [Batch 1544/7121] [D loss: 0.729354] [G loss: 0.802384] [AE loss: 0.935941]\n",
            "[Epoch 0/10] [Batch 1545/7121] [D loss: 0.600879] [G loss: 0.832912] [AE loss: 1.335400]\n",
            "[Epoch 0/10] [Batch 1546/7121] [D loss: 0.599220] [G loss: 0.863420] [AE loss: 1.351502]\n",
            "[Epoch 0/10] [Batch 1547/7121] [D loss: 0.645737] [G loss: 0.920991] [AE loss: 1.072513]\n",
            "[Epoch 0/10] [Batch 1548/7121] [D loss: 0.605865] [G loss: 0.998184] [AE loss: 1.092048]\n",
            "[Epoch 0/10] [Batch 1549/7121] [D loss: 0.495856] [G loss: 1.020366] [AE loss: 1.347330]\n",
            "[Epoch 0/10] [Batch 1550/7121] [D loss: 0.512329] [G loss: 1.071697] [AE loss: 1.099378]\n",
            "[Epoch 0/10] [Batch 1551/7121] [D loss: 0.517529] [G loss: 1.063785] [AE loss: 0.912924]\n",
            "[Epoch 0/10] [Batch 1552/7121] [D loss: 0.568852] [G loss: 1.036878] [AE loss: 1.285499]\n",
            "[Epoch 0/10] [Batch 1553/7121] [D loss: 0.567521] [G loss: 0.970699] [AE loss: 1.256702]\n",
            "[Epoch 0/10] [Batch 1554/7121] [D loss: 0.661853] [G loss: 0.905196] [AE loss: 1.248248]\n",
            "[Epoch 0/10] [Batch 1555/7121] [D loss: 0.637057] [G loss: 0.815738] [AE loss: 1.287264]\n",
            "[Epoch 0/10] [Batch 1556/7121] [D loss: 0.659943] [G loss: 0.732298] [AE loss: 1.121488]\n",
            "[Epoch 0/10] [Batch 1557/7121] [D loss: 0.773723] [G loss: 0.613865] [AE loss: 1.412010]\n",
            "[Epoch 0/10] [Batch 1558/7121] [D loss: 0.694083] [G loss: 0.549270] [AE loss: 1.474693]\n",
            "[Epoch 0/10] [Batch 1559/7121] [D loss: 0.800213] [G loss: 0.479703] [AE loss: 0.832098]\n",
            "[Epoch 0/10] [Batch 1560/7121] [D loss: 0.903901] [G loss: 0.434553] [AE loss: 1.234778]\n",
            "[Epoch 0/10] [Batch 1561/7121] [D loss: 0.885327] [G loss: 0.403267] [AE loss: 1.124068]\n",
            "[Epoch 0/10] [Batch 1562/7121] [D loss: 0.893887] [G loss: 0.397366] [AE loss: 1.168400]\n",
            "[Epoch 0/10] [Batch 1563/7121] [D loss: 0.942541] [G loss: 0.356091] [AE loss: 1.016500]\n",
            "[Epoch 0/10] [Batch 1564/7121] [D loss: 0.926356] [G loss: 0.402966] [AE loss: 1.034879]\n",
            "[Epoch 0/10] [Batch 1565/7121] [D loss: 0.823102] [G loss: 0.404915] [AE loss: 1.060872]\n",
            "[Epoch 0/10] [Batch 1566/7121] [D loss: 0.968570] [G loss: 0.381624] [AE loss: 0.876798]\n",
            "[Epoch 0/10] [Batch 1567/7121] [D loss: 0.948290] [G loss: 0.412284] [AE loss: 1.033290]\n",
            "[Epoch 0/10] [Batch 1568/7121] [D loss: 0.874048] [G loss: 0.480264] [AE loss: 1.286310]\n",
            "[Epoch 0/10] [Batch 1569/7121] [D loss: 0.774471] [G loss: 0.552063] [AE loss: 1.438736]\n",
            "[Epoch 0/10] [Batch 1570/7121] [D loss: 0.735630] [G loss: 0.546771] [AE loss: 1.266285]\n",
            "[Epoch 0/10] [Batch 1571/7121] [D loss: 0.759826] [G loss: 0.602163] [AE loss: 1.036807]\n",
            "[Epoch 0/10] [Batch 1572/7121] [D loss: 0.575430] [G loss: 0.685698] [AE loss: 1.129699]\n",
            "[Epoch 0/10] [Batch 1573/7121] [D loss: 0.629568] [G loss: 0.713185] [AE loss: 2.790866]\n",
            "[Epoch 0/10] [Batch 1574/7121] [D loss: 0.706410] [G loss: 0.758832] [AE loss: 1.057071]\n",
            "[Epoch 0/10] [Batch 1575/7121] [D loss: 0.635369] [G loss: 0.817908] [AE loss: 0.926855]\n",
            "[Epoch 0/10] [Batch 1576/7121] [D loss: 0.574718] [G loss: 0.821454] [AE loss: 1.124383]\n",
            "[Epoch 0/10] [Batch 1577/7121] [D loss: 0.594434] [G loss: 0.845219] [AE loss: 1.148596]\n",
            "[Epoch 0/10] [Batch 1578/7121] [D loss: 0.603147] [G loss: 0.890213] [AE loss: 1.145252]\n",
            "[Epoch 0/10] [Batch 1579/7121] [D loss: 0.610270] [G loss: 0.873464] [AE loss: 0.904809]\n",
            "[Epoch 0/10] [Batch 1580/7121] [D loss: 0.614603] [G loss: 0.829466] [AE loss: 1.887661]\n",
            "[Epoch 0/10] [Batch 1581/7121] [D loss: 0.628814] [G loss: 0.823952] [AE loss: 1.444889]\n",
            "[Epoch 0/10] [Batch 1582/7121] [D loss: 0.713182] [G loss: 0.805754] [AE loss: 0.872937]\n",
            "[Epoch 0/10] [Batch 1583/7121] [D loss: 0.574503] [G loss: 0.792257] [AE loss: 1.062528]\n",
            "[Epoch 0/10] [Batch 1584/7121] [D loss: 0.642489] [G loss: 0.771872] [AE loss: 0.983834]\n",
            "[Epoch 0/10] [Batch 1585/7121] [D loss: 0.605322] [G loss: 0.785728] [AE loss: 0.883225]\n",
            "[Epoch 0/10] [Batch 1586/7121] [D loss: 0.608809] [G loss: 0.707697] [AE loss: 1.571493]\n",
            "[Epoch 0/10] [Batch 1587/7121] [D loss: 0.624545] [G loss: 0.695023] [AE loss: 1.305229]\n",
            "[Epoch 0/10] [Batch 1588/7121] [D loss: 0.547132] [G loss: 0.710103] [AE loss: 1.066833]\n",
            "[Epoch 0/10] [Batch 1589/7121] [D loss: 0.650780] [G loss: 0.688893] [AE loss: 1.539301]\n",
            "[Epoch 0/10] [Batch 1590/7121] [D loss: 0.641858] [G loss: 0.694735] [AE loss: 1.139587]\n",
            "[Epoch 0/10] [Batch 1591/7121] [D loss: 0.647079] [G loss: 0.641924] [AE loss: 1.200919]\n",
            "[Epoch 0/10] [Batch 1592/7121] [D loss: 0.607610] [G loss: 0.706760] [AE loss: 1.084351]\n",
            "[Epoch 0/10] [Batch 1593/7121] [D loss: 0.583965] [G loss: 0.677171] [AE loss: 1.344561]\n",
            "[Epoch 0/10] [Batch 1594/7121] [D loss: 0.607617] [G loss: 0.695986] [AE loss: 2.112597]\n",
            "[Epoch 0/10] [Batch 1595/7121] [D loss: 0.695719] [G loss: 0.692187] [AE loss: 0.938235]\n",
            "[Epoch 0/10] [Batch 1596/7121] [D loss: 0.618805] [G loss: 0.716773] [AE loss: 1.430844]\n",
            "[Epoch 0/10] [Batch 1597/7121] [D loss: 0.619499] [G loss: 0.740181] [AE loss: 0.958122]\n",
            "[Epoch 0/10] [Batch 1598/7121] [D loss: 0.594995] [G loss: 0.751101] [AE loss: 1.303056]\n",
            "[Epoch 0/10] [Batch 1599/7121] [D loss: 0.585324] [G loss: 0.778005] [AE loss: 1.073893]\n",
            "[Epoch 0/10] [Batch 1600/7121] [D loss: 0.575148] [G loss: 0.818201] [AE loss: 1.098904]\n",
            "[Epoch 0/10] [Batch 1601/7121] [D loss: 0.578863] [G loss: 0.846587] [AE loss: 1.174551]\n",
            "[Epoch 0/10] [Batch 1602/7121] [D loss: 0.554752] [G loss: 0.894975] [AE loss: 1.673069]\n",
            "[Epoch 0/10] [Batch 1603/7121] [D loss: 0.602958] [G loss: 0.805137] [AE loss: 1.016191]\n",
            "[Epoch 0/10] [Batch 1604/7121] [D loss: 0.515626] [G loss: 0.889873] [AE loss: 1.325988]\n",
            "[Epoch 0/10] [Batch 1605/7121] [D loss: 0.504700] [G loss: 0.895333] [AE loss: 2.295614]\n",
            "[Epoch 0/10] [Batch 1606/7121] [D loss: 0.682704] [G loss: 0.758971] [AE loss: 1.176693]\n",
            "[Epoch 0/10] [Batch 1607/7121] [D loss: 0.652048] [G loss: 0.786206] [AE loss: 1.066845]\n",
            "[Epoch 0/10] [Batch 1608/7121] [D loss: 0.633016] [G loss: 0.717081] [AE loss: 1.852127]\n",
            "[Epoch 0/10] [Batch 1609/7121] [D loss: 0.504629] [G loss: 0.987301] [AE loss: 1.049242]\n",
            "[Epoch 0/10] [Batch 1610/7121] [D loss: 0.505008] [G loss: 1.129650] [AE loss: 1.091624]\n",
            "[Epoch 0/10] [Batch 1611/7121] [D loss: 0.438174] [G loss: 1.230370] [AE loss: 1.366781]\n",
            "[Epoch 0/10] [Batch 1612/7121] [D loss: 0.711036] [G loss: 0.646330] [AE loss: 1.655657]\n",
            "[Epoch 0/10] [Batch 1613/7121] [D loss: 0.626543] [G loss: 0.692348] [AE loss: 1.057547]\n",
            "[Epoch 0/10] [Batch 1614/7121] [D loss: 0.650766] [G loss: 0.565092] [AE loss: 1.432941]\n",
            "[Epoch 0/10] [Batch 1615/7121] [D loss: 0.703964] [G loss: 0.591931] [AE loss: 1.116300]\n",
            "[Epoch 0/10] [Batch 1616/7121] [D loss: 0.593266] [G loss: 0.749699] [AE loss: 1.218728]\n",
            "[Epoch 0/10] [Batch 1617/7121] [D loss: 0.560639] [G loss: 0.712199] [AE loss: 1.503163]\n",
            "[Epoch 0/10] [Batch 1618/7121] [D loss: 0.631172] [G loss: 0.634031] [AE loss: 1.235234]\n",
            "[Epoch 0/10] [Batch 1619/7121] [D loss: 0.674123] [G loss: 0.664467] [AE loss: 1.253994]\n",
            "[Epoch 0/10] [Batch 1620/7121] [D loss: 0.629417] [G loss: 0.678855] [AE loss: 1.273596]\n",
            "[Epoch 0/10] [Batch 1621/7121] [D loss: 0.660500] [G loss: 0.686290] [AE loss: 1.452439]\n",
            "[Epoch 0/10] [Batch 1622/7121] [D loss: 0.663421] [G loss: 0.740904] [AE loss: 0.948226]\n",
            "[Epoch 0/10] [Batch 1623/7121] [D loss: 0.651898] [G loss: 0.747818] [AE loss: 0.824790]\n",
            "[Epoch 0/10] [Batch 1624/7121] [D loss: 0.669389] [G loss: 0.788346] [AE loss: 0.948232]\n",
            "[Epoch 0/10] [Batch 1625/7121] [D loss: 0.642513] [G loss: 0.793291] [AE loss: 6.480242]\n",
            "[Epoch 0/10] [Batch 1626/7121] [D loss: 0.602190] [G loss: 0.823481] [AE loss: 1.120432]\n",
            "[Epoch 0/10] [Batch 1627/7121] [D loss: 0.502435] [G loss: 0.817091] [AE loss: 1.231308]\n",
            "[Epoch 0/10] [Batch 1628/7121] [D loss: 0.570096] [G loss: 0.831510] [AE loss: 1.339549]\n",
            "[Epoch 0/10] [Batch 1629/7121] [D loss: 0.527620] [G loss: 0.853823] [AE loss: 1.533066]\n",
            "[Epoch 0/10] [Batch 1630/7121] [D loss: 0.576872] [G loss: 0.872342] [AE loss: 0.834375]\n",
            "[Epoch 0/10] [Batch 1631/7121] [D loss: 0.621074] [G loss: 0.863743] [AE loss: 1.659345]\n",
            "[Epoch 0/10] [Batch 1632/7121] [D loss: 0.579819] [G loss: 0.865842] [AE loss: 1.123556]\n",
            "[Epoch 0/10] [Batch 1633/7121] [D loss: 0.597375] [G loss: 0.869999] [AE loss: 1.264654]\n",
            "[Epoch 0/10] [Batch 1634/7121] [D loss: 0.494866] [G loss: 0.878171] [AE loss: 2.337681]\n",
            "[Epoch 0/10] [Batch 1635/7121] [D loss: 0.573037] [G loss: 0.871875] [AE loss: 0.923919]\n",
            "[Epoch 0/10] [Batch 1636/7121] [D loss: 0.496169] [G loss: 0.872572] [AE loss: 1.158545]\n",
            "[Epoch 0/10] [Batch 1637/7121] [D loss: 0.505781] [G loss: 0.866863] [AE loss: 1.114144]\n",
            "[Epoch 0/10] [Batch 1638/7121] [D loss: 0.516452] [G loss: 0.849855] [AE loss: 1.482910]\n",
            "[Epoch 0/10] [Batch 1639/7121] [D loss: 0.496769] [G loss: 0.852443] [AE loss: 1.419250]\n",
            "[Epoch 0/10] [Batch 1640/7121] [D loss: 0.587306] [G loss: 0.827461] [AE loss: 1.782285]\n",
            "[Epoch 0/10] [Batch 1641/7121] [D loss: 0.519529] [G loss: 0.821925] [AE loss: 1.483052]\n",
            "[Epoch 0/10] [Batch 1642/7121] [D loss: 0.480143] [G loss: 0.827374] [AE loss: 0.965568]\n",
            "[Epoch 0/10] [Batch 1643/7121] [D loss: 0.518486] [G loss: 0.802864] [AE loss: 1.398428]\n",
            "[Epoch 0/10] [Batch 1644/7121] [D loss: 0.527494] [G loss: 0.794716] [AE loss: 1.430915]\n",
            "[Epoch 0/10] [Batch 1645/7121] [D loss: 0.557672] [G loss: 0.775500] [AE loss: 1.066642]\n",
            "[Epoch 0/10] [Batch 1646/7121] [D loss: 0.578175] [G loss: 0.775272] [AE loss: 1.108863]\n",
            "[Epoch 0/10] [Batch 1647/7121] [D loss: 0.539695] [G loss: 0.781168] [AE loss: 0.973113]\n",
            "[Epoch 0/10] [Batch 1648/7121] [D loss: 0.539908] [G loss: 0.783346] [AE loss: 0.909100]\n",
            "[Epoch 0/10] [Batch 1649/7121] [D loss: 0.573558] [G loss: 0.779770] [AE loss: 0.878391]\n",
            "[Epoch 0/10] [Batch 1650/7121] [D loss: 0.558049] [G loss: 0.780632] [AE loss: 0.975752]\n",
            "[Epoch 0/10] [Batch 1651/7121] [D loss: 0.539456] [G loss: 0.776346] [AE loss: 1.349778]\n",
            "[Epoch 0/10] [Batch 1652/7121] [D loss: 0.556600] [G loss: 0.782343] [AE loss: 1.334292]\n",
            "[Epoch 0/10] [Batch 1653/7121] [D loss: 0.553942] [G loss: 0.777505] [AE loss: 1.143873]\n",
            "[Epoch 0/10] [Batch 1654/7121] [D loss: 0.571904] [G loss: 0.811312] [AE loss: 3.641155]\n",
            "[Epoch 0/10] [Batch 1655/7121] [D loss: 0.509907] [G loss: 0.815978] [AE loss: 1.361884]\n",
            "[Epoch 0/10] [Batch 1656/7121] [D loss: 0.488890] [G loss: 0.850471] [AE loss: 1.397274]\n",
            "[Epoch 0/10] [Batch 1657/7121] [D loss: 0.518011] [G loss: 0.855203] [AE loss: 1.023995]\n",
            "[Epoch 0/10] [Batch 1658/7121] [D loss: 0.489353] [G loss: 0.862735] [AE loss: 1.217567]\n",
            "[Epoch 0/10] [Batch 1659/7121] [D loss: 0.539913] [G loss: 0.867511] [AE loss: 0.994444]\n",
            "[Epoch 0/10] [Batch 1660/7121] [D loss: 0.497289] [G loss: 0.908777] [AE loss: 1.888391]\n",
            "[Epoch 0/10] [Batch 1661/7121] [D loss: 0.479189] [G loss: 0.919106] [AE loss: 1.558970]\n",
            "[Epoch 0/10] [Batch 1662/7121] [D loss: 0.502549] [G loss: 0.930023] [AE loss: 1.411978]\n",
            "[Epoch 0/10] [Batch 1663/7121] [D loss: 0.478226] [G loss: 0.940656] [AE loss: 1.281046]\n",
            "[Epoch 0/10] [Batch 1664/7121] [D loss: 0.451687] [G loss: 0.962341] [AE loss: 1.535145]\n",
            "[Epoch 0/10] [Batch 1665/7121] [D loss: 0.453271] [G loss: 0.976239] [AE loss: 1.544526]\n",
            "[Epoch 0/10] [Batch 1666/7121] [D loss: 0.453970] [G loss: 1.007683] [AE loss: 1.626976]\n",
            "[Epoch 0/10] [Batch 1667/7121] [D loss: 0.439210] [G loss: 1.018558] [AE loss: 1.277887]\n",
            "[Epoch 0/10] [Batch 1668/7121] [D loss: 0.453900] [G loss: 1.025809] [AE loss: 1.282563]\n",
            "[Epoch 0/10] [Batch 1669/7121] [D loss: 0.435865] [G loss: 1.044469] [AE loss: 0.986526]\n",
            "[Epoch 0/10] [Batch 1670/7121] [D loss: 0.429909] [G loss: 1.056371] [AE loss: 1.510811]\n",
            "[Epoch 0/10] [Batch 1671/7121] [D loss: 0.434409] [G loss: 1.052149] [AE loss: 1.529964]\n",
            "[Epoch 0/10] [Batch 1672/7121] [D loss: 0.438520] [G loss: 1.061043] [AE loss: 1.254032]\n",
            "[Epoch 0/10] [Batch 1673/7121] [D loss: 0.413959] [G loss: 1.057340] [AE loss: 3.597298]\n",
            "[Epoch 0/10] [Batch 1674/7121] [D loss: 0.467930] [G loss: 1.083478] [AE loss: 0.970737]\n",
            "[Epoch 0/10] [Batch 1675/7121] [D loss: 0.422264] [G loss: 1.077558] [AE loss: 1.172355]\n",
            "[Epoch 0/10] [Batch 1676/7121] [D loss: 0.444953] [G loss: 1.069224] [AE loss: 1.170816]\n",
            "[Epoch 0/10] [Batch 1677/7121] [D loss: 0.430743] [G loss: 1.078948] [AE loss: 0.976087]\n",
            "[Epoch 0/10] [Batch 1678/7121] [D loss: 0.423340] [G loss: 1.084220] [AE loss: 2.323239]\n",
            "[Epoch 0/10] [Batch 1679/7121] [D loss: 0.449590] [G loss: 1.062469] [AE loss: 1.323430]\n",
            "[Epoch 0/10] [Batch 1680/7121] [D loss: 0.433056] [G loss: 1.046919] [AE loss: 1.020321]\n",
            "[Epoch 0/10] [Batch 1681/7121] [D loss: 0.438451] [G loss: 1.034467] [AE loss: 1.627628]\n",
            "[Epoch 0/10] [Batch 1682/7121] [D loss: 0.395913] [G loss: 1.014712] [AE loss: 1.394184]\n",
            "[Epoch 0/10] [Batch 1683/7121] [D loss: 0.514722] [G loss: 0.987402] [AE loss: 1.223930]\n",
            "[Epoch 0/10] [Batch 1684/7121] [D loss: 0.466453] [G loss: 0.977056] [AE loss: 1.193934]\n",
            "[Epoch 0/10] [Batch 1685/7121] [D loss: 0.395639] [G loss: 0.980989] [AE loss: 1.481234]\n",
            "[Epoch 0/10] [Batch 1686/7121] [D loss: 0.420011] [G loss: 0.959581] [AE loss: 1.356596]\n",
            "[Epoch 0/10] [Batch 1687/7121] [D loss: 0.475524] [G loss: 0.965729] [AE loss: 1.713082]\n",
            "[Epoch 0/10] [Batch 1688/7121] [D loss: 0.476191] [G loss: 0.976759] [AE loss: 1.106480]\n",
            "[Epoch 0/10] [Batch 1689/7121] [D loss: 0.454642] [G loss: 0.980231] [AE loss: 1.035632]\n",
            "[Epoch 0/10] [Batch 1690/7121] [D loss: 0.424825] [G loss: 1.004217] [AE loss: 1.062748]\n",
            "[Epoch 0/10] [Batch 1691/7121] [D loss: 0.399557] [G loss: 1.009421] [AE loss: 1.250350]\n",
            "[Epoch 0/10] [Batch 1692/7121] [D loss: 0.461919] [G loss: 1.003525] [AE loss: 2.403870]\n",
            "[Epoch 0/10] [Batch 1693/7121] [D loss: 0.458514] [G loss: 1.007785] [AE loss: 1.451385]\n",
            "[Epoch 0/10] [Batch 1694/7121] [D loss: 0.422275] [G loss: 1.030581] [AE loss: 1.463535]\n",
            "[Epoch 0/10] [Batch 1695/7121] [D loss: 0.490649] [G loss: 1.011378] [AE loss: 1.130622]\n",
            "[Epoch 0/10] [Batch 1696/7121] [D loss: 0.349148] [G loss: 1.037192] [AE loss: 1.301893]\n",
            "[Epoch 0/10] [Batch 1697/7121] [D loss: 0.470148] [G loss: 1.027847] [AE loss: 1.506601]\n",
            "[Epoch 0/10] [Batch 1698/7121] [D loss: 0.460202] [G loss: 1.009564] [AE loss: 1.244247]\n",
            "[Epoch 0/10] [Batch 1699/7121] [D loss: 0.404371] [G loss: 1.010136] [AE loss: 3.375052]\n",
            "[Epoch 0/10] [Batch 1700/7121] [D loss: 0.430448] [G loss: 0.972555] [AE loss: 1.607970]\n",
            "[Epoch 0/10] [Batch 1701/7121] [D loss: 0.461269] [G loss: 0.953195] [AE loss: 1.213066]\n",
            "[Epoch 0/10] [Batch 1702/7121] [D loss: 0.472974] [G loss: 0.951459] [AE loss: 1.329777]\n",
            "[Epoch 0/10] [Batch 1703/7121] [D loss: 0.525688] [G loss: 0.923136] [AE loss: 1.289958]\n",
            "[Epoch 0/10] [Batch 1704/7121] [D loss: 0.540703] [G loss: 0.840723] [AE loss: 1.402921]\n",
            "[Epoch 0/10] [Batch 1705/7121] [D loss: 0.603476] [G loss: 0.820603] [AE loss: 1.125691]\n",
            "[Epoch 0/10] [Batch 1706/7121] [D loss: 0.537457] [G loss: 0.765311] [AE loss: 1.645966]\n",
            "[Epoch 0/10] [Batch 1707/7121] [D loss: 0.515222] [G loss: 0.801170] [AE loss: 1.283350]\n",
            "[Epoch 0/10] [Batch 1708/7121] [D loss: 0.537787] [G loss: 0.713326] [AE loss: 1.309551]\n",
            "[Epoch 0/10] [Batch 1709/7121] [D loss: 0.533597] [G loss: 0.764287] [AE loss: 6.902536]\n",
            "[Epoch 0/10] [Batch 1710/7121] [D loss: 0.618144] [G loss: 0.731493] [AE loss: 1.119613]\n",
            "[Epoch 0/10] [Batch 1711/7121] [D loss: 0.616140] [G loss: 0.693375] [AE loss: 1.365433]\n",
            "[Epoch 0/10] [Batch 1712/7121] [D loss: 0.592381] [G loss: 0.700215] [AE loss: 1.349096]\n",
            "[Epoch 0/10] [Batch 1713/7121] [D loss: 0.543306] [G loss: 0.722501] [AE loss: 1.358361]\n",
            "[Epoch 0/10] [Batch 1714/7121] [D loss: 0.530421] [G loss: 0.739982] [AE loss: 1.496600]\n",
            "[Epoch 0/10] [Batch 1715/7121] [D loss: 0.689444] [G loss: 0.697244] [AE loss: 1.698268]\n",
            "[Epoch 0/10] [Batch 1716/7121] [D loss: 0.629566] [G loss: 0.689828] [AE loss: 1.404739]\n",
            "[Epoch 0/10] [Batch 1717/7121] [D loss: 0.556270] [G loss: 0.688388] [AE loss: 1.499968]\n",
            "[Epoch 0/10] [Batch 1718/7121] [D loss: 0.511349] [G loss: 0.664196] [AE loss: 3.641819]\n",
            "[Epoch 0/10] [Batch 1719/7121] [D loss: 0.636717] [G loss: 0.680207] [AE loss: 1.493845]\n",
            "[Epoch 0/10] [Batch 1720/7121] [D loss: 0.601669] [G loss: 0.647333] [AE loss: 2.457043]\n",
            "[Epoch 0/10] [Batch 1721/7121] [D loss: 0.559723] [G loss: 0.624875] [AE loss: 1.173916]\n",
            "[Epoch 0/10] [Batch 1722/7121] [D loss: 0.741754] [G loss: 0.643131] [AE loss: 1.248325]\n",
            "[Epoch 0/10] [Batch 1723/7121] [D loss: 0.650570] [G loss: 0.665306] [AE loss: 3.452284]\n",
            "[Epoch 0/10] [Batch 1724/7121] [D loss: 0.694966] [G loss: 0.670767] [AE loss: 1.491192]\n",
            "[Epoch 0/10] [Batch 1725/7121] [D loss: 0.730474] [G loss: 0.680277] [AE loss: 1.367070]\n",
            "[Epoch 0/10] [Batch 1726/7121] [D loss: 0.603751] [G loss: 0.703785] [AE loss: 1.329283]\n",
            "[Epoch 0/10] [Batch 1727/7121] [D loss: 0.729442] [G loss: 0.704597] [AE loss: 1.070640]\n",
            "[Epoch 0/10] [Batch 1728/7121] [D loss: 0.654241] [G loss: 0.767753] [AE loss: 1.335486]\n",
            "[Epoch 0/10] [Batch 1729/7121] [D loss: 0.742832] [G loss: 0.742092] [AE loss: 1.340217]\n",
            "[Epoch 0/10] [Batch 1730/7121] [D loss: 0.758529] [G loss: 0.712141] [AE loss: 2.527081]\n",
            "[Epoch 0/10] [Batch 1731/7121] [D loss: 0.768957] [G loss: 0.782174] [AE loss: 1.110146]\n",
            "[Epoch 0/10] [Batch 1732/7121] [D loss: 0.611101] [G loss: 0.780005] [AE loss: 1.501418]\n",
            "[Epoch 0/10] [Batch 1733/7121] [D loss: 0.559914] [G loss: 0.793269] [AE loss: 1.656229]\n",
            "[Epoch 0/10] [Batch 1734/7121] [D loss: 0.604203] [G loss: 0.819606] [AE loss: 1.290156]\n",
            "[Epoch 0/10] [Batch 1735/7121] [D loss: 0.713874] [G loss: 0.838860] [AE loss: 1.105799]\n",
            "[Epoch 0/10] [Batch 1736/7121] [D loss: 0.755261] [G loss: 0.879815] [AE loss: 1.184640]\n",
            "[Epoch 0/10] [Batch 1737/7121] [D loss: 0.518820] [G loss: 0.927773] [AE loss: 1.153497]\n",
            "[Epoch 0/10] [Batch 1738/7121] [D loss: 0.528982] [G loss: 0.893757] [AE loss: 1.249152]\n",
            "[Epoch 0/10] [Batch 1739/7121] [D loss: 0.517105] [G loss: 0.974188] [AE loss: 1.167667]\n",
            "[Epoch 0/10] [Batch 1740/7121] [D loss: 0.505379] [G loss: 0.955471] [AE loss: 1.696476]\n",
            "[Epoch 0/10] [Batch 1741/7121] [D loss: 0.464537] [G loss: 1.006980] [AE loss: 1.292239]\n",
            "[Epoch 0/10] [Batch 1742/7121] [D loss: 0.531214] [G loss: 1.076841] [AE loss: 4.337137]\n",
            "[Epoch 0/10] [Batch 1743/7121] [D loss: 0.439286] [G loss: 1.050112] [AE loss: 6.161713]\n",
            "[Epoch 0/10] [Batch 1744/7121] [D loss: 0.612099] [G loss: 1.086015] [AE loss: 1.003709]\n",
            "[Epoch 0/10] [Batch 1745/7121] [D loss: 0.440904] [G loss: 1.096003] [AE loss: 1.143131]\n",
            "[Epoch 0/10] [Batch 1746/7121] [D loss: 0.456046] [G loss: 1.108266] [AE loss: 1.239845]\n",
            "[Epoch 0/10] [Batch 1747/7121] [D loss: 0.383060] [G loss: 1.102815] [AE loss: 1.207498]\n",
            "[Epoch 0/10] [Batch 1748/7121] [D loss: 0.643568] [G loss: 1.054629] [AE loss: 1.194939]\n",
            "[Epoch 0/10] [Batch 1749/7121] [D loss: 0.462469] [G loss: 1.014184] [AE loss: 1.216344]\n",
            "[Epoch 0/10] [Batch 1750/7121] [D loss: 0.467583] [G loss: 0.988975] [AE loss: 2.575449]\n",
            "[Epoch 0/10] [Batch 1751/7121] [D loss: 0.593272] [G loss: 0.936591] [AE loss: 1.101925]\n",
            "[Epoch 0/10] [Batch 1752/7121] [D loss: 0.605072] [G loss: 0.862723] [AE loss: 1.317806]\n",
            "[Epoch 0/10] [Batch 1753/7121] [D loss: 0.621454] [G loss: 0.803390] [AE loss: 1.953459]\n",
            "[Epoch 0/10] [Batch 1754/7121] [D loss: 0.638730] [G loss: 0.834861] [AE loss: 1.185155]\n",
            "[Epoch 0/10] [Batch 1755/7121] [D loss: 0.520901] [G loss: 0.802715] [AE loss: 1.518681]\n",
            "[Epoch 0/10] [Batch 1756/7121] [D loss: 0.565898] [G loss: 0.717826] [AE loss: 1.109905]\n",
            "[Epoch 0/10] [Batch 1757/7121] [D loss: 0.581968] [G loss: 0.726238] [AE loss: 1.398363]\n",
            "[Epoch 0/10] [Batch 1758/7121] [D loss: 0.574120] [G loss: 0.760122] [AE loss: 1.163110]\n",
            "[Epoch 0/10] [Batch 1759/7121] [D loss: 0.630285] [G loss: 0.760127] [AE loss: 2.747808]\n",
            "[Epoch 0/10] [Batch 1760/7121] [D loss: 0.564394] [G loss: 0.757737] [AE loss: 1.302611]\n",
            "[Epoch 0/10] [Batch 1761/7121] [D loss: 0.617890] [G loss: 0.772968] [AE loss: 1.137532]\n",
            "[Epoch 0/10] [Batch 1762/7121] [D loss: 0.622104] [G loss: 0.761561] [AE loss: 1.403975]\n",
            "[Epoch 0/10] [Batch 1763/7121] [D loss: 0.635977] [G loss: 0.722466] [AE loss: 1.776585]\n",
            "[Epoch 0/10] [Batch 1764/7121] [D loss: 0.559382] [G loss: 0.726503] [AE loss: 1.387327]\n",
            "[Epoch 0/10] [Batch 1765/7121] [D loss: 0.651970] [G loss: 0.719389] [AE loss: 1.277862]\n",
            "[Epoch 0/10] [Batch 1766/7121] [D loss: 0.614315] [G loss: 0.721293] [AE loss: 1.516325]\n",
            "[Epoch 0/10] [Batch 1767/7121] [D loss: 0.637120] [G loss: 0.683531] [AE loss: 1.212993]\n",
            "[Epoch 0/10] [Batch 1768/7121] [D loss: 0.579203] [G loss: 0.652698] [AE loss: 1.246648]\n",
            "[Epoch 0/10] [Batch 1769/7121] [D loss: 0.575587] [G loss: 0.622275] [AE loss: 1.062208]\n",
            "[Epoch 0/10] [Batch 1770/7121] [D loss: 0.742601] [G loss: 0.596581] [AE loss: 1.205048]\n",
            "[Epoch 0/10] [Batch 1771/7121] [D loss: 0.701752] [G loss: 0.589994] [AE loss: 1.355428]\n",
            "[Epoch 0/10] [Batch 1772/7121] [D loss: 0.753547] [G loss: 0.590354] [AE loss: 1.827826]\n",
            "[Epoch 0/10] [Batch 1773/7121] [D loss: 0.763911] [G loss: 0.553712] [AE loss: 2.145806]\n",
            "[Epoch 0/10] [Batch 1774/7121] [D loss: 0.734814] [G loss: 0.545385] [AE loss: 1.031711]\n",
            "[Epoch 0/10] [Batch 1775/7121] [D loss: 0.733207] [G loss: 0.524142] [AE loss: 1.451342]\n",
            "[Epoch 0/10] [Batch 1776/7121] [D loss: 0.801018] [G loss: 0.561711] [AE loss: 1.193379]\n",
            "[Epoch 0/10] [Batch 1777/7121] [D loss: 0.647527] [G loss: 0.584650] [AE loss: 1.226921]\n",
            "[Epoch 0/10] [Batch 1778/7121] [D loss: 0.737409] [G loss: 0.613011] [AE loss: 1.274650]\n",
            "[Epoch 0/10] [Batch 1779/7121] [D loss: 0.676391] [G loss: 0.671093] [AE loss: 1.516668]\n",
            "[Epoch 0/10] [Batch 1780/7121] [D loss: 0.678470] [G loss: 0.724990] [AE loss: 1.529734]\n",
            "[Epoch 0/10] [Batch 1781/7121] [D loss: 0.673667] [G loss: 0.797546] [AE loss: 1.173560]\n",
            "[Epoch 0/10] [Batch 1782/7121] [D loss: 0.706430] [G loss: 0.842322] [AE loss: 1.394318]\n",
            "[Epoch 0/10] [Batch 1783/7121] [D loss: 0.622877] [G loss: 0.890421] [AE loss: 1.094930]\n",
            "[Epoch 0/10] [Batch 1784/7121] [D loss: 0.663275] [G loss: 0.953267] [AE loss: 2.018918]\n",
            "[Epoch 0/10] [Batch 1785/7121] [D loss: 0.679015] [G loss: 0.946897] [AE loss: 4.186383]\n",
            "[Epoch 0/10] [Batch 1786/7121] [D loss: 0.600015] [G loss: 0.983612] [AE loss: 1.506986]\n",
            "[Epoch 0/10] [Batch 1787/7121] [D loss: 0.804363] [G loss: 0.963437] [AE loss: 1.135841]\n",
            "[Epoch 0/10] [Batch 1788/7121] [D loss: 0.727800] [G loss: 0.971815] [AE loss: 1.340150]\n",
            "[Epoch 0/10] [Batch 1789/7121] [D loss: 0.627031] [G loss: 0.951974] [AE loss: 3.994534]\n",
            "[Epoch 0/10] [Batch 1790/7121] [D loss: 0.865765] [G loss: 0.946502] [AE loss: 1.245459]\n",
            "[Epoch 0/10] [Batch 1791/7121] [D loss: 0.720359] [G loss: 1.030155] [AE loss: 1.568352]\n",
            "[Epoch 0/10] [Batch 1792/7121] [D loss: 0.651996] [G loss: 1.055183] [AE loss: 1.252503]\n",
            "[Epoch 0/10] [Batch 1793/7121] [D loss: 0.674157] [G loss: 1.062918] [AE loss: 1.792607]\n",
            "[Epoch 0/10] [Batch 1794/7121] [D loss: 0.621287] [G loss: 1.118023] [AE loss: 1.129095]\n",
            "[Epoch 0/10] [Batch 1795/7121] [D loss: 0.786347] [G loss: 1.201296] [AE loss: 2.196346]\n",
            "[Epoch 0/10] [Batch 1796/7121] [D loss: 0.661463] [G loss: 1.188334] [AE loss: 1.555534]\n",
            "[Epoch 0/10] [Batch 1797/7121] [D loss: 0.705415] [G loss: 1.164569] [AE loss: 1.693403]\n",
            "[Epoch 0/10] [Batch 1798/7121] [D loss: 0.635111] [G loss: 1.200010] [AE loss: 1.254874]\n",
            "[Epoch 0/10] [Batch 1799/7121] [D loss: 0.562999] [G loss: 1.162842] [AE loss: 1.257036]\n",
            "[Epoch 0/10] [Batch 1800/7121] [D loss: 0.640389] [G loss: 1.116058] [AE loss: 1.514452]\n",
            "[Epoch 0/10] [Batch 1801/7121] [D loss: 0.554369] [G loss: 1.119305] [AE loss: 1.134587]\n",
            "[Epoch 0/10] [Batch 1802/7121] [D loss: 0.644278] [G loss: 1.057439] [AE loss: 1.123379]\n",
            "[Epoch 0/10] [Batch 1803/7121] [D loss: 0.584010] [G loss: 1.039648] [AE loss: 1.305547]\n",
            "[Epoch 0/10] [Batch 1804/7121] [D loss: 0.614599] [G loss: 1.069035] [AE loss: 1.055184]\n",
            "[Epoch 0/10] [Batch 1805/7121] [D loss: 0.608062] [G loss: 0.991352] [AE loss: 1.198039]\n",
            "[Epoch 0/10] [Batch 1806/7121] [D loss: 0.565203] [G loss: 0.894223] [AE loss: 3.441838]\n",
            "[Epoch 0/10] [Batch 1807/7121] [D loss: 0.493775] [G loss: 0.848582] [AE loss: 1.213811]\n",
            "[Epoch 0/10] [Batch 1808/7121] [D loss: 0.594280] [G loss: 0.912801] [AE loss: 1.229933]\n",
            "[Epoch 0/10] [Batch 1809/7121] [D loss: 0.573189] [G loss: 0.841136] [AE loss: 1.332201]\n",
            "[Epoch 0/10] [Batch 1810/7121] [D loss: 0.508178] [G loss: 0.874051] [AE loss: 2.088779]\n",
            "[Epoch 0/10] [Batch 1811/7121] [D loss: 0.562747] [G loss: 0.865665] [AE loss: 1.382498]\n",
            "[Epoch 0/10] [Batch 1812/7121] [D loss: 0.478563] [G loss: 0.890822] [AE loss: 1.243756]\n",
            "[Epoch 0/10] [Batch 1813/7121] [D loss: 0.474210] [G loss: 0.930362] [AE loss: 1.542397]\n",
            "[Epoch 0/10] [Batch 1814/7121] [D loss: 0.536900] [G loss: 0.926118] [AE loss: 1.016065]\n",
            "[Epoch 0/10] [Batch 1815/7121] [D loss: 0.498976] [G loss: 0.968427] [AE loss: 1.132030]\n",
            "[Epoch 0/10] [Batch 1816/7121] [D loss: 0.466674] [G loss: 0.962527] [AE loss: 1.863216]\n",
            "[Epoch 0/10] [Batch 1817/7121] [D loss: 0.424447] [G loss: 0.954015] [AE loss: 3.129431]\n",
            "[Epoch 0/10] [Batch 1818/7121] [D loss: 0.501799] [G loss: 0.923306] [AE loss: 1.996870]\n",
            "[Epoch 0/10] [Batch 1819/7121] [D loss: 0.578927] [G loss: 0.989296] [AE loss: 1.077396]\n",
            "[Epoch 0/10] [Batch 1820/7121] [D loss: 0.539912] [G loss: 0.942829] [AE loss: 1.188733]\n",
            "[Epoch 0/10] [Batch 1821/7121] [D loss: 0.636606] [G loss: 0.898066] [AE loss: 1.960847]\n",
            "[Epoch 0/10] [Batch 1822/7121] [D loss: 0.561071] [G loss: 0.873748] [AE loss: 1.062847]\n",
            "[Epoch 0/10] [Batch 1823/7121] [D loss: 0.525781] [G loss: 0.843666] [AE loss: 1.436689]\n",
            "[Epoch 0/10] [Batch 1824/7121] [D loss: 0.501211] [G loss: 0.833282] [AE loss: 2.490000]\n",
            "[Epoch 0/10] [Batch 1825/7121] [D loss: 0.591000] [G loss: 0.880506] [AE loss: 1.171773]\n",
            "[Epoch 0/10] [Batch 1826/7121] [D loss: 0.537249] [G loss: 0.888833] [AE loss: 1.238396]\n",
            "[Epoch 0/10] [Batch 1827/7121] [D loss: 0.537441] [G loss: 0.932049] [AE loss: 1.079241]\n",
            "[Epoch 0/10] [Batch 1828/7121] [D loss: 0.497427] [G loss: 0.907509] [AE loss: 1.634270]\n",
            "[Epoch 0/10] [Batch 1829/7121] [D loss: 0.499034] [G loss: 0.956092] [AE loss: 1.209569]\n",
            "[Epoch 0/10] [Batch 1830/7121] [D loss: 0.492698] [G loss: 0.997461] [AE loss: 1.259459]\n",
            "[Epoch 0/10] [Batch 1831/7121] [D loss: 0.486013] [G loss: 1.026600] [AE loss: 1.219478]\n",
            "[Epoch 0/10] [Batch 1832/7121] [D loss: 0.552592] [G loss: 1.028488] [AE loss: 1.209200]\n",
            "[Epoch 0/10] [Batch 1833/7121] [D loss: 0.390890] [G loss: 1.082280] [AE loss: 1.586401]\n",
            "[Epoch 0/10] [Batch 1834/7121] [D loss: 0.371478] [G loss: 1.135038] [AE loss: 1.355144]\n",
            "[Epoch 0/10] [Batch 1835/7121] [D loss: 0.326108] [G loss: 1.169234] [AE loss: 1.616680]\n",
            "[Epoch 0/10] [Batch 1836/7121] [D loss: 0.475213] [G loss: 1.174448] [AE loss: 1.050807]\n",
            "[Epoch 0/10] [Batch 1837/7121] [D loss: 0.421226] [G loss: 1.162134] [AE loss: 1.181775]\n",
            "[Epoch 0/10] [Batch 1838/7121] [D loss: 0.438915] [G loss: 1.163379] [AE loss: 1.093457]\n",
            "[Epoch 0/10] [Batch 1839/7121] [D loss: 0.399175] [G loss: 1.123702] [AE loss: 1.178059]\n",
            "[Epoch 0/10] [Batch 1840/7121] [D loss: 0.360195] [G loss: 1.112107] [AE loss: 1.682927]\n",
            "[Epoch 0/10] [Batch 1841/7121] [D loss: 0.384121] [G loss: 1.109555] [AE loss: 1.659452]\n",
            "[Epoch 0/10] [Batch 1842/7121] [D loss: 0.388994] [G loss: 1.101311] [AE loss: 2.025161]\n",
            "[Epoch 0/10] [Batch 1843/7121] [D loss: 0.357346] [G loss: 1.053692] [AE loss: 1.344658]\n",
            "[Epoch 0/10] [Batch 1844/7121] [D loss: 0.522927] [G loss: 0.997902] [AE loss: 1.442215]\n",
            "[Epoch 0/10] [Batch 1845/7121] [D loss: 0.470291] [G loss: 0.893860] [AE loss: 1.444089]\n",
            "[Epoch 0/10] [Batch 1846/7121] [D loss: 0.547405] [G loss: 0.817690] [AE loss: 1.748391]\n",
            "[Epoch 0/10] [Batch 1847/7121] [D loss: 0.465125] [G loss: 0.804532] [AE loss: 1.368365]\n",
            "[Epoch 0/10] [Batch 1848/7121] [D loss: 0.548448] [G loss: 0.743661] [AE loss: 1.411507]\n",
            "[Epoch 0/10] [Batch 1849/7121] [D loss: 0.533605] [G loss: 0.695603] [AE loss: 3.709236]\n",
            "[Epoch 0/10] [Batch 1850/7121] [D loss: 0.541968] [G loss: 0.727304] [AE loss: 1.632638]\n",
            "[Epoch 0/10] [Batch 1851/7121] [D loss: 0.682857] [G loss: 0.701303] [AE loss: 1.313958]\n",
            "[Epoch 0/10] [Batch 1852/7121] [D loss: 0.625215] [G loss: 0.788166] [AE loss: 2.066929]\n",
            "[Epoch 0/10] [Batch 1853/7121] [D loss: 0.546665] [G loss: 0.805483] [AE loss: 1.914047]\n",
            "[Epoch 0/10] [Batch 1854/7121] [D loss: 0.501206] [G loss: 0.843099] [AE loss: 1.502685]\n",
            "[Epoch 0/10] [Batch 1855/7121] [D loss: 0.517989] [G loss: 0.978990] [AE loss: 1.684038]\n",
            "[Epoch 0/10] [Batch 1856/7121] [D loss: 0.511229] [G loss: 1.002629] [AE loss: 2.178400]\n",
            "[Epoch 0/10] [Batch 1857/7121] [D loss: 0.398787] [G loss: 1.192497] [AE loss: 1.653543]\n",
            "[Epoch 0/10] [Batch 1858/7121] [D loss: 0.512259] [G loss: 1.147924] [AE loss: 1.376002]\n",
            "[Epoch 0/10] [Batch 1859/7121] [D loss: 0.631023] [G loss: 1.259838] [AE loss: 1.544386]\n",
            "[Epoch 0/10] [Batch 1860/7121] [D loss: 0.496128] [G loss: 1.173763] [AE loss: 2.080765]\n",
            "[Epoch 0/10] [Batch 1861/7121] [D loss: 0.428002] [G loss: 1.208508] [AE loss: 1.688594]\n",
            "[Epoch 0/10] [Batch 1862/7121] [D loss: 0.544185] [G loss: 1.083045] [AE loss: 1.441221]\n",
            "[Epoch 0/10] [Batch 1863/7121] [D loss: 0.482407] [G loss: 0.869475] [AE loss: 1.503469]\n",
            "[Epoch 0/10] [Batch 1864/7121] [D loss: 0.661591] [G loss: 0.827177] [AE loss: 2.239259]\n",
            "[Epoch 0/10] [Batch 1865/7121] [D loss: 0.691756] [G loss: 0.687372] [AE loss: 1.326341]\n",
            "[Epoch 0/10] [Batch 1866/7121] [D loss: 0.631965] [G loss: 0.698658] [AE loss: 1.310184]\n",
            "[Epoch 0/10] [Batch 1867/7121] [D loss: 0.632372] [G loss: 0.604058] [AE loss: 1.279920]\n",
            "[Epoch 0/10] [Batch 1868/7121] [D loss: 0.539681] [G loss: 0.692639] [AE loss: 2.635441]\n",
            "[Epoch 0/10] [Batch 1869/7121] [D loss: 0.595875] [G loss: 0.703883] [AE loss: 1.782848]\n",
            "[Epoch 0/10] [Batch 1870/7121] [D loss: 0.567710] [G loss: 0.811831] [AE loss: 1.732588]\n",
            "[Epoch 0/10] [Batch 1871/7121] [D loss: 0.734668] [G loss: 0.789030] [AE loss: 1.720739]\n",
            "[Epoch 0/10] [Batch 1872/7121] [D loss: 0.676479] [G loss: 0.819593] [AE loss: 1.967510]\n",
            "[Epoch 0/10] [Batch 1873/7121] [D loss: 0.529656] [G loss: 0.856215] [AE loss: 1.676932]\n",
            "[Epoch 0/10] [Batch 1874/7121] [D loss: 0.834139] [G loss: 0.812176] [AE loss: 1.435518]\n",
            "[Epoch 0/10] [Batch 1875/7121] [D loss: 0.574006] [G loss: 0.836410] [AE loss: 1.487316]\n",
            "[Epoch 0/10] [Batch 1876/7121] [D loss: 0.694537] [G loss: 0.861843] [AE loss: 1.269244]\n",
            "[Epoch 0/10] [Batch 1877/7121] [D loss: 0.587918] [G loss: 0.845411] [AE loss: 1.557974]\n",
            "[Epoch 0/10] [Batch 1878/7121] [D loss: 0.685189] [G loss: 0.803499] [AE loss: 1.317643]\n",
            "[Epoch 0/10] [Batch 1879/7121] [D loss: 0.811129] [G loss: 0.760463] [AE loss: 1.361118]\n",
            "[Epoch 0/10] [Batch 1880/7121] [D loss: 0.777023] [G loss: 0.738940] [AE loss: 1.562468]\n",
            "[Epoch 0/10] [Batch 1881/7121] [D loss: 0.667131] [G loss: 0.709416] [AE loss: 1.128837]\n",
            "[Epoch 0/10] [Batch 1882/7121] [D loss: 0.771493] [G loss: 0.683932] [AE loss: 1.489608]\n",
            "[Epoch 0/10] [Batch 1883/7121] [D loss: 0.954206] [G loss: 0.659792] [AE loss: 1.514180]\n",
            "[Epoch 0/10] [Batch 1884/7121] [D loss: 0.784316] [G loss: 0.733271] [AE loss: 1.334710]\n",
            "[Epoch 0/10] [Batch 1885/7121] [D loss: 0.825246] [G loss: 0.666869] [AE loss: 1.678892]\n",
            "[Epoch 0/10] [Batch 1886/7121] [D loss: 0.847607] [G loss: 0.689693] [AE loss: 2.513735]\n",
            "[Epoch 0/10] [Batch 1887/7121] [D loss: 0.848433] [G loss: 0.672891] [AE loss: 1.318921]\n",
            "[Epoch 0/10] [Batch 1888/7121] [D loss: 0.871960] [G loss: 0.679232] [AE loss: 1.720571]\n",
            "[Epoch 0/10] [Batch 1889/7121] [D loss: 0.838526] [G loss: 0.806989] [AE loss: 1.527259]\n",
            "[Epoch 0/10] [Batch 1890/7121] [D loss: 0.735121] [G loss: 0.825823] [AE loss: 1.738894]\n",
            "[Epoch 0/10] [Batch 1891/7121] [D loss: 0.929163] [G loss: 0.848675] [AE loss: 1.439604]\n",
            "[Epoch 0/10] [Batch 1892/7121] [D loss: 0.941481] [G loss: 0.947213] [AE loss: 1.597723]\n",
            "[Epoch 0/10] [Batch 1893/7121] [D loss: 0.709615] [G loss: 0.996359] [AE loss: 1.786438]\n",
            "[Epoch 0/10] [Batch 1894/7121] [D loss: 0.787497] [G loss: 1.104181] [AE loss: 1.999396]\n",
            "[Epoch 0/10] [Batch 1895/7121] [D loss: 0.622542] [G loss: 1.265067] [AE loss: 1.301367]\n",
            "[Epoch 0/10] [Batch 1896/7121] [D loss: 0.717541] [G loss: 1.292994] [AE loss: 1.444836]\n",
            "[Epoch 0/10] [Batch 1897/7121] [D loss: 0.688556] [G loss: 1.320904] [AE loss: 1.870966]\n",
            "[Epoch 0/10] [Batch 1898/7121] [D loss: 0.657532] [G loss: 1.363356] [AE loss: 1.337328]\n",
            "[Epoch 0/10] [Batch 1899/7121] [D loss: 0.620862] [G loss: 1.322780] [AE loss: 1.366608]\n",
            "[Epoch 0/10] [Batch 1900/7121] [D loss: 0.600690] [G loss: 1.227938] [AE loss: 1.452269]\n",
            "[Epoch 0/10] [Batch 1901/7121] [D loss: 0.714143] [G loss: 1.165989] [AE loss: 1.279283]\n",
            "[Epoch 0/10] [Batch 1902/7121] [D loss: 0.846553] [G loss: 1.001320] [AE loss: 1.236459]\n",
            "[Epoch 0/10] [Batch 1903/7121] [D loss: 0.741189] [G loss: 0.918321] [AE loss: 1.495048]\n",
            "[Epoch 0/10] [Batch 1904/7121] [D loss: 0.752181] [G loss: 0.879512] [AE loss: 1.170299]\n",
            "[Epoch 0/10] [Batch 1905/7121] [D loss: 0.759535] [G loss: 0.851236] [AE loss: 1.227954]\n",
            "[Epoch 0/10] [Batch 1906/7121] [D loss: 0.732332] [G loss: 0.842312] [AE loss: 1.214310]\n",
            "[Epoch 0/10] [Batch 1907/7121] [D loss: 0.819786] [G loss: 0.793771] [AE loss: 1.544805]\n",
            "[Epoch 0/10] [Batch 1908/7121] [D loss: 0.636002] [G loss: 0.806152] [AE loss: 3.076736]\n",
            "[Epoch 0/10] [Batch 1909/7121] [D loss: 0.889051] [G loss: 0.778599] [AE loss: 1.588148]\n",
            "[Epoch 0/10] [Batch 1910/7121] [D loss: 0.771230] [G loss: 0.752898] [AE loss: 2.394142]\n",
            "[Epoch 0/10] [Batch 1911/7121] [D loss: 0.837167] [G loss: 0.772160] [AE loss: 1.156933]\n",
            "[Epoch 0/10] [Batch 1912/7121] [D loss: 0.693235] [G loss: 0.778790] [AE loss: 1.372224]\n",
            "[Epoch 0/10] [Batch 1913/7121] [D loss: 0.695364] [G loss: 0.778968] [AE loss: 1.767452]\n",
            "[Epoch 0/10] [Batch 1914/7121] [D loss: 0.609767] [G loss: 0.846574] [AE loss: 1.317606]\n",
            "[Epoch 0/10] [Batch 1915/7121] [D loss: 0.583910] [G loss: 0.817646] [AE loss: 1.609595]\n",
            "[Epoch 0/10] [Batch 1916/7121] [D loss: 0.806123] [G loss: 0.797919] [AE loss: 1.655764]\n",
            "[Epoch 0/10] [Batch 1917/7121] [D loss: 0.707169] [G loss: 0.774408] [AE loss: 1.463246]\n",
            "[Epoch 0/10] [Batch 1918/7121] [D loss: 0.765964] [G loss: 0.729281] [AE loss: 1.175342]\n",
            "[Epoch 0/10] [Batch 1919/7121] [D loss: 0.762676] [G loss: 0.782222] [AE loss: 1.777652]\n",
            "[Epoch 0/10] [Batch 1920/7121] [D loss: 0.855253] [G loss: 0.754511] [AE loss: 2.470674]\n",
            "[Epoch 0/10] [Batch 1921/7121] [D loss: 0.841658] [G loss: 0.768525] [AE loss: 1.636318]\n",
            "[Epoch 0/10] [Batch 1922/7121] [D loss: 0.801520] [G loss: 0.739171] [AE loss: 1.529319]\n",
            "[Epoch 0/10] [Batch 1923/7121] [D loss: 0.598599] [G loss: 0.798950] [AE loss: 1.361924]\n",
            "[Epoch 0/10] [Batch 1924/7121] [D loss: 0.814237] [G loss: 0.774077] [AE loss: 1.307932]\n",
            "[Epoch 0/10] [Batch 1925/7121] [D loss: 1.173198] [G loss: 0.724155] [AE loss: 1.347121]\n",
            "[Epoch 0/10] [Batch 1926/7121] [D loss: 0.922120] [G loss: 0.760590] [AE loss: 1.317266]\n",
            "[Epoch 0/10] [Batch 1927/7121] [D loss: 0.751946] [G loss: 0.784382] [AE loss: 2.169678]\n",
            "[Epoch 0/10] [Batch 1928/7121] [D loss: 0.659171] [G loss: 0.849248] [AE loss: 1.455958]\n",
            "[Epoch 0/10] [Batch 1929/7121] [D loss: 0.815673] [G loss: 0.769344] [AE loss: 1.276296]\n",
            "[Epoch 0/10] [Batch 1930/7121] [D loss: 1.129670] [G loss: 0.781833] [AE loss: 1.933692]\n",
            "[Epoch 0/10] [Batch 1931/7121] [D loss: 0.819381] [G loss: 0.854379] [AE loss: 1.230729]\n",
            "[Epoch 0/10] [Batch 1932/7121] [D loss: 0.796514] [G loss: 0.865441] [AE loss: 1.348459]\n",
            "[Epoch 0/10] [Batch 1933/7121] [D loss: 0.934974] [G loss: 0.889518] [AE loss: 1.288979]\n",
            "[Epoch 0/10] [Batch 1934/7121] [D loss: 1.071568] [G loss: 0.977814] [AE loss: 1.319944]\n",
            "[Epoch 0/10] [Batch 1935/7121] [D loss: 0.793727] [G loss: 0.992114] [AE loss: 1.531014]\n",
            "[Epoch 0/10] [Batch 1936/7121] [D loss: 0.865302] [G loss: 1.039086] [AE loss: 2.109288]\n",
            "[Epoch 0/10] [Batch 1937/7121] [D loss: 0.945822] [G loss: 1.069640] [AE loss: 1.254164]\n",
            "[Epoch 0/10] [Batch 1938/7121] [D loss: 0.814275] [G loss: 1.096216] [AE loss: 1.293382]\n",
            "[Epoch 0/10] [Batch 1939/7121] [D loss: 0.709489] [G loss: 1.104815] [AE loss: 1.385965]\n",
            "[Epoch 0/10] [Batch 1940/7121] [D loss: 0.822578] [G loss: 1.068467] [AE loss: 1.448024]\n",
            "[Epoch 0/10] [Batch 1941/7121] [D loss: 0.745102] [G loss: 1.050626] [AE loss: 1.246700]\n",
            "[Epoch 0/10] [Batch 1942/7121] [D loss: 0.715479] [G loss: 1.053311] [AE loss: 1.240586]\n",
            "[Epoch 0/10] [Batch 1943/7121] [D loss: 0.805574] [G loss: 1.001309] [AE loss: 1.676204]\n",
            "[Epoch 0/10] [Batch 1944/7121] [D loss: 0.691328] [G loss: 0.968194] [AE loss: 1.564383]\n",
            "[Epoch 0/10] [Batch 1945/7121] [D loss: 0.652588] [G loss: 0.911335] [AE loss: 1.281147]\n",
            "[Epoch 0/10] [Batch 1946/7121] [D loss: 0.800712] [G loss: 0.888397] [AE loss: 1.195291]\n",
            "[Epoch 0/10] [Batch 1947/7121] [D loss: 0.728189] [G loss: 0.788927] [AE loss: 1.378126]\n",
            "[Epoch 0/10] [Batch 1948/7121] [D loss: 0.704488] [G loss: 0.852502] [AE loss: 1.532319]\n",
            "[Epoch 0/10] [Batch 1949/7121] [D loss: 0.766290] [G loss: 0.861546] [AE loss: 1.434664]\n",
            "[Epoch 0/10] [Batch 1950/7121] [D loss: 0.702389] [G loss: 0.858508] [AE loss: 1.246811]\n",
            "[Epoch 0/10] [Batch 1951/7121] [D loss: 0.717119] [G loss: 0.875807] [AE loss: 1.176821]\n",
            "[Epoch 0/10] [Batch 1952/7121] [D loss: 0.723067] [G loss: 0.973507] [AE loss: 1.375834]\n",
            "[Epoch 0/10] [Batch 1953/7121] [D loss: 0.658948] [G loss: 0.972885] [AE loss: 1.496087]\n",
            "[Epoch 0/10] [Batch 1954/7121] [D loss: 0.854113] [G loss: 0.991617] [AE loss: 1.344622]\n",
            "[Epoch 0/10] [Batch 1955/7121] [D loss: 0.646172] [G loss: 1.028803] [AE loss: 2.263196]\n",
            "[Epoch 0/10] [Batch 1956/7121] [D loss: 0.623922] [G loss: 1.006647] [AE loss: 1.626542]\n",
            "[Epoch 0/10] [Batch 1957/7121] [D loss: 0.729821] [G loss: 1.075101] [AE loss: 1.515908]\n",
            "[Epoch 0/10] [Batch 1958/7121] [D loss: 0.534646] [G loss: 1.031048] [AE loss: 1.173448]\n",
            "[Epoch 0/10] [Batch 1959/7121] [D loss: 0.571459] [G loss: 1.102817] [AE loss: 1.866711]\n",
            "[Epoch 0/10] [Batch 1960/7121] [D loss: 0.543218] [G loss: 1.077761] [AE loss: 1.773920]\n",
            "[Epoch 0/10] [Batch 1961/7121] [D loss: 0.512291] [G loss: 1.064038] [AE loss: 1.258908]\n",
            "[Epoch 0/10] [Batch 1962/7121] [D loss: 0.527779] [G loss: 0.966141] [AE loss: 1.153658]\n",
            "[Epoch 0/10] [Batch 1963/7121] [D loss: 0.498711] [G loss: 0.981347] [AE loss: 1.180285]\n",
            "[Epoch 0/10] [Batch 1964/7121] [D loss: 0.587239] [G loss: 0.893230] [AE loss: 1.336617]\n",
            "[Epoch 0/10] [Batch 1965/7121] [D loss: 0.465641] [G loss: 0.808885] [AE loss: 1.475623]\n",
            "[Epoch 0/10] [Batch 1966/7121] [D loss: 0.691955] [G loss: 0.809069] [AE loss: 1.469744]\n",
            "[Epoch 0/10] [Batch 1967/7121] [D loss: 0.558484] [G loss: 0.767049] [AE loss: 1.247414]\n",
            "[Epoch 0/10] [Batch 1968/7121] [D loss: 0.557119] [G loss: 0.682981] [AE loss: 1.399465]\n",
            "[Epoch 0/10] [Batch 1969/7121] [D loss: 0.713636] [G loss: 0.623431] [AE loss: 1.017781]\n",
            "[Epoch 0/10] [Batch 1970/7121] [D loss: 0.607895] [G loss: 0.568122] [AE loss: 1.275267]\n",
            "[Epoch 0/10] [Batch 1971/7121] [D loss: 0.627643] [G loss: 0.575390] [AE loss: 2.759185]\n",
            "[Epoch 0/10] [Batch 1972/7121] [D loss: 0.720281] [G loss: 0.495638] [AE loss: 1.640148]\n",
            "[Epoch 0/10] [Batch 1973/7121] [D loss: 0.773416] [G loss: 0.429223] [AE loss: 1.187326]\n",
            "[Epoch 0/10] [Batch 1974/7121] [D loss: 0.896121] [G loss: 0.457910] [AE loss: 1.165021]\n",
            "[Epoch 0/10] [Batch 1975/7121] [D loss: 0.799880] [G loss: 0.438100] [AE loss: 1.053214]\n",
            "[Epoch 0/10] [Batch 1976/7121] [D loss: 0.822000] [G loss: 0.436671] [AE loss: 1.078923]\n",
            "[Epoch 0/10] [Batch 1977/7121] [D loss: 0.848533] [G loss: 0.429079] [AE loss: 1.057249]\n",
            "[Epoch 0/10] [Batch 1978/7121] [D loss: 0.866426] [G loss: 0.443406] [AE loss: 1.323035]\n",
            "[Epoch 0/10] [Batch 1979/7121] [D loss: 0.774196] [G loss: 0.519504] [AE loss: 1.896626]\n",
            "[Epoch 0/10] [Batch 1980/7121] [D loss: 0.760584] [G loss: 0.515325] [AE loss: 1.060063]\n",
            "[Epoch 0/10] [Batch 1981/7121] [D loss: 0.840935] [G loss: 0.504667] [AE loss: 1.193167]\n",
            "[Epoch 0/10] [Batch 1982/7121] [D loss: 0.792993] [G loss: 0.520533] [AE loss: 1.166862]\n",
            "[Epoch 0/10] [Batch 1983/7121] [D loss: 0.624068] [G loss: 0.541215] [AE loss: 5.908608]\n",
            "[Epoch 0/10] [Batch 1984/7121] [D loss: 0.919068] [G loss: 0.464413] [AE loss: 1.173766]\n",
            "[Epoch 0/10] [Batch 1985/7121] [D loss: 0.844100] [G loss: 0.490829] [AE loss: 1.729065]\n",
            "[Epoch 0/10] [Batch 1986/7121] [D loss: 0.810824] [G loss: 0.518954] [AE loss: 1.153905]\n",
            "[Epoch 0/10] [Batch 1987/7121] [D loss: 0.873500] [G loss: 0.516449] [AE loss: 1.447238]\n",
            "[Epoch 0/10] [Batch 1988/7121] [D loss: 1.013579] [G loss: 0.495673] [AE loss: 1.192978]\n",
            "[Epoch 0/10] [Batch 1989/7121] [D loss: 0.814826] [G loss: 0.554934] [AE loss: 1.043262]\n",
            "[Epoch 0/10] [Batch 1990/7121] [D loss: 0.811199] [G loss: 0.585178] [AE loss: 1.323820]\n",
            "[Epoch 0/10] [Batch 1991/7121] [D loss: 0.839396] [G loss: 0.608203] [AE loss: 1.060158]\n",
            "[Epoch 0/10] [Batch 1992/7121] [D loss: 0.908676] [G loss: 0.561875] [AE loss: 2.544997]\n",
            "[Epoch 0/10] [Batch 1993/7121] [D loss: 0.865034] [G loss: 0.660169] [AE loss: 1.130176]\n",
            "[Epoch 0/10] [Batch 1994/7121] [D loss: 0.897084] [G loss: 0.693700] [AE loss: 1.282219]\n",
            "[Epoch 0/10] [Batch 1995/7121] [D loss: 0.693392] [G loss: 0.710417] [AE loss: 1.208958]\n",
            "[Epoch 0/10] [Batch 1996/7121] [D loss: 0.920416] [G loss: 0.734368] [AE loss: 1.178989]\n",
            "[Epoch 0/10] [Batch 1997/7121] [D loss: 0.879177] [G loss: 0.777145] [AE loss: 1.183103]\n",
            "[Epoch 0/10] [Batch 1998/7121] [D loss: 0.882110] [G loss: 0.742744] [AE loss: 0.965880]\n",
            "[Epoch 0/10] [Batch 1999/7121] [D loss: 0.891147] [G loss: 0.793061] [AE loss: 1.491801]\n",
            "[Epoch 0/10] [Batch 2000/7121] [D loss: 0.821341] [G loss: 0.852013] [AE loss: 1.044421]\n",
            "[Epoch 0/10] [Batch 2001/7121] [D loss: 0.812591] [G loss: 0.876641] [AE loss: 1.444853]\n",
            "[Epoch 0/10] [Batch 2002/7121] [D loss: 0.898914] [G loss: 0.825578] [AE loss: 1.041507]\n",
            "[Epoch 0/10] [Batch 2003/7121] [D loss: 0.793031] [G loss: 0.866347] [AE loss: 1.011235]\n",
            "[Epoch 0/10] [Batch 2004/7121] [D loss: 0.823303] [G loss: 0.898630] [AE loss: 0.986070]\n",
            "[Epoch 0/10] [Batch 2005/7121] [D loss: 0.790326] [G loss: 0.872160] [AE loss: 1.411668]\n",
            "[Epoch 0/10] [Batch 2006/7121] [D loss: 0.689285] [G loss: 0.880489] [AE loss: 1.246570]\n",
            "[Epoch 0/10] [Batch 2007/7121] [D loss: 0.759912] [G loss: 0.894797] [AE loss: 1.901379]\n",
            "[Epoch 0/10] [Batch 2008/7121] [D loss: 0.815485] [G loss: 0.822335] [AE loss: 1.200339]\n",
            "[Epoch 0/10] [Batch 2009/7121] [D loss: 0.828223] [G loss: 0.831661] [AE loss: 0.961655]\n",
            "[Epoch 0/10] [Batch 2010/7121] [D loss: 0.690888] [G loss: 0.872377] [AE loss: 0.838667]\n",
            "[Epoch 0/10] [Batch 2011/7121] [D loss: 0.740252] [G loss: 0.791675] [AE loss: 1.526021]\n",
            "[Epoch 0/10] [Batch 2012/7121] [D loss: 0.715598] [G loss: 0.800746] [AE loss: 1.227113]\n",
            "[Epoch 0/10] [Batch 2013/7121] [D loss: 0.871653] [G loss: 0.821439] [AE loss: 1.208167]\n",
            "[Epoch 0/10] [Batch 2014/7121] [D loss: 0.678052] [G loss: 0.813631] [AE loss: 1.044933]\n",
            "[Epoch 0/10] [Batch 2015/7121] [D loss: 0.608213] [G loss: 0.835086] [AE loss: 2.421353]\n",
            "[Epoch 0/10] [Batch 2016/7121] [D loss: 0.658851] [G loss: 0.804920] [AE loss: 2.935905]\n",
            "[Epoch 0/10] [Batch 2017/7121] [D loss: 0.700134] [G loss: 0.822372] [AE loss: 1.094710]\n",
            "[Epoch 0/10] [Batch 2018/7121] [D loss: 0.595484] [G loss: 0.837027] [AE loss: 1.387258]\n",
            "[Epoch 0/10] [Batch 2019/7121] [D loss: 0.623130] [G loss: 0.846319] [AE loss: 1.130332]\n",
            "[Epoch 0/10] [Batch 2020/7121] [D loss: 0.707077] [G loss: 0.876397] [AE loss: 0.894717]\n",
            "[Epoch 0/10] [Batch 2021/7121] [D loss: 0.596167] [G loss: 0.887779] [AE loss: 1.210524]\n",
            "[Epoch 0/10] [Batch 2022/7121] [D loss: 0.760858] [G loss: 0.895732] [AE loss: 0.915026]\n",
            "[Epoch 0/10] [Batch 2023/7121] [D loss: 0.587640] [G loss: 0.945769] [AE loss: 1.132236]\n",
            "[Epoch 0/10] [Batch 2024/7121] [D loss: 0.580508] [G loss: 0.977342] [AE loss: 1.065345]\n",
            "[Epoch 0/10] [Batch 2025/7121] [D loss: 0.558103] [G loss: 1.016923] [AE loss: 1.162551]\n",
            "[Epoch 0/10] [Batch 2026/7121] [D loss: 0.582282] [G loss: 1.025113] [AE loss: 1.305708]\n",
            "[Epoch 0/10] [Batch 2027/7121] [D loss: 0.521638] [G loss: 1.038857] [AE loss: 1.368903]\n",
            "[Epoch 0/10] [Batch 2028/7121] [D loss: 0.521130] [G loss: 1.068424] [AE loss: 0.862836]\n",
            "[Epoch 0/10] [Batch 2029/7121] [D loss: 0.404685] [G loss: 1.088068] [AE loss: 1.541180]\n",
            "[Epoch 0/10] [Batch 2030/7121] [D loss: 0.477735] [G loss: 1.078559] [AE loss: 1.048109]\n",
            "[Epoch 0/10] [Batch 2031/7121] [D loss: 0.476329] [G loss: 1.103685] [AE loss: 1.174526]\n",
            "[Epoch 0/10] [Batch 2032/7121] [D loss: 0.493787] [G loss: 1.103206] [AE loss: 1.014095]\n",
            "[Epoch 0/10] [Batch 2033/7121] [D loss: 0.529304] [G loss: 1.116021] [AE loss: 1.047213]\n",
            "[Epoch 0/10] [Batch 2034/7121] [D loss: 0.483049] [G loss: 1.113699] [AE loss: 0.944620]\n",
            "[Epoch 0/10] [Batch 2035/7121] [D loss: 0.435714] [G loss: 1.102679] [AE loss: 0.926619]\n",
            "[Epoch 0/10] [Batch 2036/7121] [D loss: 0.473862] [G loss: 1.103107] [AE loss: 1.878715]\n",
            "[Epoch 0/10] [Batch 2037/7121] [D loss: 0.532897] [G loss: 1.078317] [AE loss: 1.044878]\n",
            "[Epoch 0/10] [Batch 2038/7121] [D loss: 0.414950] [G loss: 1.074731] [AE loss: 1.080248]\n",
            "[Epoch 0/10] [Batch 2039/7121] [D loss: 0.439682] [G loss: 1.022861] [AE loss: 1.923062]\n",
            "[Epoch 0/10] [Batch 2040/7121] [D loss: 0.462164] [G loss: 1.028923] [AE loss: 1.573919]\n",
            "[Epoch 0/10] [Batch 2041/7121] [D loss: 0.419289] [G loss: 0.990319] [AE loss: 1.016688]\n",
            "[Epoch 0/10] [Batch 2042/7121] [D loss: 0.459520] [G loss: 0.974172] [AE loss: 0.916333]\n",
            "[Epoch 0/10] [Batch 2043/7121] [D loss: 0.385062] [G loss: 0.957958] [AE loss: 1.625686]\n",
            "[Epoch 0/10] [Batch 2044/7121] [D loss: 0.455666] [G loss: 0.928452] [AE loss: 1.202814]\n",
            "[Epoch 0/10] [Batch 2045/7121] [D loss: 0.519969] [G loss: 0.914797] [AE loss: 1.054382]\n",
            "[Epoch 0/10] [Batch 2046/7121] [D loss: 0.561871] [G loss: 0.879040] [AE loss: 0.842434]\n",
            "[Epoch 0/10] [Batch 2047/7121] [D loss: 0.479427] [G loss: 0.862234] [AE loss: 3.512649]\n",
            "[Epoch 0/10] [Batch 2048/7121] [D loss: 0.522187] [G loss: 0.862890] [AE loss: 0.862914]\n",
            "[Epoch 0/10] [Batch 2049/7121] [D loss: 0.510658] [G loss: 0.830901] [AE loss: 0.942788]\n",
            "[Epoch 0/10] [Batch 2050/7121] [D loss: 0.505290] [G loss: 0.847707] [AE loss: 1.763729]\n",
            "[Epoch 0/10] [Batch 2051/7121] [D loss: 0.470653] [G loss: 0.822271] [AE loss: 1.149392]\n",
            "[Epoch 0/10] [Batch 2052/7121] [D loss: 0.542899] [G loss: 0.805030] [AE loss: 1.807783]\n",
            "[Epoch 0/10] [Batch 2053/7121] [D loss: 0.458427] [G loss: 0.790888] [AE loss: 0.956845]\n",
            "[Epoch 0/10] [Batch 2054/7121] [D loss: 0.525292] [G loss: 0.801665] [AE loss: 1.347631]\n",
            "[Epoch 0/10] [Batch 2055/7121] [D loss: 0.426398] [G loss: 0.816226] [AE loss: 1.002157]\n",
            "[Epoch 0/10] [Batch 2056/7121] [D loss: 0.518064] [G loss: 0.799501] [AE loss: 1.396397]\n",
            "[Epoch 0/10] [Batch 2057/7121] [D loss: 0.537947] [G loss: 0.786128] [AE loss: 3.694600]\n",
            "[Epoch 0/10] [Batch 2058/7121] [D loss: 0.500747] [G loss: 0.786271] [AE loss: 1.106473]\n",
            "[Epoch 0/10] [Batch 2059/7121] [D loss: 0.506834] [G loss: 0.780319] [AE loss: 1.109277]\n",
            "[Epoch 0/10] [Batch 2060/7121] [D loss: 0.501876] [G loss: 0.751518] [AE loss: 1.165260]\n",
            "[Epoch 0/10] [Batch 2061/7121] [D loss: 0.493156] [G loss: 0.744102] [AE loss: 1.290850]\n",
            "[Epoch 0/10] [Batch 2062/7121] [D loss: 0.566993] [G loss: 0.718806] [AE loss: 1.028147]\n",
            "[Epoch 0/10] [Batch 2063/7121] [D loss: 0.584577] [G loss: 0.709773] [AE loss: 1.045485]\n",
            "[Epoch 0/10] [Batch 2064/7121] [D loss: 0.597222] [G loss: 0.664717] [AE loss: 1.112352]\n",
            "[Epoch 0/10] [Batch 2065/7121] [D loss: 0.512644] [G loss: 0.686715] [AE loss: 1.120267]\n",
            "[Epoch 0/10] [Batch 2066/7121] [D loss: 0.614445] [G loss: 0.653870] [AE loss: 0.860287]\n",
            "[Epoch 0/10] [Batch 2067/7121] [D loss: 0.584256] [G loss: 0.628658] [AE loss: 0.975519]\n",
            "[Epoch 0/10] [Batch 2068/7121] [D loss: 0.594861] [G loss: 0.607910] [AE loss: 1.596016]\n",
            "[Epoch 0/10] [Batch 2069/7121] [D loss: 0.617655] [G loss: 0.623459] [AE loss: 1.116465]\n",
            "[Epoch 0/10] [Batch 2070/7121] [D loss: 0.637100] [G loss: 0.567004] [AE loss: 6.724738]\n",
            "[Epoch 0/10] [Batch 2071/7121] [D loss: 0.716291] [G loss: 0.570607] [AE loss: 0.844831]\n",
            "[Epoch 0/10] [Batch 2072/7121] [D loss: 0.645168] [G loss: 0.581762] [AE loss: 0.912921]\n",
            "[Epoch 0/10] [Batch 2073/7121] [D loss: 0.689217] [G loss: 0.605590] [AE loss: 1.021850]\n",
            "[Epoch 0/10] [Batch 2074/7121] [D loss: 0.605166] [G loss: 0.644362] [AE loss: 0.920707]\n",
            "[Epoch 0/10] [Batch 2075/7121] [D loss: 0.653542] [G loss: 0.635909] [AE loss: 1.502780]\n",
            "[Epoch 0/10] [Batch 2076/7121] [D loss: 0.551155] [G loss: 0.700581] [AE loss: 1.065617]\n",
            "[Epoch 0/10] [Batch 2077/7121] [D loss: 0.657366] [G loss: 0.703197] [AE loss: 1.216255]\n",
            "[Epoch 0/10] [Batch 2078/7121] [D loss: 0.576411] [G loss: 0.741634] [AE loss: 1.390374]\n",
            "[Epoch 0/10] [Batch 2079/7121] [D loss: 0.651471] [G loss: 0.773106] [AE loss: 1.029547]\n",
            "[Epoch 0/10] [Batch 2080/7121] [D loss: 0.613656] [G loss: 0.810495] [AE loss: 1.326735]\n",
            "[Epoch 0/10] [Batch 2081/7121] [D loss: 0.600657] [G loss: 0.896184] [AE loss: 0.959314]\n",
            "[Epoch 0/10] [Batch 2082/7121] [D loss: 0.560605] [G loss: 0.947631] [AE loss: 2.279808]\n",
            "[Epoch 0/10] [Batch 2083/7121] [D loss: 0.576268] [G loss: 1.021280] [AE loss: 0.956991]\n",
            "[Epoch 0/10] [Batch 2084/7121] [D loss: 0.614033] [G loss: 1.050488] [AE loss: 1.117954]\n",
            "[Epoch 0/10] [Batch 2085/7121] [D loss: 0.502807] [G loss: 1.101787] [AE loss: 1.182719]\n",
            "[Epoch 0/10] [Batch 2086/7121] [D loss: 0.581953] [G loss: 1.155935] [AE loss: 1.310372]\n",
            "[Epoch 0/10] [Batch 2087/7121] [D loss: 0.544397] [G loss: 1.205753] [AE loss: 2.479699]\n",
            "[Epoch 0/10] [Batch 2088/7121] [D loss: 0.474898] [G loss: 1.209238] [AE loss: 1.756223]\n",
            "[Epoch 0/10] [Batch 2089/7121] [D loss: 0.559648] [G loss: 1.247882] [AE loss: 1.036120]\n",
            "[Epoch 0/10] [Batch 2090/7121] [D loss: 0.587653] [G loss: 1.268617] [AE loss: 1.010075]\n",
            "[Epoch 0/10] [Batch 2091/7121] [D loss: 0.501728] [G loss: 1.264634] [AE loss: 1.525072]\n",
            "[Epoch 0/10] [Batch 2092/7121] [D loss: 0.436599] [G loss: 1.293291] [AE loss: 1.724048]\n",
            "[Epoch 0/10] [Batch 2093/7121] [D loss: 0.547556] [G loss: 1.286594] [AE loss: 1.213134]\n",
            "[Epoch 0/10] [Batch 2094/7121] [D loss: 0.520902] [G loss: 1.275661] [AE loss: 1.056599]\n",
            "[Epoch 0/10] [Batch 2095/7121] [D loss: 0.467882] [G loss: 1.262709] [AE loss: 1.410671]\n",
            "[Epoch 0/10] [Batch 2096/7121] [D loss: 0.385758] [G loss: 1.261776] [AE loss: 2.739383]\n",
            "[Epoch 0/10] [Batch 2097/7121] [D loss: 0.419861] [G loss: 1.244450] [AE loss: 1.013532]\n",
            "[Epoch 0/10] [Batch 2098/7121] [D loss: 0.467707] [G loss: 1.202054] [AE loss: 1.252634]\n",
            "[Epoch 0/10] [Batch 2099/7121] [D loss: 0.459130] [G loss: 1.193317] [AE loss: 1.466603]\n",
            "[Epoch 0/10] [Batch 2100/7121] [D loss: 0.458425] [G loss: 1.153223] [AE loss: 1.293187]\n",
            "[Epoch 0/10] [Batch 2101/7121] [D loss: 0.446402] [G loss: 1.144636] [AE loss: 1.189027]\n",
            "[Epoch 0/10] [Batch 2102/7121] [D loss: 0.401926] [G loss: 1.117806] [AE loss: 1.380629]\n",
            "[Epoch 0/10] [Batch 2103/7121] [D loss: 0.467084] [G loss: 1.098083] [AE loss: 1.299801]\n",
            "[Epoch 0/10] [Batch 2104/7121] [D loss: 0.479419] [G loss: 1.088620] [AE loss: 1.017470]\n",
            "[Epoch 0/10] [Batch 2105/7121] [D loss: 0.392383] [G loss: 1.072026] [AE loss: 1.193135]\n",
            "[Epoch 0/10] [Batch 2106/7121] [D loss: 0.433022] [G loss: 1.023743] [AE loss: 1.120150]\n",
            "[Epoch 0/10] [Batch 2107/7121] [D loss: 0.439469] [G loss: 0.990794] [AE loss: 0.978739]\n",
            "[Epoch 0/10] [Batch 2108/7121] [D loss: 0.489859] [G loss: 0.948098] [AE loss: 3.957405]\n",
            "[Epoch 0/10] [Batch 2109/7121] [D loss: 0.421697] [G loss: 0.952281] [AE loss: 0.965834]\n",
            "[Epoch 0/10] [Batch 2110/7121] [D loss: 0.462583] [G loss: 0.879483] [AE loss: 0.998210]\n",
            "[Epoch 0/10] [Batch 2111/7121] [D loss: 0.443793] [G loss: 0.886344] [AE loss: 1.137795]\n",
            "[Epoch 0/10] [Batch 2112/7121] [D loss: 0.418199] [G loss: 0.827661] [AE loss: 0.970823]\n",
            "[Epoch 0/10] [Batch 2113/7121] [D loss: 0.433068] [G loss: 0.807297] [AE loss: 1.674523]\n",
            "[Epoch 0/10] [Batch 2114/7121] [D loss: 0.509157] [G loss: 0.757785] [AE loss: 1.499858]\n",
            "[Epoch 0/10] [Batch 2115/7121] [D loss: 0.508384] [G loss: 0.749350] [AE loss: 1.098147]\n",
            "[Epoch 0/10] [Batch 2116/7121] [D loss: 0.462682] [G loss: 0.749347] [AE loss: 1.245309]\n",
            "[Epoch 0/10] [Batch 2117/7121] [D loss: 0.449788] [G loss: 0.765410] [AE loss: 1.145663]\n",
            "[Epoch 0/10] [Batch 2118/7121] [D loss: 0.489230] [G loss: 0.753745] [AE loss: 1.128960]\n",
            "[Epoch 0/10] [Batch 2119/7121] [D loss: 0.479089] [G loss: 0.743735] [AE loss: 1.260671]\n",
            "[Epoch 0/10] [Batch 2120/7121] [D loss: 0.475844] [G loss: 0.753679] [AE loss: 1.235277]\n",
            "[Epoch 0/10] [Batch 2121/7121] [D loss: 0.477339] [G loss: 0.812757] [AE loss: 1.035220]\n",
            "[Epoch 0/10] [Batch 2122/7121] [D loss: 0.423020] [G loss: 0.828002] [AE loss: 1.024163]\n",
            "[Epoch 0/10] [Batch 2123/7121] [D loss: 0.440301] [G loss: 0.863344] [AE loss: 1.024485]\n",
            "[Epoch 0/10] [Batch 2124/7121] [D loss: 0.415345] [G loss: 0.895277] [AE loss: 1.229995]\n",
            "[Epoch 0/10] [Batch 2125/7121] [D loss: 0.417991] [G loss: 0.877597] [AE loss: 1.449041]\n",
            "[Epoch 0/10] [Batch 2126/7121] [D loss: 0.375380] [G loss: 0.934648] [AE loss: 1.192143]\n",
            "[Epoch 0/10] [Batch 2127/7121] [D loss: 0.356604] [G loss: 1.018206] [AE loss: 0.960718]\n",
            "[Epoch 0/10] [Batch 2128/7121] [D loss: 0.331457] [G loss: 0.980575] [AE loss: 1.519432]\n",
            "[Epoch 0/10] [Batch 2129/7121] [D loss: 0.344259] [G loss: 1.044941] [AE loss: 0.987825]\n",
            "[Epoch 0/10] [Batch 2130/7121] [D loss: 0.360919] [G loss: 1.066989] [AE loss: 0.910586]\n",
            "[Epoch 0/10] [Batch 2131/7121] [D loss: 0.359703] [G loss: 1.049821] [AE loss: 0.985808]\n",
            "[Epoch 0/10] [Batch 2132/7121] [D loss: 0.408015] [G loss: 1.038789] [AE loss: 0.937102]\n",
            "[Epoch 0/10] [Batch 2133/7121] [D loss: 0.285777] [G loss: 1.049244] [AE loss: 1.399166]\n",
            "[Epoch 0/10] [Batch 2134/7121] [D loss: 0.380360] [G loss: 0.982378] [AE loss: 1.077811]\n",
            "[Epoch 0/10] [Batch 2135/7121] [D loss: 0.423089] [G loss: 0.917004] [AE loss: 0.863668]\n",
            "[Epoch 0/10] [Batch 2136/7121] [D loss: 0.409065] [G loss: 0.848335] [AE loss: 1.031339]\n",
            "[Epoch 0/10] [Batch 2137/7121] [D loss: 0.452657] [G loss: 0.731614] [AE loss: 1.158277]\n",
            "[Epoch 0/10] [Batch 2138/7121] [D loss: 0.495177] [G loss: 0.618414] [AE loss: 1.599615]\n",
            "[Epoch 0/10] [Batch 2139/7121] [D loss: 0.596915] [G loss: 0.507290] [AE loss: 1.070210]\n",
            "[Epoch 0/10] [Batch 2140/7121] [D loss: 0.638312] [G loss: 0.485030] [AE loss: 1.024277]\n",
            "[Epoch 0/10] [Batch 2141/7121] [D loss: 0.676373] [G loss: 0.412888] [AE loss: 1.227477]\n",
            "[Epoch 0/10] [Batch 2142/7121] [D loss: 0.663305] [G loss: 0.403507] [AE loss: 2.625715]\n",
            "[Epoch 0/10] [Batch 2143/7121] [D loss: 0.696426] [G loss: 0.465879] [AE loss: 1.101543]\n",
            "[Epoch 0/10] [Batch 2144/7121] [D loss: 0.647251] [G loss: 0.486558] [AE loss: 1.409416]\n",
            "[Epoch 0/10] [Batch 2145/7121] [D loss: 0.588362] [G loss: 0.539392] [AE loss: 1.480695]\n",
            "[Epoch 0/10] [Batch 2146/7121] [D loss: 0.674948] [G loss: 0.557049] [AE loss: 1.054841]\n",
            "[Epoch 0/10] [Batch 2147/7121] [D loss: 0.583053] [G loss: 0.654221] [AE loss: 1.076428]\n",
            "[Epoch 0/10] [Batch 2148/7121] [D loss: 0.569305] [G loss: 0.718550] [AE loss: 1.114815]\n",
            "[Epoch 0/10] [Batch 2149/7121] [D loss: 0.454429] [G loss: 0.782824] [AE loss: 1.075823]\n",
            "[Epoch 0/10] [Batch 2150/7121] [D loss: 0.478717] [G loss: 0.882382] [AE loss: 1.187730]\n",
            "[Epoch 0/10] [Batch 2151/7121] [D loss: 0.417734] [G loss: 0.943967] [AE loss: 1.297738]\n",
            "[Epoch 0/10] [Batch 2152/7121] [D loss: 0.415125] [G loss: 1.008399] [AE loss: 2.034870]\n",
            "[Epoch 0/10] [Batch 2153/7121] [D loss: 0.388313] [G loss: 1.020187] [AE loss: 1.769252]\n",
            "[Epoch 0/10] [Batch 2154/7121] [D loss: 0.551952] [G loss: 1.057497] [AE loss: 1.129283]\n",
            "[Epoch 0/10] [Batch 2155/7121] [D loss: 0.452029] [G loss: 1.095735] [AE loss: 1.353863]\n",
            "[Epoch 0/10] [Batch 2156/7121] [D loss: 0.391093] [G loss: 1.106371] [AE loss: 3.358411]\n",
            "[Epoch 0/10] [Batch 2157/7121] [D loss: 0.527671] [G loss: 1.102407] [AE loss: 1.219151]\n",
            "[Epoch 0/10] [Batch 2158/7121] [D loss: 0.508076] [G loss: 1.130177] [AE loss: 1.890878]\n",
            "[Epoch 0/10] [Batch 2159/7121] [D loss: 0.459148] [G loss: 1.135679] [AE loss: 1.289888]\n",
            "[Epoch 0/10] [Batch 2160/7121] [D loss: 0.403355] [G loss: 1.140989] [AE loss: 1.249853]\n",
            "[Epoch 0/10] [Batch 2161/7121] [D loss: 0.563131] [G loss: 1.104953] [AE loss: 4.381698]\n",
            "[Epoch 0/10] [Batch 2162/7121] [D loss: 0.490984] [G loss: 1.080455] [AE loss: 2.111129]\n",
            "[Epoch 0/10] [Batch 2163/7121] [D loss: 0.624810] [G loss: 1.098311] [AE loss: 1.015186]\n",
            "[Epoch 0/10] [Batch 2164/7121] [D loss: 0.403221] [G loss: 1.015444] [AE loss: 1.374918]\n",
            "[Epoch 0/10] [Batch 2165/7121] [D loss: 0.505044] [G loss: 1.066929] [AE loss: 1.502688]\n",
            "[Epoch 0/10] [Batch 2166/7121] [D loss: 0.530812] [G loss: 1.014742] [AE loss: 1.156134]\n",
            "[Epoch 0/10] [Batch 2167/7121] [D loss: 0.500104] [G loss: 1.011447] [AE loss: 1.535404]\n",
            "[Epoch 0/10] [Batch 2168/7121] [D loss: 0.523098] [G loss: 1.020525] [AE loss: 1.225973]\n",
            "[Epoch 0/10] [Batch 2169/7121] [D loss: 0.385691] [G loss: 1.064572] [AE loss: 1.652213]\n",
            "[Epoch 0/10] [Batch 2170/7121] [D loss: 0.523779] [G loss: 1.043399] [AE loss: 1.855957]\n",
            "[Epoch 0/10] [Batch 2171/7121] [D loss: 0.500356] [G loss: 1.116805] [AE loss: 1.308733]\n",
            "[Epoch 0/10] [Batch 2172/7121] [D loss: 0.572300] [G loss: 1.026150] [AE loss: 1.444762]\n",
            "[Epoch 0/10] [Batch 2173/7121] [D loss: 0.455445] [G loss: 1.048429] [AE loss: 1.416215]\n",
            "[Epoch 0/10] [Batch 2174/7121] [D loss: 0.567914] [G loss: 1.081189] [AE loss: 1.715797]\n",
            "[Epoch 0/10] [Batch 2175/7121] [D loss: 0.626273] [G loss: 1.092815] [AE loss: 1.392612]\n",
            "[Epoch 0/10] [Batch 2176/7121] [D loss: 0.436898] [G loss: 1.082127] [AE loss: 1.289804]\n",
            "[Epoch 0/10] [Batch 2177/7121] [D loss: 0.552645] [G loss: 1.076275] [AE loss: 1.446432]\n",
            "[Epoch 0/10] [Batch 2178/7121] [D loss: 0.570962] [G loss: 1.103961] [AE loss: 1.399922]\n",
            "[Epoch 0/10] [Batch 2179/7121] [D loss: 0.600609] [G loss: 1.046982] [AE loss: 1.196784]\n",
            "[Epoch 0/10] [Batch 2180/7121] [D loss: 0.636921] [G loss: 1.043804] [AE loss: 1.513304]\n",
            "[Epoch 0/10] [Batch 2181/7121] [D loss: 0.502079] [G loss: 1.022145] [AE loss: 7.006490]\n",
            "[Epoch 0/10] [Batch 2182/7121] [D loss: 0.661659] [G loss: 1.018849] [AE loss: 1.376457]\n",
            "[Epoch 0/10] [Batch 2183/7121] [D loss: 0.699552] [G loss: 1.004264] [AE loss: 1.508203]\n",
            "[Epoch 0/10] [Batch 2184/7121] [D loss: 0.485045] [G loss: 0.961081] [AE loss: 2.013002]\n",
            "[Epoch 0/10] [Batch 2185/7121] [D loss: 0.565448] [G loss: 0.949043] [AE loss: 1.506068]\n",
            "[Epoch 0/10] [Batch 2186/7121] [D loss: 0.579803] [G loss: 0.943794] [AE loss: 2.055025]\n",
            "[Epoch 0/10] [Batch 2187/7121] [D loss: 0.820924] [G loss: 0.833452] [AE loss: 1.475683]\n",
            "[Epoch 0/10] [Batch 2188/7121] [D loss: 0.675820] [G loss: 0.728305] [AE loss: 1.381648]\n",
            "[Epoch 0/10] [Batch 2189/7121] [D loss: 0.642520] [G loss: 0.766701] [AE loss: 1.918788]\n",
            "[Epoch 0/10] [Batch 2190/7121] [D loss: 0.628158] [G loss: 0.719230] [AE loss: 1.225352]\n",
            "[Epoch 0/10] [Batch 2191/7121] [D loss: 0.773353] [G loss: 0.721460] [AE loss: 1.297742]\n",
            "[Epoch 0/10] [Batch 2192/7121] [D loss: 0.712200] [G loss: 0.690680] [AE loss: 1.500858]\n",
            "[Epoch 0/10] [Batch 2193/7121] [D loss: 0.746946] [G loss: 0.708464] [AE loss: 1.283162]\n",
            "[Epoch 0/10] [Batch 2194/7121] [D loss: 0.716886] [G loss: 0.709453] [AE loss: 1.553916]\n",
            "[Epoch 0/10] [Batch 2195/7121] [D loss: 0.694428] [G loss: 0.735201] [AE loss: 1.505101]\n",
            "[Epoch 0/10] [Batch 2196/7121] [D loss: 0.789785] [G loss: 0.705293] [AE loss: 1.790710]\n",
            "[Epoch 0/10] [Batch 2197/7121] [D loss: 0.684914] [G loss: 0.765981] [AE loss: 1.423617]\n",
            "[Epoch 0/10] [Batch 2198/7121] [D loss: 0.813300] [G loss: 0.764261] [AE loss: 1.601576]\n",
            "[Epoch 0/10] [Batch 2199/7121] [D loss: 0.729576] [G loss: 0.855822] [AE loss: 4.253708]\n",
            "[Epoch 0/10] [Batch 2200/7121] [D loss: 0.796608] [G loss: 0.909608] [AE loss: 1.294096]\n",
            "[Epoch 0/10] [Batch 2201/7121] [D loss: 0.696631] [G loss: 0.942154] [AE loss: 1.317437]\n",
            "[Epoch 0/10] [Batch 2202/7121] [D loss: 0.673150] [G loss: 0.971721] [AE loss: 1.310966]\n",
            "[Epoch 0/10] [Batch 2203/7121] [D loss: 0.675472] [G loss: 0.982837] [AE loss: 1.149124]\n",
            "[Epoch 0/10] [Batch 2204/7121] [D loss: 0.653077] [G loss: 1.015557] [AE loss: 1.275584]\n",
            "[Epoch 0/10] [Batch 2205/7121] [D loss: 0.677625] [G loss: 1.005213] [AE loss: 1.447691]\n",
            "[Epoch 0/10] [Batch 2206/7121] [D loss: 0.518439] [G loss: 0.990235] [AE loss: 1.209412]\n",
            "[Epoch 0/10] [Batch 2207/7121] [D loss: 0.637947] [G loss: 0.997219] [AE loss: 1.207048]\n",
            "[Epoch 0/10] [Batch 2208/7121] [D loss: 0.622609] [G loss: 0.934078] [AE loss: 1.272804]\n",
            "[Epoch 0/10] [Batch 2209/7121] [D loss: 0.591146] [G loss: 0.915359] [AE loss: 2.390694]\n",
            "[Epoch 0/10] [Batch 2210/7121] [D loss: 0.717347] [G loss: 0.839557] [AE loss: 1.161876]\n",
            "[Epoch 0/10] [Batch 2211/7121] [D loss: 0.703482] [G loss: 0.839069] [AE loss: 1.415767]\n",
            "[Epoch 0/10] [Batch 2212/7121] [D loss: 0.486873] [G loss: 0.810785] [AE loss: 1.944490]\n",
            "[Epoch 0/10] [Batch 2213/7121] [D loss: 0.517086] [G loss: 0.741510] [AE loss: 1.355324]\n",
            "[Epoch 0/10] [Batch 2214/7121] [D loss: 0.610156] [G loss: 0.710846] [AE loss: 1.272463]\n",
            "[Epoch 0/10] [Batch 2215/7121] [D loss: 0.488598] [G loss: 0.687549] [AE loss: 2.187344]\n",
            "[Epoch 0/10] [Batch 2216/7121] [D loss: 0.664660] [G loss: 0.735321] [AE loss: 1.006505]\n",
            "[Epoch 0/10] [Batch 2217/7121] [D loss: 0.653773] [G loss: 0.727780] [AE loss: 1.111036]\n",
            "[Epoch 0/10] [Batch 2218/7121] [D loss: 0.608331] [G loss: 0.782649] [AE loss: 1.501760]\n",
            "[Epoch 0/10] [Batch 2219/7121] [D loss: 0.628969] [G loss: 0.751454] [AE loss: 1.055634]\n",
            "[Epoch 0/10] [Batch 2220/7121] [D loss: 0.505164] [G loss: 0.809262] [AE loss: 1.258665]\n",
            "[Epoch 0/10] [Batch 2221/7121] [D loss: 0.714471] [G loss: 0.816006] [AE loss: 1.028042]\n",
            "[Epoch 0/10] [Batch 2222/7121] [D loss: 0.501971] [G loss: 0.940659] [AE loss: 1.555400]\n",
            "[Epoch 0/10] [Batch 2223/7121] [D loss: 0.469693] [G loss: 0.975668] [AE loss: 1.154473]\n",
            "[Epoch 0/10] [Batch 2224/7121] [D loss: 0.490992] [G loss: 1.044422] [AE loss: 1.534162]\n",
            "[Epoch 0/10] [Batch 2225/7121] [D loss: 0.611866] [G loss: 1.086239] [AE loss: 1.437344]\n",
            "[Epoch 0/10] [Batch 2226/7121] [D loss: 0.689102] [G loss: 1.097743] [AE loss: 1.124204]\n",
            "[Epoch 0/10] [Batch 2227/7121] [D loss: 0.597832] [G loss: 1.151969] [AE loss: 1.002516]\n",
            "[Epoch 0/10] [Batch 2228/7121] [D loss: 0.567535] [G loss: 1.150161] [AE loss: 1.050848]\n",
            "[Epoch 0/10] [Batch 2229/7121] [D loss: 0.484195] [G loss: 1.176547] [AE loss: 1.246801]\n",
            "[Epoch 0/10] [Batch 2230/7121] [D loss: 0.503527] [G loss: 1.196222] [AE loss: 1.246591]\n",
            "[Epoch 0/10] [Batch 2231/7121] [D loss: 0.575472] [G loss: 1.193345] [AE loss: 1.043900]\n",
            "[Epoch 0/10] [Batch 2232/7121] [D loss: 0.496108] [G loss: 1.126578] [AE loss: 1.494005]\n",
            "[Epoch 0/10] [Batch 2233/7121] [D loss: 0.471531] [G loss: 1.096559] [AE loss: 1.318685]\n",
            "[Epoch 0/10] [Batch 2234/7121] [D loss: 0.484721] [G loss: 1.074018] [AE loss: 1.141029]\n",
            "[Epoch 0/10] [Batch 2235/7121] [D loss: 0.458319] [G loss: 1.075410] [AE loss: 1.345864]\n",
            "[Epoch 0/10] [Batch 2236/7121] [D loss: 0.473858] [G loss: 1.000628] [AE loss: 1.188777]\n",
            "[Epoch 0/10] [Batch 2237/7121] [D loss: 0.478975] [G loss: 0.992062] [AE loss: 1.764606]\n",
            "[Epoch 0/10] [Batch 2238/7121] [D loss: 0.591881] [G loss: 0.958670] [AE loss: 1.098909]\n",
            "[Epoch 0/10] [Batch 2239/7121] [D loss: 0.592874] [G loss: 0.980010] [AE loss: 1.827255]\n",
            "[Epoch 0/10] [Batch 2240/7121] [D loss: 0.593515] [G loss: 0.930018] [AE loss: 1.402108]\n",
            "[Epoch 0/10] [Batch 2241/7121] [D loss: 0.494289] [G loss: 0.933809] [AE loss: 1.349859]\n",
            "[Epoch 0/10] [Batch 2242/7121] [D loss: 0.490896] [G loss: 0.947448] [AE loss: 3.281666]\n",
            "[Epoch 0/10] [Batch 2243/7121] [D loss: 0.597338] [G loss: 0.962743] [AE loss: 1.054272]\n",
            "[Epoch 0/10] [Batch 2244/7121] [D loss: 0.544054] [G loss: 0.964967] [AE loss: 1.227136]\n",
            "[Epoch 0/10] [Batch 2245/7121] [D loss: 0.496386] [G loss: 0.947551] [AE loss: 1.085541]\n",
            "[Epoch 0/10] [Batch 2246/7121] [D loss: 0.514448] [G loss: 1.037164] [AE loss: 1.186829]\n",
            "[Epoch 0/10] [Batch 2247/7121] [D loss: 0.485387] [G loss: 1.051311] [AE loss: 1.418698]\n",
            "[Epoch 0/10] [Batch 2248/7121] [D loss: 0.444201] [G loss: 1.063438] [AE loss: 1.703257]\n",
            "[Epoch 0/10] [Batch 2249/7121] [D loss: 0.454314] [G loss: 1.101743] [AE loss: 0.997878]\n",
            "[Epoch 0/10] [Batch 2250/7121] [D loss: 0.449757] [G loss: 1.127627] [AE loss: 1.195183]\n",
            "[Epoch 0/10] [Batch 2251/7121] [D loss: 0.528543] [G loss: 1.155644] [AE loss: 1.283270]\n",
            "[Epoch 0/10] [Batch 2252/7121] [D loss: 0.405076] [G loss: 1.186265] [AE loss: 1.562795]\n",
            "[Epoch 0/10] [Batch 2253/7121] [D loss: 0.442093] [G loss: 1.167367] [AE loss: 0.983446]\n",
            "[Epoch 0/10] [Batch 2254/7121] [D loss: 0.548605] [G loss: 1.231987] [AE loss: 1.239424]\n",
            "[Epoch 0/10] [Batch 2255/7121] [D loss: 0.432733] [G loss: 1.255821] [AE loss: 1.079737]\n",
            "[Epoch 0/10] [Batch 2256/7121] [D loss: 0.447773] [G loss: 1.244890] [AE loss: 1.315532]\n",
            "[Epoch 0/10] [Batch 2257/7121] [D loss: 0.379256] [G loss: 1.255967] [AE loss: 1.126749]\n",
            "[Epoch 0/10] [Batch 2258/7121] [D loss: 0.387041] [G loss: 1.206908] [AE loss: 1.542736]\n",
            "[Epoch 0/10] [Batch 2259/7121] [D loss: 0.354093] [G loss: 1.251630] [AE loss: 1.349119]\n",
            "[Epoch 0/10] [Batch 2260/7121] [D loss: 0.392482] [G loss: 1.213508] [AE loss: 1.069075]\n",
            "[Epoch 0/10] [Batch 2261/7121] [D loss: 0.401242] [G loss: 1.203619] [AE loss: 1.076733]\n",
            "[Epoch 0/10] [Batch 2262/7121] [D loss: 0.372453] [G loss: 1.160794] [AE loss: 1.356672]\n",
            "[Epoch 0/10] [Batch 2263/7121] [D loss: 0.366832] [G loss: 1.161212] [AE loss: 1.067483]\n",
            "[Epoch 0/10] [Batch 2264/7121] [D loss: 0.406013] [G loss: 1.151655] [AE loss: 1.007676]\n",
            "[Epoch 0/10] [Batch 2265/7121] [D loss: 0.435554] [G loss: 1.138919] [AE loss: 1.005997]\n",
            "[Epoch 0/10] [Batch 2266/7121] [D loss: 0.411300] [G loss: 1.153842] [AE loss: 0.996262]\n",
            "[Epoch 0/10] [Batch 2267/7121] [D loss: 0.379436] [G loss: 1.105889] [AE loss: 1.093126]\n",
            "[Epoch 0/10] [Batch 2268/7121] [D loss: 0.382977] [G loss: 1.083753] [AE loss: 1.134350]\n",
            "[Epoch 0/10] [Batch 2269/7121] [D loss: 0.391194] [G loss: 1.040009] [AE loss: 1.280327]\n",
            "[Epoch 0/10] [Batch 2270/7121] [D loss: 0.418384] [G loss: 1.035306] [AE loss: 1.194275]\n",
            "[Epoch 0/10] [Batch 2271/7121] [D loss: 0.325790] [G loss: 1.014269] [AE loss: 1.268784]\n",
            "[Epoch 0/10] [Batch 2272/7121] [D loss: 0.336071] [G loss: 1.030185] [AE loss: 1.293341]\n",
            "[Epoch 0/10] [Batch 2273/7121] [D loss: 0.346329] [G loss: 1.016259] [AE loss: 1.158007]\n",
            "[Epoch 0/10] [Batch 2274/7121] [D loss: 0.381297] [G loss: 1.032210] [AE loss: 1.631762]\n",
            "[Epoch 0/10] [Batch 2275/7121] [D loss: 0.358978] [G loss: 1.072029] [AE loss: 1.148884]\n",
            "[Epoch 0/10] [Batch 2276/7121] [D loss: 0.425360] [G loss: 1.097759] [AE loss: 1.016276]\n",
            "[Epoch 0/10] [Batch 2277/7121] [D loss: 0.296810] [G loss: 1.119687] [AE loss: 1.565051]\n",
            "[Epoch 0/10] [Batch 2278/7121] [D loss: 0.328654] [G loss: 1.096261] [AE loss: 1.134752]\n",
            "[Epoch 0/10] [Batch 2279/7121] [D loss: 0.301863] [G loss: 1.120452] [AE loss: 1.202774]\n",
            "[Epoch 0/10] [Batch 2280/7121] [D loss: 0.359972] [G loss: 1.146231] [AE loss: 1.254283]\n",
            "[Epoch 0/10] [Batch 2281/7121] [D loss: 0.394164] [G loss: 1.163702] [AE loss: 1.050539]\n",
            "[Epoch 0/10] [Batch 2282/7121] [D loss: 0.288481] [G loss: 1.156123] [AE loss: 1.116086]\n",
            "[Epoch 0/10] [Batch 2283/7121] [D loss: 0.306813] [G loss: 1.164497] [AE loss: 1.267542]\n",
            "[Epoch 0/10] [Batch 2284/7121] [D loss: 0.332291] [G loss: 1.195727] [AE loss: 1.025282]\n",
            "[Epoch 0/10] [Batch 2285/7121] [D loss: 0.343570] [G loss: 1.220891] [AE loss: 0.894912]\n",
            "[Epoch 0/10] [Batch 2286/7121] [D loss: 0.316345] [G loss: 1.166688] [AE loss: 0.964866]\n",
            "[Epoch 0/10] [Batch 2287/7121] [D loss: 0.269012] [G loss: 1.193862] [AE loss: 1.049317]\n",
            "[Epoch 0/10] [Batch 2288/7121] [D loss: 0.255703] [G loss: 1.198437] [AE loss: 0.932102]\n",
            "[Epoch 0/10] [Batch 2289/7121] [D loss: 0.379737] [G loss: 1.119738] [AE loss: 0.853211]\n",
            "[Epoch 0/10] [Batch 2290/7121] [D loss: 0.315902] [G loss: 1.108664] [AE loss: 1.089011]\n",
            "[Epoch 0/10] [Batch 2291/7121] [D loss: 0.348772] [G loss: 1.047291] [AE loss: 0.931415]\n",
            "[Epoch 0/10] [Batch 2292/7121] [D loss: 0.398950] [G loss: 1.041445] [AE loss: 0.976698]\n",
            "[Epoch 0/10] [Batch 2293/7121] [D loss: 0.352049] [G loss: 1.033505] [AE loss: 1.467977]\n",
            "[Epoch 0/10] [Batch 2294/7121] [D loss: 0.376881] [G loss: 0.988701] [AE loss: 1.150330]\n",
            "[Epoch 0/10] [Batch 2295/7121] [D loss: 0.296101] [G loss: 1.008205] [AE loss: 1.067700]\n",
            "[Epoch 0/10] [Batch 2296/7121] [D loss: 0.351267] [G loss: 0.968092] [AE loss: 1.076848]\n",
            "[Epoch 0/10] [Batch 2297/7121] [D loss: 0.359759] [G loss: 0.959013] [AE loss: 1.024960]\n",
            "[Epoch 0/10] [Batch 2298/7121] [D loss: 0.311628] [G loss: 0.965844] [AE loss: 2.117800]\n",
            "[Epoch 0/10] [Batch 2299/7121] [D loss: 0.360423] [G loss: 0.928575] [AE loss: 1.050787]\n",
            "[Epoch 0/10] [Batch 2300/7121] [D loss: 0.440699] [G loss: 0.909334] [AE loss: 1.008908]\n",
            "[Epoch 0/10] [Batch 2301/7121] [D loss: 0.370532] [G loss: 0.835096] [AE loss: 1.391667]\n",
            "[Epoch 0/10] [Batch 2302/7121] [D loss: 0.357436] [G loss: 0.888801] [AE loss: 1.958694]\n",
            "[Epoch 0/10] [Batch 2303/7121] [D loss: 0.446205] [G loss: 0.830701] [AE loss: 1.154009]\n",
            "[Epoch 0/10] [Batch 2304/7121] [D loss: 0.458852] [G loss: 0.698278] [AE loss: 1.180589]\n",
            "[Epoch 0/10] [Batch 2305/7121] [D loss: 0.380898] [G loss: 0.803339] [AE loss: 3.413082]\n",
            "[Epoch 0/10] [Batch 2306/7121] [D loss: 0.443233] [G loss: 0.722514] [AE loss: 1.251895]\n",
            "[Epoch 0/10] [Batch 2307/7121] [D loss: 0.448166] [G loss: 0.770365] [AE loss: 1.452829]\n",
            "[Epoch 0/10] [Batch 2308/7121] [D loss: 0.446445] [G loss: 0.755831] [AE loss: 1.468078]\n",
            "[Epoch 0/10] [Batch 2309/7121] [D loss: 0.510555] [G loss: 0.772619] [AE loss: 1.226285]\n",
            "[Epoch 0/10] [Batch 2310/7121] [D loss: 0.508284] [G loss: 0.810451] [AE loss: 1.259410]\n",
            "[Epoch 0/10] [Batch 2311/7121] [D loss: 0.454100] [G loss: 0.798642] [AE loss: 2.351314]\n",
            "[Epoch 0/10] [Batch 2312/7121] [D loss: 0.507968] [G loss: 0.842182] [AE loss: 1.028711]\n",
            "[Epoch 0/10] [Batch 2313/7121] [D loss: 0.493416] [G loss: 0.880676] [AE loss: 2.867603]\n",
            "[Epoch 0/10] [Batch 2314/7121] [D loss: 0.450530] [G loss: 0.903070] [AE loss: 1.553039]\n",
            "[Epoch 0/10] [Batch 2315/7121] [D loss: 0.457001] [G loss: 0.998011] [AE loss: 1.249466]\n",
            "[Epoch 0/10] [Batch 2316/7121] [D loss: 0.472907] [G loss: 0.999925] [AE loss: 1.425086]\n",
            "[Epoch 0/10] [Batch 2317/7121] [D loss: 0.457332] [G loss: 0.958939] [AE loss: 1.318173]\n",
            "[Epoch 0/10] [Batch 2318/7121] [D loss: 0.574620] [G loss: 0.960657] [AE loss: 1.472626]\n",
            "[Epoch 0/10] [Batch 2319/7121] [D loss: 0.556270] [G loss: 1.045424] [AE loss: 1.347694]\n",
            "[Epoch 0/10] [Batch 2320/7121] [D loss: 0.657730] [G loss: 1.011446] [AE loss: 1.316697]\n",
            "[Epoch 0/10] [Batch 2321/7121] [D loss: 0.578802] [G loss: 0.975540] [AE loss: 1.172198]\n",
            "[Epoch 0/10] [Batch 2322/7121] [D loss: 0.514554] [G loss: 1.032726] [AE loss: 3.308121]\n",
            "[Epoch 0/10] [Batch 2323/7121] [D loss: 0.471625] [G loss: 1.014578] [AE loss: 1.266159]\n",
            "[Epoch 0/10] [Batch 2324/7121] [D loss: 0.579586] [G loss: 0.997429] [AE loss: 1.650925]\n",
            "[Epoch 0/10] [Batch 2325/7121] [D loss: 0.696603] [G loss: 1.003192] [AE loss: 1.355681]\n",
            "[Epoch 0/10] [Batch 2326/7121] [D loss: 0.543030] [G loss: 1.046182] [AE loss: 1.650281]\n",
            "[Epoch 0/10] [Batch 2327/7121] [D loss: 0.794461] [G loss: 1.022060] [AE loss: 1.221809]\n",
            "[Epoch 0/10] [Batch 2328/7121] [D loss: 0.567573] [G loss: 1.013117] [AE loss: 1.283763]\n",
            "[Epoch 0/10] [Batch 2329/7121] [D loss: 0.556284] [G loss: 1.014077] [AE loss: 1.383011]\n",
            "[Epoch 0/10] [Batch 2330/7121] [D loss: 0.433542] [G loss: 1.108980] [AE loss: 1.461259]\n",
            "[Epoch 0/10] [Batch 2331/7121] [D loss: 0.688567] [G loss: 1.066962] [AE loss: 1.156188]\n",
            "[Epoch 0/10] [Batch 2332/7121] [D loss: 0.688487] [G loss: 1.257346] [AE loss: 4.609924]\n",
            "[Epoch 0/10] [Batch 2333/7121] [D loss: 0.418182] [G loss: 1.217920] [AE loss: 1.373554]\n",
            "[Epoch 0/10] [Batch 2334/7121] [D loss: 0.464068] [G loss: 1.244315] [AE loss: 1.311543]\n",
            "[Epoch 0/10] [Batch 2335/7121] [D loss: 0.575815] [G loss: 1.217208] [AE loss: 1.532801]\n",
            "[Epoch 0/10] [Batch 2336/7121] [D loss: 0.648616] [G loss: 1.330019] [AE loss: 1.517089]\n",
            "[Epoch 0/10] [Batch 2337/7121] [D loss: 0.527861] [G loss: 1.408865] [AE loss: 1.643171]\n",
            "[Epoch 0/10] [Batch 2338/7121] [D loss: 0.569832] [G loss: 1.340643] [AE loss: 1.741416]\n",
            "[Epoch 0/10] [Batch 2339/7121] [D loss: 0.561675] [G loss: 1.469903] [AE loss: 1.400752]\n",
            "[Epoch 0/10] [Batch 2340/7121] [D loss: 0.603998] [G loss: 1.564727] [AE loss: 1.343038]\n",
            "[Epoch 0/10] [Batch 2341/7121] [D loss: 0.499692] [G loss: 1.566953] [AE loss: 1.463334]\n",
            "[Epoch 0/10] [Batch 2342/7121] [D loss: 0.398953] [G loss: 1.682878] [AE loss: 1.635254]\n",
            "[Epoch 0/10] [Batch 2343/7121] [D loss: 0.617264] [G loss: 1.659020] [AE loss: 1.512546]\n",
            "[Epoch 0/10] [Batch 2344/7121] [D loss: 0.503770] [G loss: 1.579342] [AE loss: 1.396318]\n",
            "[Epoch 0/10] [Batch 2345/7121] [D loss: 0.533830] [G loss: 1.530264] [AE loss: 1.855871]\n",
            "[Epoch 0/10] [Batch 2346/7121] [D loss: 0.524842] [G loss: 1.467350] [AE loss: 1.362249]\n",
            "[Epoch 0/10] [Batch 2347/7121] [D loss: 0.546420] [G loss: 1.401764] [AE loss: 1.672418]\n",
            "[Epoch 0/10] [Batch 2348/7121] [D loss: 0.547509] [G loss: 1.462064] [AE loss: 1.458354]\n",
            "[Epoch 0/10] [Batch 2349/7121] [D loss: 0.518837] [G loss: 1.497282] [AE loss: 1.385064]\n",
            "[Epoch 0/10] [Batch 2350/7121] [D loss: 0.703480] [G loss: 1.504913] [AE loss: 1.513682]\n",
            "[Epoch 0/10] [Batch 2351/7121] [D loss: 0.473477] [G loss: 1.461664] [AE loss: 1.290572]\n",
            "[Epoch 0/10] [Batch 2352/7121] [D loss: 0.500758] [G loss: 1.467903] [AE loss: 1.538089]\n",
            "[Epoch 0/10] [Batch 2353/7121] [D loss: 0.495039] [G loss: 1.415943] [AE loss: 1.397694]\n",
            "[Epoch 0/10] [Batch 2354/7121] [D loss: 0.623691] [G loss: 1.456404] [AE loss: 1.917642]\n",
            "[Epoch 0/10] [Batch 2355/7121] [D loss: 0.412481] [G loss: 1.385543] [AE loss: 1.385154]\n",
            "[Epoch 0/10] [Batch 2356/7121] [D loss: 0.426595] [G loss: 1.433184] [AE loss: 1.617426]\n",
            "[Epoch 0/10] [Batch 2357/7121] [D loss: 0.464765] [G loss: 1.385570] [AE loss: 1.491325]\n",
            "[Epoch 0/10] [Batch 2358/7121] [D loss: 0.553348] [G loss: 1.480888] [AE loss: 1.293221]\n",
            "[Epoch 0/10] [Batch 2359/7121] [D loss: 0.434891] [G loss: 1.461142] [AE loss: 1.693074]\n",
            "[Epoch 0/10] [Batch 2360/7121] [D loss: 0.469315] [G loss: 1.508881] [AE loss: 1.515439]\n",
            "[Epoch 0/10] [Batch 2361/7121] [D loss: 0.518996] [G loss: 1.461349] [AE loss: 1.184220]\n",
            "[Epoch 0/10] [Batch 2362/7121] [D loss: 0.454119] [G loss: 1.472179] [AE loss: 1.329339]\n",
            "[Epoch 0/10] [Batch 2363/7121] [D loss: 0.646973] [G loss: 1.487910] [AE loss: 1.491036]\n",
            "[Epoch 0/10] [Batch 2364/7121] [D loss: 0.488476] [G loss: 1.444911] [AE loss: 1.252142]\n",
            "[Epoch 0/10] [Batch 2365/7121] [D loss: 0.322780] [G loss: 1.384641] [AE loss: 1.495436]\n",
            "[Epoch 0/10] [Batch 2366/7121] [D loss: 0.716746] [G loss: 1.420123] [AE loss: 1.255447]\n",
            "[Epoch 0/10] [Batch 2367/7121] [D loss: 0.497413] [G loss: 1.351554] [AE loss: 1.219021]\n",
            "[Epoch 0/10] [Batch 2368/7121] [D loss: 0.511995] [G loss: 1.310357] [AE loss: 1.441815]\n",
            "[Epoch 0/10] [Batch 2369/7121] [D loss: 0.624775] [G loss: 1.292724] [AE loss: 1.107494]\n",
            "[Epoch 0/10] [Batch 2370/7121] [D loss: 0.450770] [G loss: 1.155641] [AE loss: 1.255992]\n",
            "[Epoch 0/10] [Batch 2371/7121] [D loss: 0.693907] [G loss: 1.099026] [AE loss: 1.461771]\n",
            "[Epoch 0/10] [Batch 2372/7121] [D loss: 0.690144] [G loss: 1.010419] [AE loss: 1.366235]\n",
            "[Epoch 0/10] [Batch 2373/7121] [D loss: 0.626920] [G loss: 0.937804] [AE loss: 1.662896]\n",
            "[Epoch 0/10] [Batch 2374/7121] [D loss: 0.532623] [G loss: 0.941002] [AE loss: 1.392340]\n",
            "[Epoch 0/10] [Batch 2375/7121] [D loss: 0.545200] [G loss: 0.838664] [AE loss: 1.309886]\n",
            "[Epoch 0/10] [Batch 2376/7121] [D loss: 0.561132] [G loss: 0.817661] [AE loss: 1.352286]\n",
            "[Epoch 0/10] [Batch 2377/7121] [D loss: 0.523091] [G loss: 0.818800] [AE loss: 1.835041]\n",
            "[Epoch 0/10] [Batch 2378/7121] [D loss: 0.469063] [G loss: 0.795646] [AE loss: 5.432298]\n",
            "[Epoch 0/10] [Batch 2379/7121] [D loss: 0.654205] [G loss: 0.744757] [AE loss: 1.292613]\n",
            "[Epoch 0/10] [Batch 2380/7121] [D loss: 0.545372] [G loss: 0.719537] [AE loss: 1.297620]\n",
            "[Epoch 0/10] [Batch 2381/7121] [D loss: 0.633422] [G loss: 0.715058] [AE loss: 1.202274]\n",
            "[Epoch 0/10] [Batch 2382/7121] [D loss: 0.704585] [G loss: 0.649324] [AE loss: 2.146268]\n",
            "[Epoch 0/10] [Batch 2383/7121] [D loss: 0.669785] [G loss: 0.538964] [AE loss: 1.207222]\n",
            "[Epoch 0/10] [Batch 2384/7121] [D loss: 0.869379] [G loss: 0.422241] [AE loss: 1.224726]\n",
            "[Epoch 0/10] [Batch 2385/7121] [D loss: 0.740532] [G loss: 0.391202] [AE loss: 1.084145]\n",
            "[Epoch 0/10] [Batch 2386/7121] [D loss: 0.936360] [G loss: 0.381876] [AE loss: 1.201181]\n",
            "[Epoch 0/10] [Batch 2387/7121] [D loss: 0.862438] [G loss: 0.327330] [AE loss: 0.920758]\n",
            "[Epoch 0/10] [Batch 2388/7121] [D loss: 0.687705] [G loss: 0.384313] [AE loss: 1.198278]\n",
            "[Epoch 0/10] [Batch 2389/7121] [D loss: 0.683297] [G loss: 0.438192] [AE loss: 1.212514]\n",
            "[Epoch 0/10] [Batch 2390/7121] [D loss: 0.854751] [G loss: 0.449713] [AE loss: 0.952212]\n",
            "[Epoch 0/10] [Batch 2391/7121] [D loss: 0.687991] [G loss: 0.552194] [AE loss: 1.248416]\n",
            "[Epoch 0/10] [Batch 2392/7121] [D loss: 0.556857] [G loss: 0.659373] [AE loss: 1.236446]\n",
            "[Epoch 0/10] [Batch 2393/7121] [D loss: 0.634471] [G loss: 0.724874] [AE loss: 1.840298]\n",
            "[Epoch 0/10] [Batch 2394/7121] [D loss: 0.631593] [G loss: 0.824636] [AE loss: 0.977373]\n",
            "[Epoch 0/10] [Batch 2395/7121] [D loss: 0.566696] [G loss: 0.952614] [AE loss: 2.494232]\n",
            "[Epoch 0/10] [Batch 2396/7121] [D loss: 0.468933] [G loss: 1.049465] [AE loss: 2.016653]\n",
            "[Epoch 0/10] [Batch 2397/7121] [D loss: 0.524316] [G loss: 1.081503] [AE loss: 0.874899]\n",
            "[Epoch 0/10] [Batch 2398/7121] [D loss: 0.518229] [G loss: 1.130131] [AE loss: 0.952961]\n",
            "[Epoch 0/10] [Batch 2399/7121] [D loss: 0.390145] [G loss: 1.241860] [AE loss: 1.227125]\n",
            "[Epoch 0/10] [Batch 2400/7121] [D loss: 0.496253] [G loss: 1.242708] [AE loss: 1.602280]\n",
            "[Epoch 0/10] [Batch 2401/7121] [D loss: 0.425583] [G loss: 1.269778] [AE loss: 1.164644]\n",
            "[Epoch 0/10] [Batch 2402/7121] [D loss: 0.450563] [G loss: 1.271371] [AE loss: 1.050056]\n",
            "[Epoch 0/10] [Batch 2403/7121] [D loss: 0.392848] [G loss: 1.242735] [AE loss: 0.985337]\n",
            "[Epoch 0/10] [Batch 2404/7121] [D loss: 0.397166] [G loss: 1.162429] [AE loss: 1.248122]\n",
            "[Epoch 0/10] [Batch 2405/7121] [D loss: 0.525525] [G loss: 1.048540] [AE loss: 0.893585]\n",
            "[Epoch 0/10] [Batch 2406/7121] [D loss: 0.448613] [G loss: 1.040430] [AE loss: 0.942300]\n",
            "[Epoch 0/10] [Batch 2407/7121] [D loss: 0.453262] [G loss: 1.013428] [AE loss: 1.673684]\n",
            "[Epoch 0/10] [Batch 2408/7121] [D loss: 0.542546] [G loss: 0.747512] [AE loss: 0.889353]\n",
            "[Epoch 0/10] [Batch 2409/7121] [D loss: 0.690012] [G loss: 0.560720] [AE loss: 1.093066]\n",
            "[Epoch 0/10] [Batch 2410/7121] [D loss: 0.657805] [G loss: 0.485949] [AE loss: 1.855116]\n",
            "[Epoch 0/10] [Batch 2411/7121] [D loss: 0.695567] [G loss: 0.486013] [AE loss: 1.176555]\n",
            "[Epoch 0/10] [Batch 2412/7121] [D loss: 0.705777] [G loss: 0.451013] [AE loss: 1.356770]\n",
            "[Epoch 0/10] [Batch 2413/7121] [D loss: 0.731615] [G loss: 0.441009] [AE loss: 2.635026]\n",
            "[Epoch 0/10] [Batch 2414/7121] [D loss: 0.789489] [G loss: 0.413175] [AE loss: 1.011108]\n",
            "[Epoch 0/10] [Batch 2415/7121] [D loss: 0.738972] [G loss: 0.448467] [AE loss: 1.480341]\n",
            "[Epoch 0/10] [Batch 2416/7121] [D loss: 0.680930] [G loss: 0.539515] [AE loss: 1.060955]\n",
            "[Epoch 0/10] [Batch 2417/7121] [D loss: 0.680648] [G loss: 0.591218] [AE loss: 2.667514]\n",
            "[Epoch 0/10] [Batch 2418/7121] [D loss: 0.651509] [G loss: 0.604350] [AE loss: 4.733090]\n",
            "[Epoch 0/10] [Batch 2419/7121] [D loss: 0.586264] [G loss: 0.697629] [AE loss: 1.214646]\n",
            "[Epoch 0/10] [Batch 2420/7121] [D loss: 0.529316] [G loss: 0.844723] [AE loss: 1.099058]\n",
            "[Epoch 0/10] [Batch 2421/7121] [D loss: 0.521482] [G loss: 0.842336] [AE loss: 1.626488]\n",
            "[Epoch 0/10] [Batch 2422/7121] [D loss: 0.535848] [G loss: 0.924815] [AE loss: 1.203655]\n",
            "[Epoch 0/10] [Batch 2423/7121] [D loss: 0.501257] [G loss: 1.016242] [AE loss: 1.175335]\n",
            "[Epoch 0/10] [Batch 2424/7121] [D loss: 0.520572] [G loss: 1.073199] [AE loss: 1.153486]\n",
            "[Epoch 0/10] [Batch 2425/7121] [D loss: 0.491331] [G loss: 1.093536] [AE loss: 0.973711]\n",
            "[Epoch 0/10] [Batch 2426/7121] [D loss: 0.403231] [G loss: 1.170181] [AE loss: 1.153550]\n",
            "[Epoch 0/10] [Batch 2427/7121] [D loss: 0.501811] [G loss: 1.201359] [AE loss: 0.878503]\n",
            "[Epoch 0/10] [Batch 2428/7121] [D loss: 0.480386] [G loss: 1.218360] [AE loss: 0.947006]\n",
            "[Epoch 0/10] [Batch 2429/7121] [D loss: 0.431298] [G loss: 1.251650] [AE loss: 4.481553]\n",
            "[Epoch 0/10] [Batch 2430/7121] [D loss: 0.409115] [G loss: 1.260097] [AE loss: 1.107882]\n",
            "[Epoch 0/10] [Batch 2431/7121] [D loss: 0.538553] [G loss: 1.269914] [AE loss: 0.870978]\n",
            "[Epoch 0/10] [Batch 2432/7121] [D loss: 0.408674] [G loss: 1.242865] [AE loss: 1.036854]\n",
            "[Epoch 0/10] [Batch 2433/7121] [D loss: 0.449052] [G loss: 1.176128] [AE loss: 0.979637]\n",
            "[Epoch 0/10] [Batch 2434/7121] [D loss: 0.517701] [G loss: 1.184362] [AE loss: 0.943921]\n",
            "[Epoch 0/10] [Batch 2435/7121] [D loss: 0.433935] [G loss: 1.165183] [AE loss: 1.054048]\n",
            "[Epoch 0/10] [Batch 2436/7121] [D loss: 0.476479] [G loss: 1.085809] [AE loss: 1.057939]\n",
            "[Epoch 0/10] [Batch 2437/7121] [D loss: 0.431181] [G loss: 1.089618] [AE loss: 1.755451]\n",
            "[Epoch 0/10] [Batch 2438/7121] [D loss: 0.376023] [G loss: 1.094585] [AE loss: 1.667482]\n",
            "[Epoch 0/10] [Batch 2439/7121] [D loss: 0.464520] [G loss: 1.008172] [AE loss: 1.327638]\n",
            "[Epoch 0/10] [Batch 2440/7121] [D loss: 0.451867] [G loss: 1.008429] [AE loss: 1.008916]\n",
            "[Epoch 0/10] [Batch 2441/7121] [D loss: 0.495746] [G loss: 1.004621] [AE loss: 0.939093]\n",
            "[Epoch 0/10] [Batch 2442/7121] [D loss: 0.523012] [G loss: 0.949368] [AE loss: 0.891093]\n",
            "[Epoch 0/10] [Batch 2443/7121] [D loss: 0.468613] [G loss: 1.007029] [AE loss: 0.826460]\n",
            "[Epoch 0/10] [Batch 2444/7121] [D loss: 0.495255] [G loss: 1.002454] [AE loss: 0.940853]\n",
            "[Epoch 0/10] [Batch 2445/7121] [D loss: 0.430511] [G loss: 0.999807] [AE loss: 1.451268]\n",
            "[Epoch 0/10] [Batch 2446/7121] [D loss: 0.450737] [G loss: 1.024487] [AE loss: 1.003721]\n",
            "[Epoch 0/10] [Batch 2447/7121] [D loss: 0.379649] [G loss: 1.018458] [AE loss: 1.671556]\n",
            "[Epoch 0/10] [Batch 2448/7121] [D loss: 0.418310] [G loss: 1.092134] [AE loss: 1.529133]\n",
            "[Epoch 0/10] [Batch 2449/7121] [D loss: 0.398013] [G loss: 1.116818] [AE loss: 1.255318]\n",
            "[Epoch 0/10] [Batch 2450/7121] [D loss: 0.430191] [G loss: 1.164387] [AE loss: 0.949827]\n",
            "[Epoch 0/10] [Batch 2451/7121] [D loss: 0.368960] [G loss: 1.144701] [AE loss: 1.575612]\n",
            "[Epoch 0/10] [Batch 2452/7121] [D loss: 0.368690] [G loss: 1.216631] [AE loss: 1.784241]\n",
            "[Epoch 0/10] [Batch 2453/7121] [D loss: 0.319334] [G loss: 1.247844] [AE loss: 1.172924]\n",
            "[Epoch 0/10] [Batch 2454/7121] [D loss: 0.278753] [G loss: 1.291317] [AE loss: 1.286091]\n",
            "[Epoch 0/10] [Batch 2455/7121] [D loss: 0.303011] [G loss: 1.389528] [AE loss: 1.411299]\n",
            "[Epoch 0/10] [Batch 2456/7121] [D loss: 0.369464] [G loss: 1.381206] [AE loss: 1.071745]\n",
            "[Epoch 0/10] [Batch 2457/7121] [D loss: 0.251703] [G loss: 1.469162] [AE loss: 3.131576]\n",
            "[Epoch 0/10] [Batch 2458/7121] [D loss: 0.324515] [G loss: 1.487318] [AE loss: 1.619097]\n",
            "[Epoch 0/10] [Batch 2459/7121] [D loss: 0.390084] [G loss: 1.538421] [AE loss: 1.277722]\n",
            "[Epoch 0/10] [Batch 2460/7121] [D loss: 0.352462] [G loss: 1.522874] [AE loss: 1.169608]\n",
            "[Epoch 0/10] [Batch 2461/7121] [D loss: 0.290086] [G loss: 1.568409] [AE loss: 3.018744]\n",
            "[Epoch 0/10] [Batch 2462/7121] [D loss: 0.373168] [G loss: 1.565894] [AE loss: 1.117320]\n",
            "[Epoch 0/10] [Batch 2463/7121] [D loss: 0.364943] [G loss: 1.520787] [AE loss: 1.105311]\n",
            "[Epoch 0/10] [Batch 2464/7121] [D loss: 0.337939] [G loss: 1.532513] [AE loss: 1.516851]\n",
            "[Epoch 0/10] [Batch 2465/7121] [D loss: 0.294677] [G loss: 1.591093] [AE loss: 1.489047]\n",
            "[Epoch 0/10] [Batch 2466/7121] [D loss: 0.424372] [G loss: 1.551610] [AE loss: 1.134959]\n",
            "[Epoch 0/10] [Batch 2467/7121] [D loss: 0.354676] [G loss: 1.526420] [AE loss: 1.326424]\n",
            "[Epoch 0/10] [Batch 2468/7121] [D loss: 0.318262] [G loss: 1.552997] [AE loss: 1.349001]\n",
            "[Epoch 0/10] [Batch 2469/7121] [D loss: 0.330201] [G loss: 1.540629] [AE loss: 1.454869]\n",
            "[Epoch 0/10] [Batch 2470/7121] [D loss: 0.399411] [G loss: 1.518337] [AE loss: 1.488564]\n",
            "[Epoch 0/10] [Batch 2471/7121] [D loss: 0.294233] [G loss: 1.494024] [AE loss: 1.070498]\n",
            "[Epoch 0/10] [Batch 2472/7121] [D loss: 0.351254] [G loss: 1.459124] [AE loss: 1.027222]\n",
            "[Epoch 0/10] [Batch 2473/7121] [D loss: 0.404321] [G loss: 1.419229] [AE loss: 1.200101]\n",
            "[Epoch 0/10] [Batch 2474/7121] [D loss: 0.408013] [G loss: 1.405840] [AE loss: 1.019675]\n",
            "[Epoch 0/10] [Batch 2475/7121] [D loss: 0.389653] [G loss: 1.363957] [AE loss: 1.049823]\n",
            "[Epoch 0/10] [Batch 2476/7121] [D loss: 0.344754] [G loss: 1.339302] [AE loss: 1.137419]\n",
            "[Epoch 0/10] [Batch 2477/7121] [D loss: 0.313661] [G loss: 1.257883] [AE loss: 1.363896]\n",
            "[Epoch 0/10] [Batch 2478/7121] [D loss: 0.358691] [G loss: 1.218672] [AE loss: 1.152116]\n",
            "[Epoch 0/10] [Batch 2479/7121] [D loss: 0.298291] [G loss: 1.157366] [AE loss: 1.214279]\n",
            "[Epoch 0/10] [Batch 2480/7121] [D loss: 0.324793] [G loss: 1.108051] [AE loss: 1.113215]\n",
            "[Epoch 0/10] [Batch 2481/7121] [D loss: 0.332076] [G loss: 1.043109] [AE loss: 1.497507]\n",
            "[Epoch 0/10] [Batch 2482/7121] [D loss: 0.462076] [G loss: 1.034340] [AE loss: 1.033738]\n",
            "[Epoch 0/10] [Batch 2483/7121] [D loss: 0.388745] [G loss: 0.974619] [AE loss: 1.121738]\n",
            "[Epoch 0/10] [Batch 2484/7121] [D loss: 0.406631] [G loss: 0.937744] [AE loss: 1.065581]\n",
            "[Epoch 0/10] [Batch 2485/7121] [D loss: 0.584911] [G loss: 0.960760] [AE loss: 1.974455]\n",
            "[Epoch 0/10] [Batch 2486/7121] [D loss: 0.408442] [G loss: 0.965178] [AE loss: 1.402355]\n",
            "[Epoch 0/10] [Batch 2487/7121] [D loss: 0.489526] [G loss: 0.982739] [AE loss: 1.264164]\n",
            "[Epoch 0/10] [Batch 2488/7121] [D loss: 0.443305] [G loss: 0.981159] [AE loss: 0.939189]\n",
            "[Epoch 0/10] [Batch 2489/7121] [D loss: 0.448792] [G loss: 1.056354] [AE loss: 1.133722]\n",
            "[Epoch 0/10] [Batch 2490/7121] [D loss: 0.381961] [G loss: 1.075742] [AE loss: 1.227266]\n",
            "[Epoch 0/10] [Batch 2491/7121] [D loss: 0.340709] [G loss: 1.129741] [AE loss: 1.117716]\n",
            "[Epoch 0/10] [Batch 2492/7121] [D loss: 0.337354] [G loss: 1.139312] [AE loss: 1.569479]\n",
            "[Epoch 0/10] [Batch 2493/7121] [D loss: 0.493035] [G loss: 1.167236] [AE loss: 1.214496]\n",
            "[Epoch 0/10] [Batch 2494/7121] [D loss: 0.319620] [G loss: 1.202854] [AE loss: 1.107834]\n",
            "[Epoch 0/10] [Batch 2495/7121] [D loss: 0.350811] [G loss: 1.169681] [AE loss: 1.522993]\n",
            "[Epoch 0/10] [Batch 2496/7121] [D loss: 0.310037] [G loss: 1.187047] [AE loss: 1.133559]\n",
            "[Epoch 0/10] [Batch 2497/7121] [D loss: 0.404716] [G loss: 1.177383] [AE loss: 1.661585]\n",
            "[Epoch 0/10] [Batch 2498/7121] [D loss: 0.350274] [G loss: 1.253002] [AE loss: 1.359032]\n",
            "[Epoch 0/10] [Batch 2499/7121] [D loss: 0.343096] [G loss: 1.301983] [AE loss: 1.116141]\n",
            "[Epoch 0/10] [Batch 2500/7121] [D loss: 0.430701] [G loss: 1.261016] [AE loss: 1.111148]\n",
            "[Epoch 0/10] [Batch 2501/7121] [D loss: 0.368535] [G loss: 1.230902] [AE loss: 1.718081]\n",
            "[Epoch 0/10] [Batch 2502/7121] [D loss: 0.375247] [G loss: 1.222779] [AE loss: 1.315897]\n",
            "[Epoch 0/10] [Batch 2503/7121] [D loss: 0.267184] [G loss: 1.291796] [AE loss: 1.319759]\n",
            "[Epoch 0/10] [Batch 2504/7121] [D loss: 0.475838] [G loss: 1.314465] [AE loss: 1.191307]\n",
            "[Epoch 0/10] [Batch 2505/7121] [D loss: 0.288935] [G loss: 1.341017] [AE loss: 1.305178]\n",
            "[Epoch 0/10] [Batch 2506/7121] [D loss: 0.383886] [G loss: 1.348324] [AE loss: 1.126223]\n",
            "[Epoch 0/10] [Batch 2507/7121] [D loss: 0.438422] [G loss: 1.353131] [AE loss: 1.136370]\n",
            "[Epoch 0/10] [Batch 2508/7121] [D loss: 0.276781] [G loss: 1.389818] [AE loss: 1.541390]\n",
            "[Epoch 0/10] [Batch 2509/7121] [D loss: 0.256715] [G loss: 1.395379] [AE loss: 1.631399]\n",
            "[Epoch 0/10] [Batch 2510/7121] [D loss: 0.333637] [G loss: 1.407362] [AE loss: 1.261382]\n",
            "[Epoch 0/10] [Batch 2511/7121] [D loss: 0.248705] [G loss: 1.337245] [AE loss: 1.800936]\n",
            "[Epoch 0/10] [Batch 2512/7121] [D loss: 0.382832] [G loss: 1.279100] [AE loss: 1.181494]\n",
            "[Epoch 0/10] [Batch 2513/7121] [D loss: 0.357319] [G loss: 1.127518] [AE loss: 1.124716]\n",
            "[Epoch 0/10] [Batch 2514/7121] [D loss: 0.506145] [G loss: 0.985434] [AE loss: 6.415172]\n",
            "[Epoch 0/10] [Batch 2515/7121] [D loss: 0.431754] [G loss: 0.842031] [AE loss: 1.150110]\n",
            "[Epoch 0/10] [Batch 2516/7121] [D loss: 0.637411] [G loss: 0.882783] [AE loss: 1.897888]\n",
            "[Epoch 0/10] [Batch 2517/7121] [D loss: 0.642623] [G loss: 0.681875] [AE loss: 1.728751]\n",
            "[Epoch 0/10] [Batch 2518/7121] [D loss: 0.545630] [G loss: 0.580942] [AE loss: 1.295736]\n",
            "[Epoch 0/10] [Batch 2519/7121] [D loss: 0.779585] [G loss: 0.471601] [AE loss: 1.206269]\n",
            "[Epoch 0/10] [Batch 2520/7121] [D loss: 0.824201] [G loss: 0.452651] [AE loss: 1.306658]\n",
            "[Epoch 0/10] [Batch 2521/7121] [D loss: 0.756597] [G loss: 0.421745] [AE loss: 1.054518]\n",
            "[Epoch 0/10] [Batch 2522/7121] [D loss: 0.843649] [G loss: 0.463147] [AE loss: 1.555183]\n",
            "[Epoch 0/10] [Batch 2523/7121] [D loss: 0.634213] [G loss: 0.520419] [AE loss: 1.369681]\n",
            "[Epoch 0/10] [Batch 2524/7121] [D loss: 0.729964] [G loss: 0.585021] [AE loss: 1.357395]\n",
            "[Epoch 0/10] [Batch 2525/7121] [D loss: 0.803542] [G loss: 0.681994] [AE loss: 1.332685]\n",
            "[Epoch 0/10] [Batch 2526/7121] [D loss: 0.712757] [G loss: 0.652973] [AE loss: 1.430897]\n",
            "[Epoch 0/10] [Batch 2527/7121] [D loss: 0.576072] [G loss: 0.826029] [AE loss: 1.223822]\n",
            "[Epoch 0/10] [Batch 2528/7121] [D loss: 0.473745] [G loss: 0.883942] [AE loss: 1.134273]\n",
            "[Epoch 0/10] [Batch 2529/7121] [D loss: 0.642984] [G loss: 0.858797] [AE loss: 1.345962]\n",
            "[Epoch 0/10] [Batch 2530/7121] [D loss: 0.716710] [G loss: 0.889288] [AE loss: 1.112015]\n",
            "[Epoch 0/10] [Batch 2531/7121] [D loss: 0.563730] [G loss: 0.897345] [AE loss: 1.215490]\n",
            "[Epoch 0/10] [Batch 2532/7121] [D loss: 0.572109] [G loss: 1.003210] [AE loss: 1.195007]\n",
            "[Epoch 0/10] [Batch 2533/7121] [D loss: 0.660193] [G loss: 0.808356] [AE loss: 1.864006]\n",
            "[Epoch 0/10] [Batch 2534/7121] [D loss: 0.663093] [G loss: 0.807251] [AE loss: 1.225820]\n",
            "[Epoch 0/10] [Batch 2535/7121] [D loss: 0.751420] [G loss: 0.807627] [AE loss: 1.582777]\n",
            "[Epoch 0/10] [Batch 2536/7121] [D loss: 0.823665] [G loss: 0.725604] [AE loss: 1.225816]\n",
            "[Epoch 0/10] [Batch 2537/7121] [D loss: 0.863993] [G loss: 0.732637] [AE loss: 1.053243]\n",
            "[Epoch 0/10] [Batch 2538/7121] [D loss: 0.910406] [G loss: 0.759578] [AE loss: 1.211232]\n",
            "[Epoch 0/10] [Batch 2539/7121] [D loss: 0.790331] [G loss: 0.748480] [AE loss: 1.326546]\n",
            "[Epoch 0/10] [Batch 2540/7121] [D loss: 0.856444] [G loss: 0.650895] [AE loss: 2.663129]\n",
            "[Epoch 0/10] [Batch 2541/7121] [D loss: 1.030154] [G loss: 0.691431] [AE loss: 1.313890]\n",
            "[Epoch 0/10] [Batch 2542/7121] [D loss: 0.945497] [G loss: 0.652474] [AE loss: 1.208557]\n",
            "[Epoch 0/10] [Batch 2543/7121] [D loss: 0.819067] [G loss: 0.763936] [AE loss: 1.411079]\n",
            "[Epoch 0/10] [Batch 2544/7121] [D loss: 1.207068] [G loss: 0.738391] [AE loss: 1.327177]\n",
            "[Epoch 0/10] [Batch 2545/7121] [D loss: 1.023713] [G loss: 0.792922] [AE loss: 1.079992]\n",
            "[Epoch 0/10] [Batch 2546/7121] [D loss: 1.257817] [G loss: 0.816447] [AE loss: 1.052412]\n",
            "[Epoch 0/10] [Batch 2547/7121] [D loss: 1.036639] [G loss: 0.808432] [AE loss: 1.073741]\n",
            "[Epoch 0/10] [Batch 2548/7121] [D loss: 0.963332] [G loss: 0.866920] [AE loss: 1.253967]\n",
            "[Epoch 0/10] [Batch 2549/7121] [D loss: 1.015586] [G loss: 0.939200] [AE loss: 1.643613]\n",
            "[Epoch 0/10] [Batch 2550/7121] [D loss: 0.976865] [G loss: 0.978817] [AE loss: 1.294082]\n",
            "[Epoch 0/10] [Batch 2551/7121] [D loss: 0.971504] [G loss: 1.055628] [AE loss: 1.119967]\n",
            "[Epoch 0/10] [Batch 2552/7121] [D loss: 0.930734] [G loss: 1.010205] [AE loss: 1.741685]\n",
            "[Epoch 0/10] [Batch 2553/7121] [D loss: 0.777486] [G loss: 1.054405] [AE loss: 1.469424]\n",
            "[Epoch 0/10] [Batch 2554/7121] [D loss: 0.714532] [G loss: 1.134758] [AE loss: 1.431071]\n",
            "[Epoch 0/10] [Batch 2555/7121] [D loss: 1.108711] [G loss: 1.080793] [AE loss: 1.099615]\n",
            "[Epoch 0/10] [Batch 2556/7121] [D loss: 0.804263] [G loss: 1.115531] [AE loss: 1.539524]\n",
            "[Epoch 0/10] [Batch 2557/7121] [D loss: 0.914336] [G loss: 1.045703] [AE loss: 1.300438]\n",
            "[Epoch 0/10] [Batch 2558/7121] [D loss: 0.702588] [G loss: 1.040805] [AE loss: 1.580571]\n",
            "[Epoch 0/10] [Batch 2559/7121] [D loss: 1.054509] [G loss: 0.979457] [AE loss: 1.214069]\n",
            "[Epoch 0/10] [Batch 2560/7121] [D loss: 1.065498] [G loss: 0.911871] [AE loss: 1.720794]\n",
            "[Epoch 0/10] [Batch 2561/7121] [D loss: 0.883895] [G loss: 0.898757] [AE loss: 1.446442]\n",
            "[Epoch 0/10] [Batch 2562/7121] [D loss: 0.759283] [G loss: 0.866212] [AE loss: 1.235595]\n",
            "[Epoch 0/10] [Batch 2563/7121] [D loss: 1.093175] [G loss: 0.810170] [AE loss: 1.209955]\n",
            "[Epoch 0/10] [Batch 2564/7121] [D loss: 0.717015] [G loss: 0.760416] [AE loss: 1.594828]\n",
            "[Epoch 0/10] [Batch 2565/7121] [D loss: 0.712724] [G loss: 0.750899] [AE loss: 1.844548]\n",
            "[Epoch 0/10] [Batch 2566/7121] [D loss: 0.833808] [G loss: 0.720387] [AE loss: 1.409668]\n",
            "[Epoch 0/10] [Batch 2567/7121] [D loss: 0.934750] [G loss: 0.667831] [AE loss: 1.275205]\n",
            "[Epoch 0/10] [Batch 2568/7121] [D loss: 1.001537] [G loss: 0.707482] [AE loss: 1.249672]\n",
            "[Epoch 0/10] [Batch 2569/7121] [D loss: 0.847113] [G loss: 0.652907] [AE loss: 1.180880]\n",
            "[Epoch 0/10] [Batch 2570/7121] [D loss: 0.750327] [G loss: 0.666789] [AE loss: 1.102978]\n",
            "[Epoch 0/10] [Batch 2571/7121] [D loss: 0.792598] [G loss: 0.615047] [AE loss: 1.094605]\n",
            "[Epoch 0/10] [Batch 2572/7121] [D loss: 1.050438] [G loss: 0.597892] [AE loss: 1.134012]\n",
            "[Epoch 0/10] [Batch 2573/7121] [D loss: 0.919040] [G loss: 0.586158] [AE loss: 1.081175]\n",
            "[Epoch 0/10] [Batch 2574/7121] [D loss: 0.999768] [G loss: 0.582436] [AE loss: 1.154986]\n",
            "[Epoch 0/10] [Batch 2575/7121] [D loss: 0.865593] [G loss: 0.579902] [AE loss: 1.395005]\n",
            "[Epoch 0/10] [Batch 2576/7121] [D loss: 0.850196] [G loss: 0.598619] [AE loss: 1.106397]\n",
            "[Epoch 0/10] [Batch 2577/7121] [D loss: 1.070145] [G loss: 0.599427] [AE loss: 1.238775]\n",
            "[Epoch 0/10] [Batch 2578/7121] [D loss: 0.978140] [G loss: 0.611826] [AE loss: 1.261332]\n",
            "[Epoch 0/10] [Batch 2579/7121] [D loss: 0.920464] [G loss: 0.634791] [AE loss: 0.971289]\n",
            "[Epoch 0/10] [Batch 2580/7121] [D loss: 0.815160] [G loss: 0.685423] [AE loss: 1.445354]\n",
            "[Epoch 0/10] [Batch 2581/7121] [D loss: 1.071372] [G loss: 0.674619] [AE loss: 1.117364]\n",
            "[Epoch 0/10] [Batch 2582/7121] [D loss: 0.973060] [G loss: 0.720671] [AE loss: 1.516444]\n",
            "[Epoch 0/10] [Batch 2583/7121] [D loss: 0.841218] [G loss: 0.743645] [AE loss: 1.096593]\n",
            "[Epoch 0/10] [Batch 2584/7121] [D loss: 0.793685] [G loss: 0.786578] [AE loss: 0.912072]\n",
            "[Epoch 0/10] [Batch 2585/7121] [D loss: 0.697986] [G loss: 0.867110] [AE loss: 1.150916]\n",
            "[Epoch 0/10] [Batch 2586/7121] [D loss: 0.773546] [G loss: 0.857158] [AE loss: 1.159937]\n",
            "[Epoch 0/10] [Batch 2587/7121] [D loss: 0.746729] [G loss: 0.901384] [AE loss: 1.374398]\n",
            "[Epoch 0/10] [Batch 2588/7121] [D loss: 0.745022] [G loss: 0.947565] [AE loss: 1.498585]\n",
            "[Epoch 0/10] [Batch 2589/7121] [D loss: 0.662547] [G loss: 0.922004] [AE loss: 1.086662]\n",
            "[Epoch 0/10] [Batch 2590/7121] [D loss: 0.982577] [G loss: 0.953663] [AE loss: 1.538883]\n",
            "[Epoch 0/10] [Batch 2591/7121] [D loss: 0.685766] [G loss: 0.984788] [AE loss: 1.234901]\n",
            "[Epoch 0/10] [Batch 2592/7121] [D loss: 0.717445] [G loss: 0.979249] [AE loss: 1.025859]\n",
            "[Epoch 0/10] [Batch 2593/7121] [D loss: 0.588482] [G loss: 0.982129] [AE loss: 2.155676]\n",
            "[Epoch 0/10] [Batch 2594/7121] [D loss: 0.641834] [G loss: 0.993381] [AE loss: 0.950770]\n",
            "[Epoch 0/10] [Batch 2595/7121] [D loss: 0.580630] [G loss: 0.994246] [AE loss: 1.028143]\n",
            "[Epoch 0/10] [Batch 2596/7121] [D loss: 0.630760] [G loss: 0.994035] [AE loss: 1.273858]\n",
            "[Epoch 0/10] [Batch 2597/7121] [D loss: 0.647748] [G loss: 1.011377] [AE loss: 0.925616]\n",
            "[Epoch 0/10] [Batch 2598/7121] [D loss: 0.543653] [G loss: 1.020985] [AE loss: 1.272291]\n",
            "[Epoch 0/10] [Batch 2599/7121] [D loss: 0.543733] [G loss: 1.007096] [AE loss: 1.032099]\n",
            "[Epoch 0/10] [Batch 2600/7121] [D loss: 0.618937] [G loss: 1.002017] [AE loss: 1.235175]\n",
            "[Epoch 0/10] [Batch 2601/7121] [D loss: 0.503862] [G loss: 1.003622] [AE loss: 1.259887]\n",
            "[Epoch 0/10] [Batch 2602/7121] [D loss: 0.594721] [G loss: 0.984919] [AE loss: 1.220074]\n",
            "[Epoch 0/10] [Batch 2603/7121] [D loss: 0.544302] [G loss: 0.965268] [AE loss: 1.033965]\n",
            "[Epoch 0/10] [Batch 2604/7121] [D loss: 0.585096] [G loss: 0.975327] [AE loss: 1.105677]\n",
            "[Epoch 0/10] [Batch 2605/7121] [D loss: 0.523831] [G loss: 0.987910] [AE loss: 1.874247]\n",
            "[Epoch 0/10] [Batch 2606/7121] [D loss: 0.561525] [G loss: 1.005823] [AE loss: 1.371173]\n",
            "[Epoch 0/10] [Batch 2607/7121] [D loss: 0.509967] [G loss: 0.985047] [AE loss: 1.082450]\n",
            "[Epoch 0/10] [Batch 2608/7121] [D loss: 0.611334] [G loss: 1.004210] [AE loss: 0.689946]\n",
            "[Epoch 0/10] [Batch 2609/7121] [D loss: 0.477844] [G loss: 1.031443] [AE loss: 1.245564]\n",
            "[Epoch 0/10] [Batch 2610/7121] [D loss: 0.461311] [G loss: 1.032791] [AE loss: 1.380043]\n",
            "[Epoch 0/10] [Batch 2611/7121] [D loss: 0.524456] [G loss: 1.050422] [AE loss: 0.912964]\n",
            "[Epoch 0/10] [Batch 2612/7121] [D loss: 0.467458] [G loss: 1.063447] [AE loss: 0.845365]\n",
            "[Epoch 0/10] [Batch 2613/7121] [D loss: 0.533670] [G loss: 1.072516] [AE loss: 1.382349]\n",
            "[Epoch 0/10] [Batch 2614/7121] [D loss: 0.469407] [G loss: 1.079082] [AE loss: 0.704553]\n",
            "[Epoch 0/10] [Batch 2615/7121] [D loss: 0.482396] [G loss: 1.095885] [AE loss: 1.577255]\n",
            "[Epoch 0/10] [Batch 2616/7121] [D loss: 0.476968] [G loss: 1.098056] [AE loss: 0.786213]\n",
            "[Epoch 0/10] [Batch 2617/7121] [D loss: 0.432420] [G loss: 1.090972] [AE loss: 0.852044]\n",
            "[Epoch 0/10] [Batch 2618/7121] [D loss: 0.392266] [G loss: 1.088663] [AE loss: 1.348521]\n",
            "[Epoch 0/10] [Batch 2619/7121] [D loss: 0.449521] [G loss: 1.094184] [AE loss: 0.850761]\n",
            "[Epoch 0/10] [Batch 2620/7121] [D loss: 0.476144] [G loss: 1.102703] [AE loss: 1.153929]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-dd642cb6b760>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Call the training function with the correct input size for discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m train_gan_autoencoder(autoencoder=autoencoder_model, \n\u001b[0m\u001b[1;32m     27\u001b[0m                       \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                       \u001b[0mdiscriminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-23707a51b129>\u001b[0m in \u001b[0;36mtrain_gan_autoencoder\u001b[0;34m(autoencoder, generator, discriminator, data_loader, epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer_AE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mreconstructed_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mae_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mae_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-879b5428f042>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mencoded_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdecoded_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfinal_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMtZQ3qVfdsX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}